Project structure for '/c/Users/Galaxy/LEVI/projects/Python/autofinance/b3_alloc_system':
===============================================================================
  config/config.yaml
  config/portfolio_A.yaml
  config/universe_conservative.csv
  config/universe_small.csv
  config/views/qualitative_views_A.yaml
  data/processed/fundamentals_quarterly.parquet
  data/processed/fx_usd_brl_daily.parquet
  data/processed/risk_free_daily.parquet
  environment.yml
  notebooks/00_data_audit.ipynb
  notebooks/01_factor_construction.ipynb
  notebooks/02_risk_engine_validation.ipynb
  notebooks/03_return_engine_validation.ipynb
  notebooks/04_black_litterman_sandbox.ipynb
  notebooks/05_optimizer_tests.ipynb
  notebooks/06_full_backtest_driver.ipynb
  notebooks/07_results_report_template.ipynb
  pyproject.toml
  README.md
  run_backtest.py
  scripts/build_factors.py
  scripts/generate_report.py
  scripts/run_backtests.py
  scripts/update_data.py
  src/__init__.py
  src/b3_alloc_system.egg-info/dependency_links.txt
  src/b3_alloc_system.egg-info/PKG-INFO
  src/b3_alloc_system.egg-info/requires.txt
  src/b3_alloc_system.egg-info/SOURCES.txt
  src/b3_alloc_system.egg-info/top_level.txt
  src/b3alloc/__init__.py
  src/b3alloc/__pycache__/__init__.cpython-313.pyc
  src/b3alloc/__pycache__/config.cpython-313.pyc
  src/b3alloc/__pycache__/utils_dates.cpython-313.pyc
  src/b3alloc/backtest/__init__.py
  src/b3alloc/backtest/__pycache__/__init__.cpython-313.pyc
  src/b3alloc/backtest/__pycache__/analytics.cpython-313.pyc
  src/b3alloc/backtest/__pycache__/engine.cpython-313.pyc
  src/b3alloc/backtest/__pycache__/portfolio_accounting.cpython-313.pyc
  src/b3alloc/backtest/analytics.py
  src/b3alloc/backtest/engine.py
  src/b3alloc/backtest/portfolio_accounting.py
  src/b3alloc/bl/__init__.py
  src/b3alloc/bl/__pycache__/__init__.cpython-313.pyc
  src/b3alloc/bl/__pycache__/black_litterman.cpython-313.pyc
  src/b3alloc/bl/__pycache__/confidence.cpython-313.pyc
  src/b3alloc/bl/__pycache__/view_builder.cpython-313.pyc
  src/b3alloc/bl/black_litterman.py
  src/b3alloc/bl/confidence.py
  src/b3alloc/bl/view_builder.py
  src/b3alloc/config.py
  src/b3alloc/data/__init__.py
  src/b3alloc/data/__pycache__/__init__.cpython-313.pyc
  src/b3alloc/data/__pycache__/ingest_fundamentals.cpython-313.pyc
  src/b3alloc/data/__pycache__/ingest_fx.cpython-313.pyc
  src/b3alloc/data/__pycache__/ingest_prices.cpython-313.pyc
  src/b3alloc/data/__pycache__/ingest_selic.cpython-313.pyc
  src/b3alloc/data/ingest_fundamentals.py
  src/b3alloc/data/ingest_fx.py
  src/b3alloc/data/ingest_prices.py
  src/b3alloc/data/ingest_selic.py
  src/b3alloc/factors/__init__.py
  src/b3alloc/factors/__pycache__/__init__.cpython-313.pyc
  src/b3alloc/factors/__pycache__/fama_french_b3.cpython-313.pyc
  src/b3alloc/factors/fama_french_b3.py
  src/b3alloc/optimize/__init__.py
  src/b3alloc/optimize/__pycache__/__init__.cpython-313.pyc
  src/b3alloc/optimize/__pycache__/constraints.cpython-313.pyc
  src/b3alloc/optimize/__pycache__/mean_variance.cpython-313.pyc
  src/b3alloc/optimize/constraints.py
  src/b3alloc/optimize/mean_variance.py
  src/b3alloc/preprocess/__init__.py
  src/b3alloc/preprocess/__pycache__/__init__.cpython-313.pyc
  src/b3alloc/preprocess/__pycache__/align.cpython-313.pyc
  src/b3alloc/preprocess/__pycache__/clean.cpython-313.pyc
  src/b3alloc/preprocess/__pycache__/returns.cpython-313.pyc
  src/b3alloc/preprocess/align.py
  src/b3alloc/preprocess/clean.py
  src/b3alloc/preprocess/returns.py
  src/b3alloc/returns/__init__.py
  src/b3alloc/returns/__pycache__/__init__.cpython-313.pyc
  src/b3alloc/returns/__pycache__/ff_view.cpython-313.pyc
  src/b3alloc/returns/__pycache__/var_view.cpython-313.pyc
  src/b3alloc/returns/ff_view.py
  src/b3alloc/returns/var_view.py
  src/b3alloc/risk/__init__.py
  src/b3alloc/risk/__pycache__/__init__.cpython-313.pyc
  src/b3alloc/risk/__pycache__/dcc.cpython-313.pyc
  src/b3alloc/risk/__pycache__/garch.cpython-313.pyc
  src/b3alloc/risk/__pycache__/risk_engine.cpython-313.pyc
  src/b3alloc/risk/__pycache__/shrinkage.cpython-313.pyc
  src/b3alloc/risk/dcc.py
  src/b3alloc/risk/garch.py
  src/b3alloc/risk/risk_engine.py
  src/b3alloc/risk/shrinkage.py
  src/b3alloc/taxes/__init__.py
  src/b3alloc/taxes/__pycache__/__init__.cpython-313.pyc
  src/b3alloc/taxes/__pycache__/darf_reporter.cpython-313.pyc
  src/b3alloc/taxes/__pycache__/ledger.cpython-313.pyc
  src/b3alloc/taxes/__pycache__/tax_tracker.cpython-313.pyc
  src/b3alloc/taxes/darf_reporter.py
  src/b3alloc/taxes/ledger.py
  src/b3alloc/taxes/tax_tracker.py
  src/b3alloc/trades/__init__.py
  src/b3alloc/trades/__pycache__/__init__.cpython-313.pyc
  src/b3alloc/trades/__pycache__/trade_calculator.cpython-313.pyc
  src/b3alloc/trades/trade_calculator.py
  src/b3alloc/utils_dates.py
  src/b3alloc/views/__init__.py
  src/b3alloc/views/__pycache__/__init__.cpython-313.pyc
  src/b3alloc/views/__pycache__/views_parser.cpython-313.pyc
  src/b3alloc/views/views_parser.py
  src/b3alloc/viz/__init__.py
  src/b3alloc/viz/__pycache__/__init__.cpython-313.pyc
  src/b3alloc/viz/__pycache__/plots_portfolio.cpython-313.pyc
  src/b3alloc/viz/plots_portfolio.py



###############################################################################
### FILE: config/config.yaml
###############################################################################
meta:
  portfolio_name: B3_Allocation_System_Test
  base_currency: BRL

data:
  start: "2021-01-01"
  end: "2023-12-31"
  tickers_file: "config/universe_small.csv"
  selic_series: 11
  publish_lag_days: 90

universe:
  include_fx_factor: false
  fx_series_id: 1
  asset_flags: {}

risk_engine:
  garch:
    dist: student-t
    min_obs: 252
    refit_freq_days: 63
  dcc:
    a_init: 0.03
    b_init: 0.96
  shrinkage:
    method: ledoit_wolf_constant_corr
    floor: 0.0

return_engine:
  factor:
    lookback_days: 756
    include_alpha: true
    premium_estimator: expanding_mean
  var:
    max_lag: 5
    criterion: bic
    log_returns: true

black_litterman:
  tau: 0.025
  confidence:
    method: user_scaled
    factor_scaler: 1.0
    var_scaler: 1.0
  qualitative_views_file: null

optimizer:
  objective: max_sharpe
  long_only: true
  name_cap: 0.20
  sector_cap: 0.30
  turnover_penalty_bps: 5

tax:
  enable: false
  brokerage_fee_bps: 0

backtest:
  lookback_years: 3
  rebalance: monthly
  start: "2022-01-01"
  end: "2023-12-31"
  costs_bps: 10
  initial_capital: 1000000.0 


###############################################################################
### FILE: config/universe_small.csv
###############################################################################
ticker
PETR4.SA
VALE3.SA
ITUB4.SA
BBDC4.SA
WEGE3.SA
ABEV3.SA 


###############################################################################
### FILE: run_backtest.py
###############################################################################
import pandas as pd
from pathlib import Path
import logging
import traceback
import sys

# Add the src directory to the Python path
sys.path.insert(0, str(Path(__file__).resolve().parent / 'src'))

from b3alloc.config import load_config
from b3alloc.data import (
    create_equity_price_series,
    create_index_series,
    create_fundamentals_series,
    create_risk_free_series,
    create_fx_series,
)
from b3alloc.preprocess.align import align_fundamentals_to_prices
from b3alloc.preprocess.returns import compute_returns
from b3alloc.factors.fama_french_b3 import build_fama_french_factors
from b3alloc.backtest.engine import BacktestEngine
from b3alloc.backtest.analytics import compute_performance_metrics

# --- Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def run():
    """Main function to orchestrate the entire backtesting pipeline."""
    try:
        # 1. Load Configuration
        logging.info("--- 1. Loading Configuration ---")
        # Make path relative to the script's location
        script_dir = Path(__file__).resolve().parent
        config_path = script_dir / "config/config.yaml"
        cfg = load_config(config_path)
        
        # The tickers_file path in YAML is relative to the script's parent (the project root)
        # So, we join the script's parent with the path from config.
        tickers_path = script_dir / cfg.data.tickers_file
        if not tickers_path.exists():
            raise FileNotFoundError(f"Tickers file not found at: {tickers_path}")
        tickers = pd.read_csv(tickers_path)['ticker'].tolist()
        logging.info(f"Loaded {len(tickers)} tickers for backtest.")

        # 2. Data Ingestion
        logging.info("--- 2. Ingesting Data ---")
        prices_long = create_equity_price_series(tickers, cfg.data.start, cfg.data.end)
        index_prices = create_index_series(cfg.data.start, cfg.data.end)
        fundamentals = create_fundamentals_series(tickers)
        risk_free = create_risk_free_series(cfg.data.start, cfg.data.end, series_id=cfg.data.selic_series)

        # 3. Data Preprocessing & Feature Engineering
        logging.info("--- 3. Preprocessing Data ---")
        
        # Calculate returns
        prices_wide = prices_long.pivot(index='date', columns='ticker', values='adj_close')
        returns_bundle = compute_returns(prices_wide, risk_free, benchmark_df=index_prices)
        
        # Align fundamentals
        daily_fundamentals = align_fundamentals_to_prices(
            fundamentals,
            prices_wide.index,
            cfg.data.publish_lag_days
        )
        
        # Calculate market caps
        shares_outstanding_wide = daily_fundamentals.pivot(index='date', columns='ticker', values='shares_outstanding')
        market_caps = prices_wide * shares_outstanding_wide
        
        # Build Fama-French Factors
        logging.info("--- 4. Building Factors ---")
        factors = build_fama_french_factors(
            daily_fundamentals,
            prices_wide,
            returns_bundle['simple'],
            returns_bundle['market_excess']
        )

        # 4. Assemble Data Stores for Backtest Engine
        logging.info("--- 5. Assembling Data Stores ---")
        data_stores = {
            "prices": prices_long,
            "log_returns": returns_bundle['log'],
            "excess_log_returns": returns_bundle['log_excess'],
            "market_excess_returns": returns_bundle['market_excess'],
            "factors": factors,
            "market_caps": market_caps,
        }

        # 5. Run Backtest Engine
        logging.info("--- 6. Running Backtest Engine ---")
        engine = BacktestEngine(cfg, data_stores)
        history_df = engine.run_backtest()

        # 6. Compute and Display Performance Analytics
        logging.info("--- 7. Computing Performance Analytics ---")
        strategy_returns = history_df['portfolio_value'].pct_change().dropna()
        benchmark_returns = returns_bundle['market_simple']
        
        # Align strategy and benchmark returns
        aligned_returns = pd.concat([strategy_returns, benchmark_returns], axis=1).dropna()
        
        performance = compute_performance_metrics(
            strategy_returns=aligned_returns.iloc[:, 0],
            benchmark_returns=aligned_returns.iloc[:, 1],
            risk_free_rate=risk_free['rf_daily'],
            periods_per_year=252 # Daily data
        )
        
        print("\n" + "="*50)
        print("          BACKTEST PERFORMANCE METRICS")
        print("="*50)
        print(performance)
        print("="*50)

    except Exception as e:
        logging.error(f"An error occurred in the pipeline: {e}")
        traceback.print_exc()

if __name__ == '__main__':
    run() 


###############################################################################
### FILE: README.md
###############################################################################
# Advanced Automated Portfolio Allocation System for the Brazilian (B3) Market

This repository contains the source code for a robust, reproducible, and academically grounded research stack for Brazilian equity portfolio allocation, as detailed in the project specification.

The system is designed to be modular, separating signal generation from risk estimation and using a Bayesian framework (Black-Litterman) to synthesize views and produce optimized portfolio weights.

**Status:** Under Development

---

## Setup and Installation

This project is managed using `conda` for environment control and `pip` for installing the local package.

1.  **Clone the repository (if you haven't already):**
    ```bash
    git clone <your-repo-url>
    cd b3_alloc_system
    ```

2.  **Create and activate the Conda environment:**
    The `environment.yml` file contains all necessary dependencies. The environment should be named `finance`.
    ```bash
    # Create the environment from the file
    conda env create -f environment.yml

    # Activate the environment
    conda activate finance
    ```
    *Note: The final line in the `environment.yml` file (`- -e .`) also installs the project's own source code in "editable" mode. This means any changes you make to the Python files in `/src/b3alloc` will be immediately available without needing to reinstall.*

3.  **Verify the installation:**
    Once the environment is active, you should be able to run `python` and `import b3alloc` without errors.

---

## Project Structure

-   **/src/b3alloc**: The core Python package containing all logic.
-   **/notebooks**: Jupyter notebooks for research, validation, and reporting.
-   **/config**: YAML configuration files for managing parameters.
-   **/data**: Raw, processed, and intermediate data files.
*   **/scripts**: Standalone Python scripts for running major tasks (e.g., data ingestion, backtesting).
-   **/reports**: Output for generated backtest reports, figures, and artifacts.


###############################################################################
### FILE: config/portfolio_A.yaml
###############################################################################
meta:
  portfolio_name: "Conservative Portfolio"
  base_currency: "BRL"
data:
  start: "2015-01-01"
  end: "2023-12-31"
  tickers_file: "config/universe_conservative.csv"
  selic_series: 11
  publish_lag_days: 45
universe:
  include_fx_factor: true
  fx_series_id: 1
  asset_flags:
    "AAPL34.SA": { "fx_sensitive": true }
risk_engine:
  garch:
    dist: "gaussian"
    min_obs: 253
    refit_freq_days: 63
  dcc:
    a_init: 0.05
    b_init: 0.94
  shrinkage:
    method: "ledoit_wolf_constant_corr"
    floor: 0.01
return_engine:
  factor:
    lookback_days: 756
    include_alpha: false
    premium_estimator: "long_term_mean"
  var:
    max_lag: 5
    criterion: "bic"
    log_returns: true
black_litterman:
  tau: 0.025
  confidence:
    method: "rmse_based"
    factor_scaler: 1.0
    var_scaler: 1.5
  qualitative_views_file: "config/views/qualitative_views_A.yaml"
optimizer:
  objective: "max_sharpe"
  long_only: true
  name_cap: 0.15
  sector_cap: 0.30
  turnover_penalty_bps: 2
tax:
  enable: true
  brokerage_fee_bps: 5
backtest:
  lookback_years: 3
  rebalance: "monthly"
  start: "2018-01-01"
  end: "2023-12-31"
  costs_bps: 10
  initial_capital: 1000000.0 


###############################################################################
### FILE: config/universe_conservative.csv
###############################################################################
ticker
PETR4.SA
VALE3.SA
ITUB4.SA
BBDC4.SA
ABEV3.SA
WEGE3.SA
MGLU3.SA
LREN3.SA
B3SA3.SA
SUZB3.SA
GGBR4.SA
BBAS3.SA
ITSA4.SA
RENT3.SA
ELET3.SA
AAPL34.SA 


###############################################################################
### FILE: config/views/qualitative_views_A.yaml
###############################################################################
views:
  - type: relative
    expr: "VALE3.SA – PETR4.SA"
    magnitude: 0.02
    confidence: 0.6
  - type: absolute
    expr: "MGLU3.SA"
    magnitude: -0.03
    confidence: 0.4 


###############################################################################
### FILE: environment.yml
###############################################################################
﻿name: finance # You can change this to your environment name
channels:
  - conda-forge
  - defaults
dependencies:
  # Core Python version
  - python=3.12   

  # Core data and modeling libraries from conda-forge
  - pandas>=2.0.0
  - numpy>=1.24.0
  - scipy>=1.10.0
  - pydantic>=2.0
  - pyyaml>=6.0
  - yfinance>=0.2.61
  - curl_cffi>=0.6            # yfinance backend
  - brfinance>=0.0.9          # NEW
  - requests>=2.30.0
  - arch-py>=5.3.0
  - statsmodels>=0.14.0
  - scikit-learn>=1.3.0
  - cvxpy>=1.3.0
  - osqp>=0.6.0
  - holidays>=0.30

  # Development and notebook tools
  - jupyterlab>=4.0.0
  - notebook>=7.0.0
  - pytest>=7.4.0
  - ruff>=0.1.0
  - dask>=2023.9.0
  - joblib>=1.3.0

  # Reporting and visualization
  - matplotlib>=3.7.0
  - plotly>=5.15.0
  - bokeh>=3.2.0
  - nbconvert>=7.8.0
  - openpyxl>=3.1.0
  - python-docx>=1.1.0

  # Pip-specific packages
  - pip
  - pip:
      # This line is crucial: it installs your project's source code
      # in "editable" mode, so changes to your .py files are
      # immediately available without reinstalling.
      - -e .


###############################################################################
### FILE: notebooks/00_data_audit.ipynb
###############################################################################
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700784b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. SETUP AND IMPORTS ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set plotting style and suppress scientific notation for clarity\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('deep')\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "# --- 2. ESTABLISH PATHS ---\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "PROCESSED_DATA_PATH = PROJECT_ROOT / 'data' / 'processed'\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Attempting to load data from: {PROCESSED_DATA_PATH}\")\n",
    "\n",
    "# --- 3. LOAD ALL DATASETS ---\n",
    "try:\n",
    "    prices_df = pd.read_parquet(PROCESSED_DATA_PATH / 'prices_equity_daily.parquet')\n",
    "    index_df = pd.read_parquet(PROCESSED_DATA_PATH / 'index_ibov_daily.parquet')\n",
    "    rf_df = pd.read_parquet(PROCESSED_DATA_PATH / 'risk_free_daily.parquet')\n",
    "    fundamentals_df = pd.read_parquet(PROCESSED_DATA_PATH / 'fundamentals_quarterly.parquet')\n",
    "    print(\"\\nSUCCESS: All required data files loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: Could not load data file: {e}\")\n",
    "    print(\"Please ensure you have run 'scripts/update_data.py' successfully before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045cb1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Step 2: Price Data Audit (Equities & Index)\n",
    "#\n",
    "# Perform sanity checks on the core price series and visualize key assets.\n",
    "#\n",
    "\n",
    "print(\"--- Equity Price Data Sanity Checks ---\")\n",
    "print(f\"Any negative adjusted close prices? {'Yes' if (prices_df['adj_close'] <= 0).any() else 'No'}\")\n",
    "print(f\"Number of rows with any missing data: {prices_df.isnull().any(axis=1).sum()}\")\n",
    "\n",
    "# Check for extreme one-day returns, which may indicate data errors or splits.\n",
    "prices_df['daily_return'] = prices_df.groupby('ticker')['adj_close'].pct_change()\n",
    "extreme_returns = prices_df[prices_df['daily_return'].abs() > 0.80]\n",
    "print(f\"\\nFound {len(extreme_returns)} instances of daily returns > 80%.\")\n",
    "if not extreme_returns.empty:\n",
    "    print(\"Instances of extreme returns to investigate:\")\n",
    "    print(extreme_returns[['ticker', 'date', 'daily_return']])\n",
    "\n",
    "# --- Visual Inspection ---\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "fig.suptitle('Visual Audit of Price Series', fontsize=16)\n",
    "\n",
    "# Plot a selection of key tickers\n",
    "tickers_to_plot = ['PETR4.SA', 'AAPL34.SA'] # A domestic stock and a BDR\n",
    "for ticker in tickers_to_plot:\n",
    "    asset_data = prices_df[prices_df['ticker'] == ticker].set_index('date')\n",
    "    if not asset_data.empty:\n",
    "        ax[0].plot(asset_data.index, asset_data['adj_close'], label=ticker, alpha=0.9)\n",
    "ax[0].set_title('Selected Equity Price Series')\n",
    "ax[0].set_ylabel('Adjusted Close (R$)')\n",
    "ax[0].legend()\n",
    "ax[0].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Plot the IBOV index\n",
    "ax[1].plot(index_df.index, index_df['adj_close'], label='IBOV Index (^BVSP)', color='black')\n",
    "ax[1].set_title('IBOV Index Price Series')\n",
    "ax[1].set_ylabel('Index Level')\n",
    "ax[1].legend()\n",
    "ax[1].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()```\n",
    "\n",
    "---\n",
    "**Cell 3: Code**\n",
    "```python\n",
    "#\n",
    "# Step 3: Risk-Free & Fundamentals Audit\n",
    "#\n",
    "# Audit the SELIC rate for plausibility and inspect the structure of the\n",
    "# quarterly fundamental data.\n",
    "#\n",
    "\n",
    "print(\"--- Risk-Free (SELIC) Data Audit ---\")\n",
    "print(rf_df.describe())\n",
    "\n",
    "# --- Visual Inspection of SELIC and Fundamentals ---\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "fig.suptitle('Visual Audit of Non-Price Data', fontsize=16)\n",
    "\n",
    "# Plot SELIC Rate\n",
    "axes[0].plot(rf_df.index, rf_df['selic_annualized'], label='SELIC (Annualized)')\n",
    "axes[0].set_title('Annualized SELIC Rate Over Time')\n",
    "axes[0].set_ylabel('Annualized Rate')\n",
    "axes[0].yaxis.set_major_formatter(plt.FuncFormatter('{:.1%}'.format))\n",
    "axes[0].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Plot Book Value Per Share for a sample ticker to confirm quarterly steps\n",
    "vale_fundamentals = fundamentals_df[fundamentals_df['ticker'] == 'VALE3.SA'].set_index('fiscal_period_end')\n",
    "if not vale_fundamentals.empty:\n",
    "    axes[1].plot(vale_fundamentals.index, vale_fundamentals['book_per_share'], marker='o', linestyle='-')\n",
    "    axes[1].set_title('Book Value Per Share (VPA) for VALE3.SA')\n",
    "    axes[1].set_ylabel('VPA (R$)')\n",
    "    axes[1].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()\n",
    "\n",
    "# --- Fundamentals Sanity Checks ---\n",
    "print(\"\\n--- Fundamentals Sanity Checks ---\")\n",
    "print(f\"Any negative book equity? {'Yes' if (fundamentals_df['book_equity'] < 0).any() else 'No'}\")\n",
    "print(f\"Any zero/negative shares outstanding? {'Yes' if (fundamentals_df['shares_outstanding'] <= 0).any() else 'No'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}



###############################################################################
### FILE: pyproject.toml
###############################################################################
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "b3_alloc_system"
version = "0.1.0"
description = "Advanced Automated Portfolio Allocation System for the Brazilian (B3) Market"
authors = [{ name = "Your Name", email = "your@email.com" }]
readme = "README.md"
requires-python = ">=3.10"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]

# Core dependencies inferred from the project spec
dependencies = [
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "scipy>=1.10.0",
    "pydantic>=2.0",
    "pyyaml>=6.0",
    "yfinance>=0.2.61",
    "requests>=2.32",
    "curl_cffi>=0.6",
    "certifi>=2025.5",
    "arch>=5.3.0",          # For GARCH/DCC models
    "statsmodels>=0.14.0",     # For VAR, OLS
    "scikit-learn>=1.3.0",     # For Ledoit-Wolf, potential ML models
    "cvxpy>=1.3.0",            # Primary optimizer
    "osqp>=0.6.0",             # Solver for CVXPY
    "holidays>=0.30"           # For B3 trading calendar
    "tenacity>=8.2.0",
    "brfinance>=0.0.9"
]

[project.optional-dependencies]
# Dependencies for development, testing, and notebooks
dev = [
    "jupyterlab>=4.0.0",
    "notebook>=7.0.0",
    "pytest>=7.4.0",
    "ruff>=0.1.0",
    "dask>=2023.9.0",
    "joblib>=1.3.0"
]
# Dependencies for visualization and reporting
reporting = [
    "matplotlib>=3.7.0",
    "plotly>=5.15.0",
    "bokeh>=3.2.0",
    "nbconvert>=7.8.0",
    "openpyxl>=3.1.0",         # For Excel DARF report
    "python-docx>=1.1.0"       # For PDF/Docx DARF report
]
# All optional dependencies
all = [
    "b3_alloc_system[dev]",
    "b3_alloc_system[reporting]"
]


[project.urls]
"Homepage" = "https://github.com/yourusername/b3_alloc_system"
"Bug Tracker" = "https://github.com/yourusername/b3_alloc_system/issues"

[tool.setuptools.packages.find]
where = ["src"]


###############################################################################
### FILE: scripts/build_factors.py
###############################################################################
import pandas as pd
from pathlib import Path
import sys

# Add the source directory to the Python path
sys.path.append(str(Path(__file__).resolve().parents[1]))

from b3alloc.factors.fama_french_b3 import build_fama_french_factors
from b3alloc.preprocess.align import align_fundamentals_to_prices
from b3alloc.preprocess.returns import compute_returns
from b3alloc.config import load_config

def main():
    """
    Main function to pre-compute and save the Fama-French factor panel.
    """
    print("--- Starting Fama-French Factor Construction ---")
    
    project_root = Path(__file__).resolve().parents[1]
    data_path = project_root / "data" / "processed"

    # Load config to get the publication lag
    try:
        config_path = project_root / 'config' / 'portfolio_A.yaml'
        cfg = load_config(config_path)
        publish_lag = cfg.data.publish_lag_days
        print(f"Using publication lag of {publish_lag} days from config.")
    except (FileNotFoundError, AttributeError):
        print("Warning: Could not load config. Falling back to default lag of 65 days.")
        publish_lag = 65
    
    if not data_path.exists():
        print(f"ERROR: Processed data directory not found at {data_path}")
        print("Please run 'scripts/update_data.py' first.")
        return

    # --- 1. Load Required Processed Data ---
    print("Loading processed price, fundamental, and risk-free data...")
    try:
        prices_df_long = pd.read_parquet(data_path / "prices_equity_daily.parquet")
        fundamentals_df = pd.read_parquet(data_path / "fundamentals_quarterly.parquet")
        rf_df = pd.read_parquet(data_path / "risk_free_daily.parquet")
        ibov_df = pd.read_parquet(data_path / "index_ibov_daily.parquet")
    except FileNotFoundError as e:
        print(f"ERROR: Missing data file - {e.filename}")
        print("Please ensure 'scripts/update_data.py' has been run successfully.")
        return

    # --- 2. Prepare Data for Factor Model ---
    print("Preparing data panels for factor construction...")
    
    # We need prices in wide format for market cap calculation
    prices_wide = prices_df_long.pivot(index='date', columns='ticker', values='adj_close')
    
    # We need daily fundamentals, correctly lagged
    # A generic lag of 65 trading days (~3 months) is a common heuristic
    # if publish dates aren't available.
    daily_fundamentals = align_fundamentals_to_prices(
        fundamentals_df=fundamentals_df,
        price_dates=prices_wide.index,
        publish_lag_days=publish_lag 
    )
    
    # We need simple returns
    all_returns = compute_returns(prices_df_long, rf_df)
    simple_returns_wide = all_returns['simple']
    
    # We need market excess returns for the 'MKT' factor
    market_simple_returns = ibov_df['adj_close'].pct_change()
    market_excess_returns = market_simple_returns - rf_df['rf_daily']
    market_excess_returns = market_excess_returns.reindex(simple_returns_wide.index).ffill().dropna()

    # --- 3. Build Factors ---
    factor_panel = build_fama_french_factors(
        daily_fundamentals_df=daily_fundamentals,
        prices_df=prices_wide,
        returns_df=simple_returns_wide,
        market_excess_returns=market_excess_returns
    )
    
    # --- 4. Save the Factor Panel ---
    output_file = data_path / "factor_panel_daily.parquet"
    factor_panel.to_parquet(output_file)
    
    print(f"\nSuccessfully built and saved Fama-French factor panel to:")
    print(output_file)
    print("\n--- Factor Construction Finished ---")

if __name__ == '__main__':
    main()


###############################################################################
### FILE: scripts/generate_report.py
###############################################################################
import argparse
import pandas as pd
from pathlib import Path
import json

def create_markdown_report(run_path: Path) -> str:
    """
    Generates a full Markdown report from a backtest run's artifacts.

    Args:
        run_path: The Path object pointing to the specific run directory
                  (e.g., 'reports/runs/run_20250718_103000').

    Returns:
        A string containing the complete Markdown report.
    """
    print(f"Generating Markdown report for run: {run_path.name}")
    
    # --- 1. Load Artifacts ---
    try:
        metrics = json.loads((run_path / "performance_summary.json").read_text())
        config_text = (run_path / "config.yaml").read_text()
    except FileNotFoundError:
        return f"# Error: Could not generate report for {run_path.name}.\nMissing 'performance_summary.json' or 'config.yaml'."

    # --- 2. Build Markdown Sections ---
    
    # Header
    header = f"# Backtest Report: {run_path.name}\n\n"
    header += f"This report details the performance of a strategy run executed on `{run_path.name.split('_')[1]}`.\n"
    
    # Summary Table
    summary_table = "## 1. Summary Performance\n\n"
    summary_table += "| Metric                  | Strategy | Benchmark |\n"
    summary_table += "| ----------------------- | -------- | --------- |\n"
    summary_table += f"| **CAGR**                | `{metrics.get('CAGR', 0):.2%}` | `{metrics.get('Benchmark CAGR', 0):.2%}` |\n"
    summary_table += f"| **Annualized Volatility** | `{metrics.get('Annualized Volatility', 0):.2%}` | `{metrics.get('Benchmark Volatility', 0):.2%}` |\n"
    summary_table += f"| **Sharpe Ratio**          | `{metrics.get('Sharpe Ratio', 0):.2f}` | *N/A* |\n"
    summary_table += f"| **Max Drawdown**          | `{metrics.get('Max Drawdown', 0):.2%}` | *N/A* |\n"
    summary_table += f"| **Alpha (annualized)**    | `{metrics.get('Alpha (annualized)', 0):.2%}` | *N/A* |\n"
    summary_table += f"| **Beta**                  | `{metrics.get('Beta', 0):.2f}` | *N/A* |\n\n"

    # Plots
    plots = "## 2. Visualizations\n\n"
    plots += "### Equity Curve (Log Scale)\n"
    plots += "![Equity Curve](./equity_curve.png)\n\n"
    plots += "### Drawdowns\n"
    plots += "![Drawdowns](./drawdowns.png)\n\n"
    
    # Conditionally include the weights plot if it exists
    if (run_path / "weights_evolution.png").exists():
        plots += "### Portfolio Weights Evolution\n"
        plots += "![Weights Evolution](./weights_evolution.png)\n\n"
    
    # Configuration
    config_section = "## 3. Configuration\n\n"
    config_section += "This run was executed with the following configuration:\n"
    config_section += f"```yaml\n{config_text}\n```\n"

    # Combine all parts
    full_report = header + summary_table + plots + config_section
    
    return full_report

def main():
    """Main execution function."""
    parser = argparse.ArgumentParser(
        description="Generates a Markdown report from a backtest run directory."
    )
    parser.add_argument(
        "--run_dir",
        type=str,
        required=True,
        help="Path to the backtest run directory (e.g., reports/runs/run_...).",
    )
    args = parser.parse_args()
    
    project_root = Path(__file__).resolve().parents[1]
    run_path = project_root / args.run_dir
    
    if not run_path.is_dir():
        print(f"ERROR: Run directory not found at '{run_path}'")
        return
        
    # Generate the report content
    markdown_content = create_markdown_report(run_path)
    
    # Save the report to a file
    report_file_path = run_path / "report.md"
    report_file_path.write_text(markdown_content, encoding='utf-8')
    
    print(f"\nSuccessfully generated and saved report to:")
    print(report_file_path)

if __name__ == '__main__':
    # To test this script, you would first need to run `scripts/run_backtest.py`
    # which creates the necessary artifacts in a 'reports/runs/run_...' directory.
    # Then, you would execute this script from the command line:
    # python scripts/generate_report.py --run_dir reports/runs/run_20240101_120000
    
    print("--- Report Generation Script ---")
    print("This script is intended to be run from the command line after a backtest.")
    print("Example usage:")
    print("python scripts/generate_report.py --run_dir <path_to_your_run_directory>")


###############################################################################
### FILE: scripts/run_backtests.py
###############################################################################
import argparse
import pandas as pd
from pathlib import Path
import shutil
import yaml
from datetime import datetime
import sys
import numpy as np
import cvxpy as cp
from typing import Dict

# --- System Path Setup ---
sys.path.append(str(Path(__file__).resolve().parents[1]))

# --- Import All Necessary Modules from the Library ---
from b3alloc.config import load_config
from b3alloc.utils_dates import generate_rebalance_dates
from b3alloc.preprocess.clean import get_liquid_universe
from b3alloc.risk.risk_engine import build_covariance_matrix
from b3alloc.returns.ff_view import create_fama_french_view
from b3alloc.returns.var_view import create_var_view
from b3alloc.bl.black_litterman import calculate_risk_aversion, calculate_equilibrium_returns, calculate_posterior_returns
from b3alloc.bl.view_builder import build_full_view_matrices
from b3alloc.bl.confidence import estimate_view_uncertainty
from b3alloc.optimize.mean_variance import run_mean_variance_optimization
from b3alloc.optimize.constraints import build_optimizer_constraints
from b3alloc.trades.trade_calculator import calculate_target_shares, resolve_fractional_shares, compute_trade_list
from b3alloc.backtest.portfolio_accounting import PortfolioLedger
from b3alloc.taxes.ledger import TaxLedger
from b3alloc.taxes.tax_tracker import calculate_monthly_taxes
from b3alloc.taxes.darf_reporter import generate_darf_excel_report
from b3alloc.backtest.analytics import compute_performance_metrics
from b3alloc.viz.plots_portfolio import plot_equity_curve, plot_drawdowns


def _prepare_data_stores(data_path: Path) -> Dict[str, pd.DataFrame]:
    """Loads all necessary data from processed files into a dictionary."""
    print("Loading all processed data stores...")
    stores = {}
    
    # Dynamically find all parquet files in the processed data directory
    required_files = list(data_path.glob("*.parquet"))
    if not required_files:
        raise FileNotFoundError(f"No .parquet files found in {data_path}. Please run data ingestion scripts.")

    for f_path in required_files:
        try:
            key = f_path.stem # Use the filename without extension as the key
            stores[key] = pd.read_parquet(f_path)
        except Exception as e:
            print(f"Warning: Could not load or process {f_path.name}. Error: {e}")

    # Pre-calculate useful wide-format dataframes
    if 'prices_equity_daily' in stores:
        prices_long = stores['prices_equity_daily']
        stores['prices_wide'] = prices_long.pivot(index='date', columns='ticker', values='adj_close')
        stores['volume_wide'] = prices_long.pivot(index='date', columns='ticker', values='volume')
        
        # Pre-calculate market caps if fundamentals are also available
        if 'fundamentals_quarterly' in stores and 'shares_outstanding' in prices_long.columns:
            # This is a simplified approach. A full alignment would be needed for point-in-time caps.
            # For the purpose of data store prep, this is a reasonable pre-calculation.
            shares_wide = prices_long.pivot(index='date', columns='ticker', values='shares_outstanding').ffill()
            stores['market_caps'] = (stores['prices_wide'] * shares_wide).dropna(how='all')
    
    # Calculate simple returns for performance attribution
    if 'prices_wide' in stores and 'risk_free_daily' in stores:
        rf_series = stores['risk_free_daily']['rf_daily']
        stores['simple_returns'] = stores['prices_wide'].pct_change()
        
        if 'index_ibov_daily' in stores:
            stores['market_simple_returns'] = stores['index_ibov_daily']['adj_close'].pct_change()
            stores['market_excess_returns'] = (stores['market_simple_returns'] - rf_series).dropna()

    # Pre-calculate log returns for model inputs
    if 'simple_returns' in stores:
        stores['log_returns'] = np.log(1 + stores['simple_returns']).dropna()
    
    print("Data stores prepared successfully.")
    return stores

def main():
    parser = argparse.ArgumentParser(description="Run the full portfolio allocation backtest.")
    parser.add_argument("--config", type=str, required=True, help="Path to the master YAML configuration file.")
    args = parser.parse_args()

    # --- 1. Setup ---
    project_root = Path(__file__).resolve().parents[1]
    config_path = project_root / args.config
    cfg = load_config(config_path)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = project_root / "reports" / "runs" / f"run_{timestamp}"
    output_path.mkdir(parents=True, exist_ok=True)
    
    data_path = project_root / "data" / "processed"
    data_stores = _prepare_data_stores(data_path)

    # --- 2. Initialization ---
    p_ledger = PortfolioLedger(initial_capital=cfg.backtest.initial_capital)
    tax_ledger = TaxLedger()
    rebalance_dates = generate_rebalance_dates(cfg.backtest.start, cfg.backtest.end, cfg.backtest.rebalance)

    # --- 3. Main Backtesting Loop ---
    for rebalance_date in rebalance_dates:
        print(f"\n--- Processing Rebalance Date: {rebalance_date.date()} ---")
        
        # --- A. Get Current State & Universe ---
        current_prices = data_stores['prices_wide'].loc[rebalance_date]
        portfolio_value = p_ledger.get_portfolio_value(current_prices, rebalance_date)
        
        lookback_start = rebalance_date - pd.DateOffset(years=cfg.backtest.lookback_years)
        universe = get_liquid_universe(
            data_stores['prices_wide'].loc[lookback_start:rebalance_date],
            data_stores['volume_wide'].loc[lookback_start:rebalance_date],
            rebalance_date, lookback_days=365
        )
        if len(universe) < 2: continue

        # --- B. Run Full Model Pipeline ---
        try:
            # Slice data for models
            returns_slice = data_stores['log_returns'].loc[lookback_start:rebalance_date, universe]
            factors_slice = data_stores['factor_panel_daily'].loc[lookback_start:rebalance_date]
            fx_slice = data_stores['fx_usd_brl_daily'][['USD_BRL_log_return']].loc[lookback_start:rebalance_date]
            
            # Risk Model (FX-aware)
            sigma, risk_diags = build_covariance_matrix(returns_slice, cfg.risk_engine, fx_returns_df=fx_slice)
            final_universe = sorted(sigma.columns.tolist())
            
            # Return Models (FX-aware)
            ff_betas, ff_view = create_fama_french_view(
                returns_slice, factors_slice, cfg.return_engine.factor,
                fx_returns_df=fx_slice, asset_fx_sensitivity=cfg.universe.get('asset_flags')
            )
            var_view, var_diags = create_var_view(returns_slice, cfg.return_engine.var)
            
            # Black-Litterman (with Qualitative Views)
            qual_views = None
            if cfg.black_litterman.qualitative_views_file:
                qual_views_path = project_root / cfg.black_litterman.qualitative_views_file
                if qual_views_path.exists():
                    qual_views = yaml.safe_load(qual_views_path.read_text())['views']
                else:
                    print(f"Warning: Qualitative views file not found at {qual_views_path}")

            model_views = {'ff_view': ff_view, 'var_view': var_view}
            P, Q = build_full_view_matrices(model_views, qual_views, final_universe)
            
            if P is None: raise ValueError("No valid views could be constructed.")
            
            view_diags = {'ff_view': {'residual_variance': ff_betas['alpha']}} # Simplified for this run
            Omega = estimate_view_uncertainty(model_views, qual_views, view_diags, cfg.black_litterman)

            market_caps = data_stores['market_caps'].loc[rebalance_date, final_universe]
            market_weights = market_caps / market_caps.sum()
            lambda_aversion = calculate_risk_aversion(data_stores['market_excess_returns'].loc[:rebalance_date])
            pi_prior = calculate_equilibrium_returns(lambda_aversion, sigma, market_weights)
            
            mu_posterior = calculate_posterior_returns(pi_prior, sigma, P, Q, Omega, cfg.black_litterman.tau)
            
            # Optimizer
            w_var = cp.Variable(len(final_universe))
            constraints = build_optimizer_constraints(w_var, cfg.optimizer, final_universe)
            target_weights = run_mean_variance_optimization(mu_posterior, sigma, lambda_aversion, constraints)
            if target_weights is None: target_weights = market_weights.copy()
            
        except Exception as e:
            print(f"  -> ERROR in model pipeline: {e}. Holding previous positions.")
            continue

        # --- C. Generate and Execute Trades ---
        target_shares_df = calculate_target_shares(target_weights, portfolio_value, current_prices)
        target_shares_df = resolve_fractional_shares(target_shares_df)
        trade_list = compute_trade_list(p_ledger.holdings.copy(), target_shares_df.set_index('ticker')['target_shares'])
        
        if not trade_list.empty:
            p_ledger.execute_trades(trade_list, current_prices, rebalance_date, cost_per_trade_bps=cfg.backtest.costs_bps)
            # Record trades in tax ledger
            for _, trade in trade_list.iterrows():
                if trade['action'] == 'BUY':
                    tax_ledger.record_buy(trade['ticker'], trade['delta_shares'], current_prices[trade['ticker']], rebalance_date)
                elif trade['action'] == 'SELL':
                    tax_ledger.record_sell(trade['ticker'], abs(trade['delta_shares']), current_prices[trade['ticker']], rebalance_date)
        
        p_ledger.record_state(rebalance_date, current_prices)

    # --- 4. Post-Backtest Analysis ---
    results_history = p_ledger.get_history_df()
    results_history['returns'] = results_history['portfolio_value'].pct_change().fillna(0)
    
    # Performance Metrics
    total_years = (results_history.index[-1] - results_history.index[0]).days / 365.25
    periods_per_year = len(rebalance_dates) / total_years if total_years > 0 else 0
    
    metrics = compute_performance_metrics(
        results_history['returns'],
        data_stores['market_simple_returns'].reindex(results_history.index).ffill(),
        data_stores['risk_free_daily']['rf_daily'].reindex(results_history.index).ffill(),
        periods_per_year=periods_per_year
    )
    print("\n--- PERFORMANCE SUMMARY ---\n", metrics)
    
    # Tax Calculation
    monthly_taxes = [
        calculate_monthly_taxes(tax_ledger.sales_log, date.year, date.month)
        for date in pd.date_range(cfg.backtest.start, cfg.backtest.end, freq='M')
    ]
    generate_darf_excel_report(monthly_taxes, output_path)

    # --- 5. Save All Artifacts ---
    results_history.to_csv(output_path / "portfolio_history.csv")
    metrics.to_json(output_path / "performance_summary.json", indent=4)
    pd.DataFrame(tax_ledger.sales_log).to_csv(output_path / "sales_log.csv")
    shutil.copy(config_path, output_path / "config.yaml")

    fig_equity = plot_equity_curve(results_history['returns'], data_stores['market_simple_returns'])
    fig_equity.savefig(output_path / "equity_curve.png", dpi=300)
    fig_drawdown = plot_drawdowns(results_history['returns'])
    fig_drawdown.savefig(output_path / "drawdowns.png", dpi=300)
    
    print(f"\nRun complete. All artifacts saved to:\n{output_path}")

if __name__ == '__main__':
    # This script is intended to be run from the command line, e.g.:
    # python scripts/run_backtest.py --config config/portfolio_A.yaml
    # A user must first create the config file and run update_data.py & build_factors.py
    main()


###############################################################################
### FILE: scripts/update_data.py
###############################################################################
import pandas as pd
from pathlib import Path
import sys
from datetime import datetime

# Add the source directory to the Python path
sys.path.append(str(Path(__file__).resolve().parents[1]))

from b3alloc.data.ingest_selic import create_risk_free_series
from b3alloc.data.ingest_prices import create_equity_price_series, create_index_series
from b3alloc.data.ingest_fundamentals import create_fundamentals_series
from b3alloc.data.ingest_fx import create_fx_series
from b3alloc.config import load_config

# --- Configuration ---
# The universe of tickers is now loaded from the config file.
IBOV_TICKER = "^BVSP"

def main():
    """
    Main function to run the entire data ingestion and processing pipeline.
    """
    print("--- Starting Data Update and Processing ---")
    
    project_root = Path(__file__).resolve().parents[1]
    
    # Load configuration to get the universe and date ranges
    # This assumes a default or specified config file. For simplicity, we point to one.
    # A more robust CLI would allow specifying which portfolio's data to update.
    try:
        config_path = project_root / 'config' / 'portfolio_A.yaml'
        cfg = load_config(config_path)
        print(f"Loaded configuration from: {config_path}")
    except FileNotFoundError:
        print("ERROR: Default config 'config/portfolio_A.yaml' not found.")
        print("Please create a config file for your portfolio.")
        return

    output_path = project_root / "data" / "processed"
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"Data will be saved to: {output_path}")

    # --- Step 1: Ingest SELIC (Risk-Free Rate) ---
    try:
        print("\n[1/5] Fetching and processing SELIC risk-free rate...")
        risk_free_df = create_risk_free_series(cfg.data.start, cfg.data.end)
        risk_free_df.to_parquet(output_path / "risk_free_daily.parquet")
        print("  -> Saved 'risk_free_daily.parquet'")
    except Exception as e:
        print(f"  -> ERROR fetching SELIC data: {e}")

    # --- Step 2: Ingest IBOVESPA Index Prices ---
    try:
        print(f"\n[2/5] Fetching and processing benchmark index ({IBOV_TICKER})...")
        index_df = create_index_series(cfg.data.start, cfg.data.end, index_ticker=IBOV_TICKER)
        index_df.to_parquet(output_path / "index_ibov_daily.parquet")
        print(f"  -> Saved 'index_ibov_daily.parquet'")
    except Exception as e:
        print(f"  -> ERROR fetching index data: {e}")

    # --- Step 3: Ingest Equity Prices ---
    try:
        print("\n[3/5] Fetching and processing equity prices...")
        # Load tickers from the config's data section
        ticker_list = pd.read_csv(project_root / cfg.data.tickers_file)['ticker'].tolist()
        equity_prices_df = create_equity_price_series(ticker_list, cfg.data.start, cfg.data.end)
        equity_prices_df.to_parquet(output_path / "prices_equity_daily.parquet")
        print("  -> Saved 'prices_equity_daily.parquet'")
    except Exception as e:
        print(f"  -> ERROR fetching equity prices: {e}")

    # --- Step 4: Ingest Fundamentals Data ---
    try:
        print("\n[4/5] Fetching and processing quarterly fundamentals...")
        fundamentals_df = create_fundamentals_series(ticker_list)
        fundamentals_df.to_parquet(output_path / "fundamentals_quarterly.parquet")
        print("  -> Saved 'fundamentals_quarterly.parquet'")
    except Exception as e:
        print(f"  -> ERROR fetching fundamentals: {e}")

    # --- Step 5: Ingest FX Data ---
    try:
        print("\n[5/5] Fetching and processing FX (USD/BRL) data...")
        fx_df = create_fx_series(cfg.data.start, cfg.data.end, series_id=cfg.universe.fx_series_id)
        fx_df.to_parquet(output_path / "fx_usd_brl_daily.parquet")
        print("  -> Saved 'fx_usd_brl_daily.parquet'")
    except Exception as e:
        print(f"  -> ERROR fetching FX data: {e}")
        
    print("\n--- Data Update and Processing Finished ---")

if __name__ == '__main__':
    # To run this script, ensure you have an active internet connection
    # and have installed all dependencies from environment.yml.
    # Execute from the project root directory: `python scripts/update_data.py`
    main()



###############################################################################
### FILE: src/__init__.py
###############################################################################
# This file marks the 'b3alloc' directory as a Python package.
# It can also be used to define package-level variables or import key functions.

__version__ = "0.1.0"


###############################################################################
### FILE: src/b3_alloc_system.egg-info/PKG-INFO
###############################################################################
Metadata-Version: 2.4
Name: b3_alloc_system
Version: 0.1.0
Summary: Advanced Automated Portfolio Allocation System for the Brazilian (B3) Market
Author-email: Your Name <your@email.com>
Project-URL: Homepage, https://github.com/yourusername/b3_alloc_system
Project-URL: Bug Tracker, https://github.com/yourusername/b3_alloc_system/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: pandas>=2.0.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: scipy>=1.10.0
Requires-Dist: pydantic>=2.0
Requires-Dist: pyyaml>=6.0
Requires-Dist: yfinance>=0.2.30
Requires-Dist: requests>=2.30.0
Requires-Dist: arch>=5.3.0
Requires-Dist: statsmodels>=0.14.0
Requires-Dist: scikit-learn>=1.3.0
Requires-Dist: cvxpy>=1.3.0
Requires-Dist: osqp>=0.6.0
Requires-Dist: holidays>=0.30
Provides-Extra: dev
Requires-Dist: jupyterlab>=4.0.0; extra == "dev"
Requires-Dist: notebook>=7.0.0; extra == "dev"
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: dask>=2023.9.0; extra == "dev"
Requires-Dist: joblib>=1.3.0; extra == "dev"
Provides-Extra: reporting
Requires-Dist: matplotlib>=3.7.0; extra == "reporting"
Requires-Dist: plotly>=5.15.0; extra == "reporting"
Requires-Dist: bokeh>=3.2.0; extra == "reporting"
Requires-Dist: nbconvert>=7.8.0; extra == "reporting"
Requires-Dist: openpyxl>=3.1.0; extra == "reporting"
Requires-Dist: python-docx>=1.1.0; extra == "reporting"
Provides-Extra: all
Requires-Dist: b3_alloc_system[dev]; extra == "all"
Requires-Dist: b3_alloc_system[reporting]; extra == "all"

# Advanced Automated Portfolio Allocation System for the Brazilian (B3) Market

This repository contains the source code for a robust, reproducible, and academically grounded research stack for Brazilian equity portfolio allocation, as detailed in the project specification.

The system is designed to be modular, separating signal generation from risk estimation and using a Bayesian framework (Black-Litterman) to synthesize views and produce optimized portfolio weights.

**Status:** Under Development

---

## Setup and Installation

This project is managed using `conda` for environment control and `pip` for installing the local package.

1.  **Clone the repository (if you haven't already):**
    ```bash
    git clone <your-repo-url>
    cd b3_alloc_system
    ```

2.  **Create and activate the Conda environment:**
    The `environment.yml` file contains all necessary dependencies. The environment should be named `finance`.
    ```bash
    # Create the environment from the file
    conda env create -f environment.yml

    # Activate the environment
    conda activate finance
    ```
    *Note: The final line in the `environment.yml` file (`- -e .`) also installs the project's own source code in "editable" mode. This means any changes you make to the Python files in `/src/b3alloc` will be immediately available without needing to reinstall.*

3.  **Verify the installation:**
    Once the environment is active, you should be able to run `python` and `import b3alloc` without errors.

---

## Project Structure

-   **/src/b3alloc**: The core Python package containing all logic.
-   **/notebooks**: Jupyter notebooks for research, validation, and reporting.
-   **/config**: YAML configuration files for managing parameters.
-   **/data**: Raw, processed, and intermediate data files.
*   **/scripts**: Standalone Python scripts for running major tasks (e.g., data ingestion, backtesting).
-   **/reports**: Output for generated backtest reports, figures, and artifacts.



###############################################################################
### FILE: src/b3_alloc_system.egg-info/SOURCES.txt
###############################################################################
README.md
pyproject.toml
src/b3_alloc_system.egg-info/PKG-INFO
src/b3_alloc_system.egg-info/SOURCES.txt
src/b3_alloc_system.egg-info/dependency_links.txt
src/b3_alloc_system.egg-info/requires.txt
src/b3_alloc_system.egg-info/top_level.txt
src/b3alloc/__init__.py
src/b3alloc/config.py
src/b3alloc/utils_dates.py
src/b3alloc/backtest/__init__.py
src/b3alloc/backtest/analytics.py
src/b3alloc/backtest/engine.py
src/b3alloc/backtest/portfolio_accounting.py
src/b3alloc/bl/__init__.py
src/b3alloc/bl/black_litterman.py
src/b3alloc/bl/confidence.py
src/b3alloc/bl/view_builder.py
src/b3alloc/data/__init__.py
src/b3alloc/data/ingest_fundamentals.py
src/b3alloc/data/ingest_prices.py
src/b3alloc/data/ingest_selic.py
src/b3alloc/factors/__init__.py
src/b3alloc/factors/fama_french_b3.py
src/b3alloc/fx/__init__.py
src/b3alloc/fx/ff_view.py
src/b3alloc/fx/fx_ingest.py
src/b3alloc/optimize/__init__.py
src/b3alloc/optimize/constraints.py
src/b3alloc/optimize/mean_variance.py
src/b3alloc/preprocess/__init__.py
src/b3alloc/preprocess/align.py
src/b3alloc/preprocess/clean.py
src/b3alloc/preprocess/returns.py
src/b3alloc/returns/__init__.py
src/b3alloc/returns/ff_view.py
src/b3alloc/returns/var_view.py
src/b3alloc/risk/__init__.py
src/b3alloc/risk/dcc.py
src/b3alloc/risk/garch.py
src/b3alloc/risk/risk_engine.py
src/b3alloc/risk/shrinkage.py
src/b3alloc/taxes/__init__.py
src/b3alloc/taxes/ledger.py
src/b3alloc/taxes/tax_tracker.py
src/b3alloc/trades/__init__.py
src/b3alloc/trades/trade_calculator.py
src/b3alloc/views/__init__.py
src/b3alloc/views/views_parser.py
src/b3alloc/viz/__init__.py
src/b3alloc/viz/plots_portfolio.py


###############################################################################
### FILE: src/b3_alloc_system.egg-info/requires.txt
###############################################################################
pandas>=2.0.0
numpy>=1.24.0
scipy>=1.10.0
pydantic>=2.0
pyyaml>=6.0
yfinance>=0.2.30
requests>=2.30.0
arch>=5.3.0
statsmodels>=0.14.0
scikit-learn>=1.3.0
cvxpy>=1.3.0
osqp>=0.6.0
holidays>=0.30

[all]
b3_alloc_system[dev]
b3_alloc_system[reporting]

[dev]
jupyterlab>=4.0.0
notebook>=7.0.0
pytest>=7.4.0
ruff>=0.1.0
dask>=2023.9.0
joblib>=1.3.0

[reporting]
matplotlib>=3.7.0
plotly>=5.15.0
bokeh>=3.2.0
nbconvert>=7.8.0
openpyxl>=3.1.0
python-docx>=1.1.0



###############################################################################
### FILE: src/b3_alloc_system.egg-info/top_level.txt
###############################################################################
b3alloc



###############################################################################
### FILE: src/b3alloc/__init__.py
###############################################################################
# This file marks the 'b3alloc' directory as a Python package.
# It can also be used to define package-level variables or import key functions.

__version__ = "0.1.0"


###############################################################################
### FILE: src/b3alloc/backtest/__init__.py
###############################################################################
# This file marks the 'b3alloc' directory as a Python package.
# It can also be used to define package-level variables or import key functions.

__version__ = "0.1.0"


###############################################################################
### FILE: src/b3alloc/backtest/analytics.py
###############################################################################
import pandas as pd
import numpy as np
import statsmodels.api as sm
from typing import Dict

def _calculate_max_drawdown(cumulative_returns: pd.Series) -> tuple[float, pd.Timestamp, pd.Timestamp]:
    """Helper function to calculate the maximum drawdown and its dates."""
    running_max = cumulative_returns.cummax()
    drawdown = (cumulative_returns - running_max) / running_max
    
    max_dd = drawdown.min()
    end_date = drawdown.idxmin()
    start_date = cumulative_returns.index[cumulative_returns.index <= end_date] \
                                   [np.argmax(cumulative_returns.loc[:end_date].values)]
    
    return max_dd, start_date, end_date

def compute_performance_metrics(
    strategy_returns: pd.Series,
    benchmark_returns: pd.Series,
    risk_free_rate: pd.Series,
    periods_per_year: int,
    mar: float = 0.0
) -> pd.Series:
    """
    Computes a comprehensive set of performance and risk analytics for a strategy.

    Args:
        strategy_returns: A pandas Series of the strategy's periodic returns.
        benchmark_returns: A pandas Series of the benchmark's periodic returns.
        risk_free_rate: A pandas Series of the risk-free rate for the same periods.
        periods_per_year: The number of return periods in a year (e.g., 12 for monthly).
        mar: The minimum acceptable return (MAR) for the Sortino ratio calculation.

    Returns:
        A pandas Series containing all calculated performance metrics.
    """
    # --- Data Alignment ---
    data = pd.DataFrame({
        'strategy': strategy_returns,
        'benchmark': benchmark_returns,
        'risk_free': risk_free_rate
    }).dropna()
    
    # --- Cumulative Returns ---
    strategy_cumulative = (1 + data['strategy']).cumprod()
    benchmark_cumulative = (1 + data['benchmark']).cumprod()
    total_return_strategy = strategy_cumulative.iloc[-1] - 1
    total_return_benchmark = benchmark_cumulative.iloc[-1] - 1
    
    # --- Annualized Metrics ---
    num_years = len(data) / periods_per_year
    cagr_strategy = (1 + total_return_strategy)**(1/num_years) - 1
    cagr_benchmark = (1 + total_return_benchmark)**(1/num_years) - 1
    
    vol_strategy = data['strategy'].std() * np.sqrt(periods_per_year)
    vol_benchmark = data['benchmark'].std() * np.sqrt(periods_per_year)
    
    # --- Risk-Adjusted Returns ---
    annualized_rf = data['risk_free'].mean() * periods_per_year
    sharpe_ratio = (cagr_strategy - annualized_rf) / vol_strategy
    
    # Sortino Ratio
    downside_returns = data['strategy'][data['strategy'] < mar]
    downside_deviation = downside_returns.std() * np.sqrt(periods_per_year)
    sortino_ratio = (cagr_strategy - annualized_rf) / downside_deviation if downside_deviation > 0 else np.inf
    
    # --- Drawdown Analysis ---
    max_dd_strategy, dd_start, dd_end = _calculate_max_drawdown(strategy_cumulative)
    calmar_ratio = cagr_strategy / abs(max_dd_strategy)
    
    # --- Regression-Based Metrics (Alpha and Beta) ---
    strategy_excess_ret = data['strategy'] - data['risk_free']
    benchmark_excess_ret = data['benchmark'] - data['risk_free']
    
    X = sm.add_constant(benchmark_excess_ret)
    Y = strategy_excess_ret
    model = sm.OLS(Y, X).fit()
    
    beta = model.params['benchmark']
    # Annualize the alpha (which is a per-period intercept)
    alpha = model.params['const'] * periods_per_year
    
    # Information Ratio (measures consistency of alpha)
    tracking_error = (strategy_excess_ret - benchmark_excess_ret).std() * np.sqrt(periods_per_year)
    information_ratio = alpha / tracking_error if tracking_error > 0 else np.inf
    
    metrics = {
        'Total Return': total_return_strategy,
        'CAGR': cagr_strategy,
        'Annualized Volatility': vol_strategy,
        'Sharpe Ratio': sharpe_ratio,
        'Sortino Ratio': sortino_ratio,
        'Max Drawdown': max_dd_strategy,
        'Calmar Ratio': calmar_ratio,
        'Alpha (annualized)': alpha,
        'Beta': beta,
        'Information Ratio': information_ratio,
        'Skewness': data['strategy'].skew(),
        'Kurtosis': data['strategy'].kurtosis(),
        'Benchmark CAGR': cagr_benchmark,
        'Benchmark Volatility': vol_benchmark
    }
    
    return pd.Series(metrics, name='Performance Metrics')


if __name__ == '__main__':
    print("--- Running Analytics Module Standalone Test ---")

    # --- GIVEN ---
    # Create synthetic data where the strategy clearly outperforms the benchmark
    # with lower volatility and some generated alpha.
    periods = 12 * 5 # 5 years of monthly data
    periods_per_year = 12
    dates = pd.date_range("2018-01-01", periods=periods, freq="MS")
    
    # Benchmark has a 8% CAGR, 20% vol
    bench_ret = np.random.randn(periods) * (0.20 / np.sqrt(periods_per_year)) + (0.08 / periods_per_year)
    
    # Strategy has a 12% CAGR, 15% vol, and an alpha component
    # alpha = 0.04/12 per month; beta = 0.8
    alpha_monthly = 0.04 / periods_per_year
    beta_sim = 0.8
    strat_ret = alpha_monthly + beta_sim * bench_ret + np.random.randn(periods) * (0.05 / np.sqrt(periods_per_year))
    
    # Convert to Series
    strategy_returns = pd.Series(strat_ret, index=dates)
    benchmark_returns = pd.Series(bench_ret, index=dates)
    risk_free_rate = pd.Series(0.02 / periods_per_year, index=dates)

    # --- WHEN ---
    try:
        performance_summary = compute_performance_metrics(
            strategy_returns, benchmark_returns, risk_free_rate, periods_per_year, mar=0.0
        )
        
        # --- THEN ---
        print("\n--- Performance Summary ---")
        print(performance_summary)
        
        # --- Validation ---
        print("\n--- Validation ---")
        assert performance_summary['CAGR'] > performance_summary['Benchmark CAGR']
        print("OK: Strategy CAGR > Benchmark CAGR.")
        
        assert performance_summary['Annualized Volatility'] < performance_summary['Benchmark Volatility']
        print("OK: Strategy Volatility < Benchmark Volatility.")
        
        assert performance_summary['Sharpe Ratio'] > 0.5
        print("OK: Sharpe Ratio is in a reasonable range for a good strategy.")
        
        # Check if estimated alpha and beta are close to the simulation parameters
        assert abs(performance_summary['Alpha (annualized)'] - 0.04) < 0.015
        print(f"OK: Estimated Alpha ({performance_summary['Alpha (annualized)']:.4f}) is close to simulated alpha (0.04).")
        
        assert abs(performance_summary['Beta'] - beta_sim) < 0.1
        print(f"OK: Estimated Beta ({performance_summary['Beta']:.4f}) is close to simulated beta ({beta_sim}).")

        # --- Test with non-zero MAR ---
        print("\n--- Testing with non-zero MAR ---")
        performance_summary_mar = compute_performance_metrics(
            strategy_returns, benchmark_returns, risk_free_rate, periods_per_year, mar=0.01
        )
        assert performance_summary_mar['Sortino Ratio'] != performance_summary['Sortino Ratio']
        print("OK: Sortino Ratio changes with a non-zero MAR.")

    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/backtest/engine.py
###############################################################################
import pandas as pd
import numpy as np
import cvxpy as cp
from typing import Dict, Tuple

from ..config import Config
from ..utils_dates import generate_rebalance_dates
from ..preprocess.clean import get_liquid_universe
from ..risk.risk_engine import build_covariance_matrix
from ..returns.ff_view import create_fama_french_view
from ..returns.var_view import create_var_view
from ..bl.black_litterman import calculate_risk_aversion, calculate_equilibrium_returns, calculate_posterior_returns
from ..bl.view_builder import build_absolute_views
from ..bl.confidence import estimate_view_uncertainty
from ..optimize.mean_variance import run_mean_variance_optimization
from ..optimize.constraints import build_optimizer_constraints
from ..trades.trade_calculator import calculate_target_shares, resolve_fractional_shares, compute_trade_list
from .portfolio_accounting import PortfolioLedger

class BacktestEngine:
    """
    Orchestrates the entire rolling-window backtesting simulation using a
    detailed portfolio ledger for realistic accounting.
    """
    def __init__(self, config: Config, data_stores: Dict[str, pd.DataFrame]):
        self.config = config
        self.data_stores = data_stores
        self.ledger = PortfolioLedger(initial_capital=config.backtest.initial_capital)

    def run_backtest(self) -> pd.DataFrame:
        """
        Executes the main backtesting loop.
        """
        cfg_bt = self.config.backtest
        rebalance_dates = generate_rebalance_dates(cfg_bt.start, cfg_bt.end, cfg_bt.rebalance)
        prices_wide = self.data_stores['prices'].pivot(index='date', columns='ticker', values='adj_close')

        print(f"Starting backtest from {rebalance_dates[0].date()} to {rebalance_dates[-1].date()}...")

        for rebalance_date in rebalance_dates:
            print(f"\n--- Processing Rebalance Date: {rebalance_date.date()} ---")
            
            # --- State at Start of Period ---
            current_prices = prices_wide.loc[rebalance_date]
            portfolio_value_start = self.ledger.get_portfolio_value(current_prices, rebalance_date)
            
            # --- Run Model Pipeline to Get Target Weights ---
            target_weights, diags = self._run_rebalance_pipeline(rebalance_date, current_prices)
            
            if target_weights is None:
                print("  -> Rebalance pipeline failed. No trades will be executed.")
                continue # Skip to next rebalance date, holding current positions
            
            # --- Generate and Execute Trades ---
            # 1. Convert weights to target share counts
            target_shares_df = calculate_target_shares(target_weights, portfolio_value_start, current_prices)
            target_shares_df = resolve_fractional_shares(target_shares_df)
            
            # 2. Compute trade list
            current_holdings = self.ledger.holdings.copy()
            target_holdings = target_shares_df.set_index('ticker')['target_shares']
            trade_list = compute_trade_list(current_holdings, target_holdings)
            
            # 3. Execute trades in the ledger
            if not trade_list.empty:
                print(f"  -> Executing {len(trade_list)} trades...")
                self.ledger.execute_trades(
                    trade_list,
                    current_prices,
                    rebalance_date,
                    cost_per_trade_bps=cfg_bt.costs_bps
                )
            else:
                print("  -> No trades needed to reach target allocation.")

            # Record state at the end of the rebalance day
            self.ledger.record_state(rebalance_date, current_prices)

        print("\nBacktest finished.")
        return self.ledger.get_history_df()

    def _run_rebalance_pipeline(self, rebalance_date: pd.Timestamp, current_prices: pd.Series) -> Tuple[pd.Series, Dict]:
        """
        Executes the full data prep, risk, return, and optimization pipeline.
        """
        cfg = self.config
        lookback_start = rebalance_date - pd.DateOffset(years=cfg.backtest.lookback_years)
        
        # --- Data Slicing and Universe ---
        history_prices = self.data_stores['prices'][
            (self.data_stores['prices']['date'] >= lookback_start) &
            (self.data_stores['prices']['date'] <= rebalance_date)
        ]
        prices_wide = history_prices.pivot(index='date', columns='ticker', values='adj_close')
        volume_wide = history_prices.pivot(index='date', columns='ticker', values='volume')
        
        liquid_universe = get_liquid_universe(
            prices_wide, volume_wide, rebalance_date, lookback_days=365
        )
        if len(liquid_universe) < 2:
            return None, {"error": "Insufficient liquid assets"}
            
        # --- Model Execution ---
        try:
            log_returns = self.data_stores['log_returns'].loc[lookback_start:rebalance_date, liquid_universe]
            
            sigma_final, diags = build_covariance_matrix(log_returns.dropna(), cfg.risk_engine)
            final_universe = sorted(sigma_final.columns.tolist())
            
            # --- View Generation ---
            ff_betas, ff_view = create_fama_french_view(
                self.data_stores['excess_log_returns'].loc[lookback_start:rebalance_date, final_universe],
                self.data_stores['factors'].loc[lookback_start:rebalance_date],
                cfg.return_engine.factor
            )
            var_view, var_diags = create_var_view(log_returns.dropna(), cfg.return_engine.var)
            views = {'ff_view': ff_view, 'var_view': var_view}
            
            market_caps = self.data_stores['market_caps'].loc[rebalance_date, final_universe]
            market_weights = market_caps / market_caps.sum()
            lambda_aversion = calculate_risk_aversion(self.data_stores['market_excess_returns'].loc[:rebalance_date])
            pi_prior = calculate_equilibrium_returns(lambda_aversion, sigma_final, market_weights)
            
            P, Q = build_absolute_views(views, final_universe)
            
            if P is None or Q is None:
                print("  -> No valid views generated. Falling back to prior.")
                mu_posterior = pi_prior
            else:
                Omega = estimate_view_uncertainty(
                    views,
                    {'ff_view': {'res_var': ff_betas['residual_variance']}, 'var_view': var_diags},
                    cfg.black_litterman
                )
                mu_posterior = calculate_posterior_returns(pi_prior, sigma_final, P, Q, Omega, cfg.black_litterman['tau'])

            # --- Optimization ---
            w_var = cp.Variable(len(final_universe))
            constraints = build_optimizer_constraints(w_var, cfg.optimizer, final_universe)
            optimal_weights = run_mean_variance_optimization(mu_posterior, sigma_final, lambda_aversion, constraints)
            
            if optimal_weights is None:
                print("  -> Optimizer failed. Falling back to market weights.")
                optimal_weights = market_weights.copy()
            
            return optimal_weights, diags

        except Exception as e:
            import traceback
            print(f"  -> ERROR during rebalance pipeline: {e}")
            traceback.print_exc()
            return None, {"error": str(e)}

if __name__ == '__main__':
    print("--- Backtest Engine (Ledger-Based) ---")
    print("This module is designed to be driven by a master script like 'run_backtest.py'.")
    print("Its standalone test would be complex, requiring the instantiation of a full")
    print("Config object and a complete 'data_stores' dictionary.")
    print("The individual component modules have their own unit tests.")


###############################################################################
### FILE: src/b3alloc/backtest/portfolio_accounting.py
###############################################################################
import pandas as pd
from typing import Dict, Tuple

class PortfolioLedger:
    """
    Manages the portfolio's state, including holdings, cash, and valuations.

    This class provides a detailed accounting system to track the portfolio's
    composition and value at each time step, serving as the "source of truth"
    for performance measurement and transaction cost analysis.
    """
    def __init__(self, initial_capital: float):
        if initial_capital <= 0:
            raise ValueError("Initial capital must be positive.")
            
        self.initial_capital = initial_capital
        self.cash = initial_capital
        self.holdings = pd.Series(dtype=int, name="shares")
        self.history = []
        self.record_state(pd.Timestamp.min) # Record initial state

    def get_portfolio_value(self, current_prices: pd.Series, date: pd.Timestamp) -> float:
        """Calculates the total market value of the portfolio at a given point in time."""
        market_value_of_holdings = (self.holdings * current_prices.reindex(self.holdings.index).fillna(0)).sum()
        return market_value_of_holdings + self.cash

    def record_state(self, date: pd.Timestamp, prices: pd.Series = None):
        """Records the current state (holdings, cash, value) of the portfolio for a given date."""
        if prices is None:
            portfolio_value = self.initial_capital
        else:
            portfolio_value = self.get_portfolio_value(prices, date)
            
        self.history.append({
            'date': date,
            'portfolio_value': portfolio_value,
            'cash': self.cash,
            'holdings': self.holdings.to_dict()
        })
    
    def get_history_df(self) -> pd.DataFrame:
        """Returns the recorded history as a pandas DataFrame."""
        return pd.DataFrame(self.history).set_index('date')

    def execute_trades(
        self,
        trade_list: pd.DataFrame,
        trade_prices: pd.Series,
        date: pd.Timestamp,
        cost_per_trade_bps: int = 0
    ) -> float:
        """
        Updates portfolio holdings and cash by executing a list of trades.

        Args:
            trade_list: A DataFrame with columns ['ticker', 'delta_shares'].
            trade_prices: A Series of prices at which trades are executed.
            date: The timestamp of the trade execution.
            cost_per_trade_bps: Transaction costs in basis points.

        Returns:
            The total transaction cost incurred.
        """
        total_cost = 0.0
        
        for _, trade in trade_list.iterrows():
            ticker = trade['ticker']
            delta = trade['delta_shares']
            price = trade_prices.get(ticker)
            
            if price is None:
                print(f"Warning: No price found for {ticker} on {date.date()}. Skipping trade.")
                continue

            trade_value = delta * price
            trade_cost = abs(trade_value) * (cost_per_trade_bps / 10000.0)
            
            # Update cash
            self.cash -= trade_value  # Cash decreases on BUY, increases on SELL
            self.cash -= trade_cost
            total_cost += trade_cost
            
            # Update holdings
            current_shares = self.holdings.get(ticker, 0)
            new_shares = current_shares + delta
            
            if new_shares > 0:
                self.holdings[ticker] = new_shares
            elif ticker in self.holdings:
                # Remove ticker if we sold all shares
                self.holdings.drop(ticker, inplace=True)
                
            # After each trade, record the new state
            self.record_state(date, trade_prices)
            
        return total_cost


if __name__ == '__main__':
    print("--- Running Portfolio Accounting Module Standalone Test ---")
    
    # --- GIVEN ---
    initial_pv = 100_000.0
    ledger = PortfolioLedger(initial_capital=initial_pv)
    
    start_date = pd.to_datetime("2023-01-02")
    rebal_date = pd.to_datetime("2023-01-03")
    
    prices_t0 = pd.Series({'PETR4.SA': 30.0, 'VALE3.SA': 70.0})
    prices_t1 = pd.Series({'PETR4.SA': 31.0, 'VALE3.SA': 69.0})
    
    # Initial BUY trades
    initial_trades = pd.DataFrame({
        'ticker': ['PETR4.SA', 'VALE3.SA'],
        'delta_shares': [1000, 500] # Buy 1000 PETR4, 500 VALE3
    })
    
    # --- WHEN ---
    # 1. Execute initial trades
    t_costs1 = ledger.execute_trades(initial_trades, prices_t0, start_date, cost_per_trade_bps=10)
    
    # Check state after first set of trades
    state1 = ledger.history[-1]
    value1 = ledger.get_portfolio_value(prices_t0, start_date)
    
    # 2. On the next day, rebalance
    # Sell 200 PETR4, Buy 100 VALE3
    rebal_trades = pd.DataFrame({
        'ticker': ['PETR4.SA', 'VALE3.SA'],
        'delta_shares': [-200, 100]
    })
    t_costs2 = ledger.execute_trades(rebal_trades, prices_t1, rebal_date, cost_per_trade_bps=10)
    
    state2 = ledger.history[-1]
    value2 = ledger.get_portfolio_value(prices_t1, rebal_date)
    
    # --- THEN ---
    print("\n--- State after initial trades ---")
    print(f"Transaction Costs: R$ {t_costs1:.2f}")
    print(f"Holdings: {state1['holdings']}")
    print(f"Cash: R$ {state1['cash']:,.2f}")
    print(f"Portfolio Value: R$ {value1:,.2f}")
    
    # Validation 1
    expected_petr4_value = 1000 * 30.0
    expected_vale3_value = 500 * 70.0
    expected_total_value = expected_petr4_value + expected_vale3_value
    expected_costs = expected_total_value * 0.0010
    assert np.isclose(t_costs1, expected_costs)
    assert np.isclose(state1['cash'], initial_pv - expected_total_value - expected_costs)
    
    print("\n--- State after rebalance ---")
    print(f"Transaction Costs: R$ {t_costs2:.2f}")
    print(f"Holdings: {state2['holdings']}")
    print(f"Cash: R$ {state2['cash']:,.2f}")
    print(f"Portfolio Value: R$ {value2:,.2f}")
    
    # Validation 2
    # New holdings should be 800 PETR4, 600 VALE3
    assert state2['holdings']['PETR4.SA'] == 800
    assert state2['holdings']['VALE3.SA'] == 600
    # Check portfolio value
    expected_value2 = (800 * 31.0) + (600 * 69.0) + state2['cash']
    assert np.isclose(value2, expected_value2)
    
    print("\nOK: Portfolio ledger correctly tracks holdings, cash, and value across trades.")

    # --- Test history recording ---
    print("\n--- Testing history recording ---")
    assert len(ledger.history) == 5, "Expected 5 history records (initial + 2 buys + 2 rebal trades)."
    print("OK: History is being recorded for each transaction.")


###############################################################################
### FILE: src/b3alloc/bl/__init__.py
###############################################################################
# This file marks the 'b3alloc' directory as a Python package.
# It can also be used to define package-level variables or import key functions.

__version__ = "0.1.0"


###############################################################################
### FILE: src/b3alloc/bl/black_litterman.py
###############################################################################
import pandas as pd
import numpy as np
from typing import Tuple, Dict

from ..config import BlackLittermanConfig

def calculate_risk_aversion(market_excess_returns: pd.Series) -> float:
    """
    Calculates the market-implied risk aversion parameter (lambda) from historical
    market data. Lambda = Market Sharpe Ratio / Market Volatility.

    This simplifies to: E[R_m - R_f] / Var(R_m - R_f)

    Args:
        market_excess_returns: A Series of historical daily market excess returns.

    Returns:
        The scalar risk aversion parameter, lambda.
    """
    if market_excess_returns.empty:
        raise ValueError("Market excess returns cannot be empty.")
        
    mean_excess_return = market_excess_returns.mean()
    var_excess_return = market_excess_returns.var()
    
    if var_excess_return < 1e-12:
        # Handle case of zero variance to avoid division by zero
        return 2.0 # Return a default, sensible value
        
    risk_aversion = mean_excess_return / var_excess_return
    return risk_aversion

def calculate_equilibrium_returns(
    risk_aversion: float,
    sigma_matrix: pd.DataFrame,
    market_cap_weights: pd.Series
) -> pd.Series:
    """
    Calculates the market equilibrium prior returns (Pi) based on reverse optimization.

    Formula: Pi = lambda * Sigma * w_mkt

    Args:
        risk_aversion: The scalar risk aversion parameter (lambda).
        sigma_matrix: The NxN covariance matrix of asset returns.
        market_cap_weights: A Series of market-cap weights for the assets.

    Returns:
        A Series of annualized equilibrium expected excess returns.
    """
    # Align weights and sigma matrix
    aligned_weights = market_cap_weights.reindex(sigma_matrix.columns).fillna(0)
    
    pi_vector = risk_aversion * sigma_matrix.dot(aligned_weights)
    pi_series = pd.Series(pi_vector, index=sigma_matrix.columns, name="pi_prior")
    
    return pi_series

def calculate_posterior_returns(
    mu_prior: pd.Series,
    sigma_matrix: pd.DataFrame,
    p_matrix: np.ndarray,
    q_vector: np.ndarray,
    omega_matrix: np.ndarray,
    tau: float
) -> pd.Series:
    """
    Calculates the Black-Litterman posterior expected returns (mu_BL).

    This function implements the core BL formula using the more stable matrix form:
    mu_post = inv(inv(tau*Sigma) + P'*inv(Omega)*P) * (inv(tau*Sigma)*Pi + P'*inv(Omega)*Q)

    Args:
        mu_prior: The vector of prior equilibrium returns (Pi).
        sigma_matrix: The covariance matrix of asset returns (Sigma).
        p_matrix: The pick/link matrix for views (P).
        q_vector: The vector of view returns (Q).
        omega_matrix: The diagonal covariance matrix of view errors (Omega).
        tau: A scalar controlling the uncertainty of the prior (tau).

    Returns:
        A Series containing the final posterior expected excess returns.
    """
    print("Calculating Black-Litterman posterior returns...")
    # Ensure consistent ordering
    asset_names = mu_prior.index
    sigma_matrix = sigma_matrix.reindex(index=asset_names, columns=asset_names)
    
    # 1. Pre-compute inverses for stability and clarity
    # Add a small jitter to Omega's diagonal to ensure invertibility, just in case
    omega_inv = np.linalg.inv(omega_matrix + np.eye(omega_matrix.shape[0]) * 1e-12)
    
    tau_sigma = tau * sigma_matrix.values
    tau_sigma_inv = np.linalg.inv(tau_sigma)
    
    # 2. Calculate the two main terms of the BL formula
    # First term: The precision (inverse covariance) of the posterior distribution
    posterior_precision = tau_sigma_inv + p_matrix.T @ omega_inv @ p_matrix

    # Second term: The weighted average of the prior and the views
    weighted_priors_and_views = (tau_sigma_inv @ mu_prior.values.reshape(-1, 1)) + (p_matrix.T @ omega_inv @ q_vector)
    
    # 3. Calculate the posterior mean
    posterior_mean_vector = np.linalg.inv(posterior_precision) @ weighted_priors_and_views
    
    # 4. Format the output
    mu_posterior = pd.Series(
        posterior_mean_vector.flatten(), 
        index=asset_names,
        name="mu_posterior"
    )
    
    print("Successfully calculated posterior returns.")
    return mu_posterior

if __name__ == '__main__':
    print("--- Running Black-Litterman Module Standalone Test ---")

    # --- GIVEN ---
    # 1. A 2-asset universe
    tickers = ['ASSET_A', 'ASSET_B']
    sigma = pd.DataFrame(
        [[0.02, 0.01], [0.01, 0.03]], 
        columns=tickers, index=tickers
    )
    market_caps = pd.Series([0.6, 0.4], index=tickers)
    market_returns = pd.Series(np.random.randn(252) * 0.01 + 0.0005) # Dummy history
    
    # 2. Hyperparameters
    tau = 0.05
    
    # 3. A single, absolute view on Asset A
    # "Asset A will have an annualized excess return of 10%"
    P = np.array([[1.0, 0.0]]) # Picks Asset A
    Q = np.array([[0.10]])      # The view value
    
    # We are very confident in this view, so Omega has a small variance
    Omega = np.array([[0.001]])

    try:
        # --- WHEN ---
        # A. Calculate intermediate inputs
        lmbda = calculate_risk_aversion(market_returns)
        pi = calculate_equilibrium_returns(lmbda, sigma, market_caps)
        
        # B. Calculate the final posterior
        mu_bl = calculate_posterior_returns(pi, sigma, P, Q, Omega, tau)
        
        # --- THEN ---
        print("\n--- Inputs to BL ---")
        print(f"Calculated Lambda: {lmbda:.4f}")
        print("Prior Returns (Pi):\n", pi)
        
        print("\n--- Output from BL ---")
        print("Posterior Returns (mu_BL):\n", mu_bl)
        
        # --- Validation ---
        # The posterior return for Asset A should have moved from its prior (pi)
        # towards the view (Q=0.10).
        pi_A = pi['ASSET_A']
        mu_bl_A = mu_bl['ASSET_A']
        view_Q = Q[0,0]
        
        print(f"\nPrior for Asset A:      {pi_A:.4%}")
        print(f"View for Asset A:       {view_Q:.4%}")
        print(f"Posterior for Asset A:  {mu_bl_A:.4%}")
        
        # Check if the posterior is between the prior and the view
        assert (mu_bl_A > pi_A and mu_bl_A < view_Q) or \
               (mu_bl_A < pi_A and mu_bl_A > view_Q)
        print("\nOK: Posterior for Asset A has shifted from the prior towards the view.")

        # The posterior for Asset B should also have shifted due to its covariance with A.
        pi_B = pi['ASSET_B']
        mu_bl_B = mu_bl['ASSET_B']
        
        assert not np.isclose(pi_B, mu_bl_B)
        print("OK: Posterior for Asset B has also been updated.")

    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/bl/confidence.py
###############################################################################
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional

from ..config import BlackLittermanConfig

def _get_ff_view_variances(
    ff_view: pd.Series,
    ff_diagnostics: Dict,
    config: BlackLittermanConfig
) -> pd.Series:
    """Calculates variances for the Fama-French model views."""
    if config.confidence.method == "rmse_based":
        if 'residual_variance' not in ff_diagnostics:
            raise ValueError("RMSE-based confidence requires 'residual_variance' in ff_view diagnostics.")
        res_var = ff_diagnostics['residual_variance']
        variances = res_var.reindex(ff_view.index).fillna(res_var.mean())
    else: # user_scaled
        variances = pd.Series(np.var(ff_view), index=ff_view.index)
        
    return variances * config.confidence.factor_scaler

def _get_var_view_variances(
    var_view: pd.Series,
    var_diagnostics: Dict,
    config: BlackLittermanConfig
) -> pd.Series:
    """Calculates variances for the VAR model views."""
    if config.confidence.method == "rmse_based":
        if 'sigma_u' not in var_diagnostics:
             raise ValueError("RMSE-based confidence requires 'sigma_u' in var_view diagnostics.")
        sigma_u = var_diagnostics['sigma_u']
        variances = pd.Series(np.diag(sigma_u), index=var_view.index)
    else: # user_scaled
        variances = pd.Series(np.var(var_view), index=var_view.index)
        
    return variances * config.confidence.var_scaler

def _get_qualitative_view_variances(
    qualitative_views: List[Dict],
    p_matrix_qual: np.ndarray,
    sigma: pd.DataFrame
) -> np.ndarray:
    """
    Calculates variances for qualitative views based on specified confidence.
    Follows the common Idzorek (2005) method where uncertainty is proportional
    to the variance of the portfolio defined by the view.
    """
    variances = []
    for i, view in enumerate(qualitative_views):
        p_row = p_matrix_qual[i, :]
        variance_of_view_portfolio = p_row.T @ sigma.values @ p_row
        
        # Heuristic: Confidence of 1.0 = variance of view.
        # Confidence of 0.0 = infinite variance.
        confidence = view.get('confidence', 0.5) # Default to 50% confidence
        if confidence <= 0 or confidence >= 1:
            raise ValueError("Qualitative view confidence must be between 0 and 1.")
            
        # The less confident, the higher the variance
        # This is one of many possible heuristics.
        uncertainty = variance_of_view_portfolio / confidence
        variances.append(uncertainty)
        
    return np.array(variances)


def estimate_view_uncertainty(
    model_views: Dict[str, pd.Series],
    qualitative_views: Optional[List[Dict]],
    view_diagnostics: Dict[str, Dict],
    config: BlackLittermanConfig,
    p_matrix_qual: Optional[np.ndarray],
    sigma_matrix: pd.DataFrame
) -> np.ndarray:
    """
    Estimates the complete view uncertainty matrix (Omega) for all view types.
    """
    print(f"Estimating view uncertainty (Omega) using '{config.confidence.method}' method...")
    
    all_variances = []
    
    # --- Model-driven view uncertainty ---
    if 'ff_view' in model_views and model_views['ff_view'] is not None:
        ff_vars = _get_ff_view_variances(model_views['ff_view'], view_diagnostics.get('ff_view', {}), config)
        all_variances.append(ff_vars)

    if 'var_view' in model_views and model_views['var_view'] is not None:
        var_vars = _get_var_view_variances(model_views['var_view'], view_diagnostics.get('var_view', {}), config)
        all_variances.append(var_vars)

    # --- Qualitative view uncertainty ---
    if qualitative_views and p_matrix_qual is not None and p_matrix_qual.shape[0] > 0:
        qual_vars = _get_qualitative_view_variances(qualitative_views, p_matrix_qual, sigma_matrix)
        all_variances.append(pd.Series(qual_vars))

    if not all_variances:
        raise ValueError("No views provided to estimate uncertainty.")
        
    final_variances = pd.concat(all_variances)
    omega_matrix = np.diag(final_variances.values)

    print(f"Successfully constructed Omega matrix with shape {omega_matrix.shape}.")
    return omega_matrix


if __name__ == '__main__':
    # Use a mock/in-memory config instead of file I/O to simplify the test.
    from pydantic import BaseModel

    class MockConfidenceConfig(BaseModel):
        method: str = "rmse_based"
        factor_scaler: float = 1.0
        var_scaler: float = 2.0

    class MockBLConfig(BaseModel):
        tau: float = 0.05
        confidence: MockConfidenceConfig = MockConfidenceConfig()

    config = MockBLConfig()

    # --- Test Data ---
    tickers = ['ASSET_A', 'ASSET_B', 'ASSET_C']
    
    # Dummy views
    ff_view = pd.Series([0.08, 0.06, 0.05], index=tickers)
    var_view = pd.Series([0.07, 0.09, 0.08], index=tickers)
    model_views = {'ff_view': ff_view, 'var_view': var_view}
    
    # Dummy diagnostics for model views
    diagnostics = {
        'ff_view': {'residual_variance': pd.Series([0.005, 0.004, 0.0045], index=tickers)},
        'var_view': {'sigma_u': np.array([[0.003, 0.001, 0.001], 
                                          [0.001, 0.002, 0.001],
                                          [0.001, 0.001, 0.0025]])}
    }

    # Dummy qualitative views and associated matrices
    qual_views = [
        {'view': {'type': 'absolute', 'ticker': 'ASSET_A', 'expected_return': 0.10, 'confidence': 0.7}},
        {'view': {'type': 'relative', 'ticker1': 'ASSET_B', 'ticker2': 'ASSET_C', 'expected_difference': 0.02, 'confidence': 0.5}}
    ]
    # P matrix would be generated by ViewBuilder, we mock it here
    p_matrix_qual = np.array([
        [1, 0, 0],
        [0, 1, -1]
    ])
    sigma_matrix = pd.DataFrame(diagnostics['var_view']['sigma_u'], index=tickers, columns=tickers)
    
    print("--- Running Confidence Module Standalone Test ---")
    
    try:
        omega = estimate_view_uncertainty(
            model_views, 
            qual_views, 
            diagnostics, 
            config, 
            p_matrix_qual, 
            sigma_matrix
        )
        
        print("\n--- Output Omega Matrix (Combined) ---")
        print(omega)
        
        # Validation
        num_model_views = sum(v.size for v in model_views.values())
        num_qual_views = len(qual_views)
        expected_size = num_model_views + num_qual_views
        
        assert omega.shape == (expected_size, expected_size), f"Omega matrix shape should be ({expected_size}, {expected_size})"
        assert np.count_nonzero(omega - np.diag(np.diagonal(omega))) == 0, "Omega must be diagonal."
        
        # Check that variances are positive
        assert np.all(np.diagonal(omega) > 0), "All diagonal elements of Omega must be positive."

        print(f"\nOK: Combined Omega matrix ({expected_size}x{expected_size}) looks correct.")

        # Test with only model views
        omega_models_only = estimate_view_uncertainty(model_views, None, diagnostics, config, None, sigma_matrix)
        assert omega_models_only.shape == (num_model_views, num_model_views), "Model-only Omega shape is incorrect."
        print("\nOK: Model-only Omega matrix is correctly constructed.")

    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/bl/view_builder.py
###############################################################################
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any, Optional

from ..views.views_parser import parse_qualitative_views

def build_absolute_views(
    model_views: Dict[str, pd.Series],
    universe: List[str]
) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
    """
    Constructs the Black-Litterman P and Q matrices for absolute, model-driven views.
    (This function remains largely the same but is now a component of the full builder).
    """
    # Ensure the universe is sorted for consistent matrix construction
    universe = sorted(universe)

    p_matrices = []
    q_vectors = []
    num_assets = len(universe)
    
    for view_name, view_series in model_views.items():
        if view_series is None or view_series.empty:
            continue

        aligned_view = view_series.reindex(universe)
        if aligned_view.isnull().any():
            missing = aligned_view[aligned_view.isnull()].index.tolist()
            raise ValueError(f"Model view '{view_name}' is missing forecasts for: {missing}")

        p_matrices.append(np.eye(num_assets))
        q_vectors.append(aligned_view.values.reshape(-1, 1))

    if not p_matrices:
        return None, None

    return np.vstack(p_matrices), np.vstack(q_vectors)


def build_full_view_matrices(
    model_views: Dict[str, pd.Series],
    qualitative_view_defs: Optional[List[Dict[str, Any]]],
    universe: List[str]
) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
    """
    Orchestrates the construction of the complete P and Q matrices from all
    view sources (model-based and qualitative).

    Args:
        model_views: Dictionary of programmatic views (e.g., from FF and VAR models).
        qualitative_view_defs: A list of qualitative view dictionaries from a config file.
        universe: A sorted list of all ticker symbols in the investment universe.

    Returns:
        A tuple containing the final, stacked P and Q matrices.
    """
    print("Building full P and Q matrices from all view sources...")
    all_p = []
    all_q = []

    # 1. Process model-driven absolute views
    P_model, Q_model = build_absolute_views(model_views, universe)
    if P_model is not None and Q_model is not None:
        print(f"  -> Added {P_model.shape[0]} model-driven views.")
        all_p.append(P_model)
        all_q.append(Q_model)

    # 2. Process user-defined qualitative views
    if qualitative_view_defs:
        P_qual, Q_qual = parse_qualitative_views(qualitative_view_defs, universe)
        if P_qual.shape[0] > 0:
            print(f"  -> Added {P_qual.shape[0]} qualitative views.")
            all_p.append(P_qual)
            all_q.append(Q_qual)

    if not all_p:
        print("Warning: No valid views were processed. Returning empty matrices.")
        return None, None

    # 3. Stack all matrices together
    P_final = np.vstack(all_p)
    Q_final = np.vstack(all_q)
    
    print(f"Successfully constructed final P matrix with shape {P_final.shape} and Q vector with shape {Q_final.shape}.")
    return P_final, Q_final


if __name__ == '__main__':
    print("--- Running Full View Builder Module Standalone Test ---")

    # --- GIVEN ---
    test_universe = sorted(['PETR4.SA', 'VALE3.SA', 'ITUB4.SA'])
    
    # 1. Model views
    model_views_test = {
        'ff_view': pd.Series({'PETR4.SA': 0.10, 'VALE3.SA': 0.12, 'ITUB4.SA': 0.08})
    }
    
    # 2. Qualitative views
    qual_views_test = [
        {'type': 'relative', 'expr': 'VALE3.SA – PETR4.SA', 'magnitude': 0.03, 'confidence': 0.7},
        {'type': 'absolute', 'expr': 'ITUB4.SA', 'magnitude': 0.09, 'confidence': 0.9}
    ]
    
    # --- WHEN ---
    P, Q = build_full_view_matrices(model_views_test, qual_views_test, test_universe)
    
    # --- THEN ---
    print("\nFinal P Matrix (Stacked):\n", P)
    print("\nFinal Q Vector (Stacked):\n", Q)
    
    # --- Validation ---
    # Universe order should be sorted: ITUB4.SA, PETR4.SA, VALE3.SA
    
    # Expected P from models (3 views)
    P_exp_model = np.eye(3)
    
    # Expected P from qualitative views (2 views)
    P_exp_qual = np.array([
        [0., -1., 1.], # VALE3 - PETR4
        [1.,  0., 0.]  # ITUB4
    ])
    
    expected_P = np.vstack([P_exp_model, P_exp_qual])
    
    # Expected Q from models
    # Reordered: ITUB4=0.08, PETR4=0.10, VALE3=0.12
    Q_exp_model = np.array([[0.08], [0.10], [0.12]])
    
    # Expected Q from qualitative views
    Q_exp_qual = np.array([[0.03], [0.09]])
    
    expected_Q = np.vstack([Q_exp_model, Q_exp_qual])

    assert P.shape == (5, 3), "Final P matrix has incorrect shape."
    assert Q.shape == (5, 1), "Final Q vector has incorrect shape."
    
    assert np.allclose(P, expected_P), "Final P matrix content is incorrect."
    assert np.allclose(Q, expected_Q), "Final Q vector content is incorrect."
    
    print("\nOK: Model-driven and qualitative views were correctly stacked into final P and Q.")

    # --- Test with unsorted universe ---
    print("\n--- Testing with unsorted universe ---")
    unsorted_universe = ['PETR4.SA', 'VALE3.SA', 'ITUB4.SA']
    P_unsorted, Q_unsorted = build_full_view_matrices(model_views_test, qual_views_test, unsorted_universe)
    assert np.allclose(P_unsorted, expected_P), "P matrix is incorrect when universe is unsorted."
    assert np.allclose(Q_unsorted, expected_Q), "Q vector is incorrect when universe is unsorted."
    print("OK: View builder correctly handles unsorted universe input.")


###############################################################################
### FILE: src/b3alloc/config.py
###############################################################################
from pathlib import Path
from typing import Literal, Optional, Dict, Any

import yaml
from pydantic import BaseModel, Field, field_validator

# Pydantic models provide data validation, type hints, and a clear structure
# that mirrors the YAML configuration files specified in the project blueprint.

# --- Nested Models for Configuration Sections ---

class DataConfig(BaseModel):
    """Configuration for data sources and parameters."""
    start: str
    end: str
    tickers_file: str
    selic_series: int
    publish_lag_days: int = Field(..., gt=0) # Must be a positive integer

class GarchConfig(BaseModel):
    """Configuration for GARCH model in the Risk Engine."""
    dist: str = "student-t"  # Changed from Literal["gaussian", "student-t"]
    min_obs: int = Field(252, ge=252)
    refit_freq_days: int = Field(21, gt=0)

class DccConfig(BaseModel):
    """Configuration for DCC model in the Risk Engine."""
    a_init: float = Field(..., gt=0, lt=1)
    b_init: float = Field(..., gt=0, lt=1)

class ShrinkageConfig(BaseModel):
    """Configuration for covariance shrinkage."""
    method: Literal["ledoit_wolf_constant_corr", "manual"]
    floor: float = Field(0.0, ge=0, lt=1)

class RiskEngineConfig(BaseModel):
    """Configuration group for the entire Risk Engine."""
    garch: GarchConfig
    dcc: DccConfig
    shrinkage: ShrinkageConfig

class FactorViewConfig(BaseModel):
    """Configuration for the Fama-French return view."""
    lookback_days: int = Field(..., gt=252)
    include_alpha: bool
    premium_estimator: Literal["long_term_mean", "expanding_mean", "ewm"]

class VarViewConfig(BaseModel):
    """Configuration for the VAR return view."""
    max_lag: int = Field(..., gt=0, le=20)
    criterion: Literal["aic", "bic"]
    log_returns: bool

class ReturnEngineConfig(BaseModel):
    """Configuration group for the Return Engine."""
    factor: FactorViewConfig
    var: VarViewConfig

class BLConfidenceConfig(BaseModel):
    """Configuration for Black-Litterman view confidences (Omega matrix)."""
    method: Literal["rmse_based", "user_scaled"]
    factor_scaler: float = Field(1.0, gt=0)
    var_scaler: float = Field(1.0, gt=0)

class BlackLittermanConfig(BaseModel):
    """Configuration for the Black-Litterman synthesizer."""
    tau: float = Field(..., gt=0, lt=1)
    confidence: BLConfidenceConfig
    # Per Amendment 1
    qualitative_views_file: Optional[str] = None

class OptimizerConfig(BaseModel):
    """Configuration for the portfolio optimizer."""
    objective: Literal["max_sharpe", "min_variance", "target_return", "target_vol"]
    long_only: bool
    name_cap: float = Field(..., ge=0.01, le=1.0)
    sector_cap: float = Field(..., ge=0.01, le=1.0)
    turnover_penalty_bps: int = Field(..., ge=0)

class BacktestConfig(BaseModel):
    """Configuration for the backtesting engine."""
    lookback_years: int = Field(..., gt=1)
    rebalance: Literal["monthly", "quarterly"]
    start: str
    end: str
    costs_bps: int = Field(..., ge=0)
    # Per Amendment 1
    initial_capital: float = Field(1_000_000.0, gt=0)


# --- Models from Amendment 1 ---

class MetaConfig(BaseModel):
    """Configuration for portfolio metadata."""
    portfolio_name: str
    base_currency: Literal["BRL"]

class UniverseConfig(BaseModel):
    """Configuration for asset universe and FX factor handling."""
    include_fx_factor: bool = False
    fx_series_id: int = 1
    asset_flags: Dict[str, Dict[str, Any]] = {}

class TaxConfig(BaseModel):
    """Configuration for Brazilian tax calculation."""
    enable: bool = False
    brokerage_fee_bps: int = Field(0, ge=0)

# --- Top-Level Configuration Model ---

class Config(BaseModel):
    """The main configuration object, loading all sub-sections."""
    meta: MetaConfig
    data: DataConfig
    universe: UniverseConfig
    risk_engine: RiskEngineConfig
    return_engine: ReturnEngineConfig
    black_litterman: BlackLittermanConfig
    optimizer: OptimizerConfig
    tax: TaxConfig
    backtest: BacktestConfig


def load_config(config_path: str | Path) -> Config:
    """
    Loads and validates the master YAML configuration file.

    Args:
        config_path: Path to the YAML file.

    Returns:
        A validated Config object.
    """
    path = Path(config_path)
    if not path.is_file():
        raise FileNotFoundError(f"Configuration file not found at: {path}")

    with open(path, 'r') as f:
        config_dict = yaml.safe_load(f)

    return Config(**config_dict)


if __name__ == '__main__':
    # This block is for demonstrating and testing the config loading.
    # It requires creating a sample YAML file.

    # 1. Create a dummy YAML for testing, now including amended fields
    dummy_yaml_content = """
meta:
  portfolio_name: Test_Portfolio_A
  base_currency: BRL

data:
  start: 2010-01-01
  end: 2025-01-01
  tickers_file: config/universe_small.csv
  selic_series: 11
  publish_lag_days: 3

universe:
  include_fx_factor: true
  fx_series_id: 1
  asset_flags:
    PETR4.SA: {fx_sensitive: false}
    AAPL34.SA: {fx_sensitive: true}

risk_engine:
  garch:
    dist: gaussian
    min_obs: 500
    refit_freq_days: 21
  dcc:
    a_init: 0.02
    b_init: 0.97
  shrinkage:
    method: ledoit_wolf_constant_corr
    floor: 0.05

return_engine:
  factor:
    lookback_days: 756
    include_alpha: false
    premium_estimator: long_term_mean
  var:
    max_lag: 5
    criterion: bic
    log_returns: true

black_litterman:
  tau: 0.05
  confidence:
    method: rmse_based
    factor_scaler: 1.0
    var_scaler: 0.5
  qualitative_views_file: views/qualitative_views_A.yaml

optimizer:
  objective: max_sharpe
  long_only: true
  name_cap: 0.10
  sector_cap: 0.25
  turnover_penalty_bps: 5

tax:
  enable: true
  brokerage_fee_bps: 5

backtest:
  lookback_years: 5
  rebalance: monthly
  start: 2012-01-01
  end: 2025-01-01
  costs_bps: 10
  initial_capital: 500000.0
"""
    # Create a temporary directory and file
    temp_dir = Path("./temp_config")
    temp_dir.mkdir(exist_ok=True)
    dummy_config_path = temp_dir / "test_config.yaml"
    with open(dummy_config_path, "w") as f:
        f.write(dummy_yaml_content)

    # 2. Test loading the config
    print("--- Loading Test Config ---")
    try:
        config = load_config(dummy_config_path)
        print("Configuration loaded and validated successfully!")
        
        # 3. Access nested parameters, including new ones
        print("\n--- Accessing Parameters ---")
        print(f"Portfolio Name: {config.meta.portfolio_name}")
        print(f"FX Factor Enabled: {config.universe.include_fx_factor}")
        print(f"Tax Calculation Enabled: {config.tax.enable}")
        print(f"Qualitative Views File: {config.black_litterman.qualitative_views_file}")
        print(f"Initial Capital for Backtest: {config.backtest.initial_capital}")

    except Exception as e:
        print(f"An error occurred: {e}")

    finally:
        # Clean up the dummy file and directory
        import shutil
        if temp_dir.exists():
            shutil.rmtree(temp_dir)
        print("\nCleaned up temporary config files.")


###############################################################################
### FILE: src/b3alloc/data/__init__.py
###############################################################################
# This file marks the 'b3alloc' directory as a Python package.
# It can also be used to define package-level variables or import key functions.

from .ingest_prices import create_equity_price_series, create_index_series
from .ingest_fundamentals import create_fundamentals_series
from .ingest_selic import create_risk_free_series
from .ingest_fx import create_fx_series

__version__ = "0.1.0"


###############################################################################
### FILE: src/b3alloc/data/ingest_fundamentals.py
###############################################################################
import pandas as pd
import brfinance as brf
import time
from typing import List, Optional
import numpy as np
import functools         # ← add this
import yfinance as yf
import requests
from bs4 import BeautifulSoup

from brfinance import CVMAsyncBackend             # NEW (brfinance ≥ 1.2.0)
import logging

import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)   # ADD

yf.enable_debug_mode()
logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(message)s")

@functools.lru_cache(maxsize=128)
def _get_cvm_code_from_b3(ticker: str) -> Optional[str]:
   """
   Resolve a B3 ticker (e.g. 'PETR4') to its CVM code via the nightly CSV:
     https://arquivos.b3.com.br/emissores/EmissoresListados.csv
   """
   base = ticker.split(".")[0].upper()
   url = "https://arquivos.b3.com.br/emissores/EmissoresListados.csv"
   try:
       resp = requests.get(url, timeout=30)
       resp.raise_for_status()
       for line in resp.text.splitlines():
           cols = [c.strip() for c in line.split(";")]
           # CSV columns: Ticker;CodCVM;...
           if len(cols) >= 2 and cols[0].upper() == base and cols[1].isdigit():
               return cols[1]
   except Exception as e:
       logging.warning(f"Failed to fetch CVM code for {base} from CSV: {e}")
   logging.warning(f"CVM code not found for {base} via CSV.")
   return None


def _fetch_shares_outstanding_from_cvm(ticker_sa: str) -> Optional[pd.DataFrame]:
    """
    Return a time–series DataFrame indexed by fiscal_period_end with one column:
        shares_outstanding  (float)

    Data source: CVM “shares” canvas, downloaded through brfinance ≥ 1.1.0.
    Falls back to None if the company is missing or the canvas is empty.
    """
    cvm_backend = CVMAsyncBackend()            # brfinance async helper
    cleaned_ticker = ticker_sa.split(".")[0]  # 'PETR4.SA' ➜ 'PETR4'

    try:
        # 1) map B3 ticker → CVM code (one cheap web‑scrape; cached per run)
        cvm_code = _get_cvm_code_from_b3(cleaned_ticker)

        if cvm_code is None:
            logging.warning(f"CVM code not found for {ticker_sa}.")
            return None

        # 2) download & convert to DataFrame
        df_cvm = cvm_backend.get_canvas_df(
            company_code=cvm_code,
            canvas="shares",
            from_year=2010,
        )
        if df_cvm is None or df_cvm.empty:
            logging.warning(f"CVM shares canvas empty for {ticker_sa}.")
            return None

        # 3) normalise column names (they come in Portuguese)
        df_cvm = (
            df_cvm.rename(
                columns={
                    "dataFimExercicioSocial": "fiscal_period_end",
                    "quantidadeAcoes": "shares_outstanding",
                }
            )
            .loc[:, ["fiscal_period_end", "shares_outstanding"]]
            .dropna()
        )

        df_cvm["fiscal_period_end"] = pd.to_datetime(df_cvm["fiscal_period_end"])
        df_cvm["shares_outstanding"] = (
            df_cvm["shares_outstanding"].astype(str).str.replace("[^0-9]", "", regex=True).astype(float)
        )

        # keep the last filing of each period
        df_cvm = df_cvm.drop_duplicates(subset="fiscal_period_end", keep="last")
        return df_cvm.set_index("fiscal_period_end")

    except Exception as e:
        logging.warning(f"CVM fetch error for {ticker_sa}: {e}")
        return None




def _fetch_single_ticker_fundamentals(ticker_sa: str) -> pd.DataFrame:
    """
    Return a quarterly DataFrame with columns:
        fiscal_period_end | book_equity | shares_outstanding | ticker | publish_date | book_per_share
    Uses yfinance for book equity, CVM for shares; degrades gracefully.

    The function never raises – it returns an empty DataFrame if it truly fails.
    """
    tkr = yf.Ticker(ticker_sa)

    try:
        # ---------- 1) BOOK EQUITY ----------
        def _extract_equity(df: pd.DataFrame) -> pd.DataFrame:
            logging.info(f"Columns returned for {ticker_sa}: {list(df.columns)}")

            if df.empty:
                return pd.DataFrame()

            equity_cols = [
                c for c in df.columns
                if any(key in c.lower() for key in ("equity", "patrim", "total stockholder"))
            ]
            if not equity_cols:
                return pd.DataFrame()

            equity_col = equity_cols[0]
            logging.info(f"Using equity column: {equity_col} for {ticker_sa}")

            out = df[[equity_col]].rename(columns={equity_col: "book_equity"})  # ← NO .T here
            out.index.name = "fiscal_period_end"
            return out


        # Try quarterly first, then annual.
        book_df = _extract_equity(tkr.quarterly_balance_sheet.T)
        if book_df.empty:
            book_df = _extract_equity(tkr.balance_sheet.T)

        # Ultimate fallback – single point from the `info` dict
        if book_df.empty:
            equity_now = tkr.info.get("totalStockholderEquity")
            if equity_now is not None:
                book_df = pd.DataFrame(
                    {
                        "fiscal_period_end": [pd.Timestamp.today().normalize()],
                        "book_equity": [equity_now],
                    }
                )
            else:
                logging.warning(f"Book equity not found for {ticker_sa}.")
                return pd.DataFrame()

        # ---------- 2) SHARES OUTSTANDING ----------
        shares_df = _fetch_shares_outstanding_from_cvm(ticker_sa)
        if shares_df is not None and not shares_df.empty:
            fundamentals = pd.merge(
                book_df.reset_index(), shares_df.reset_index(),
                how="left", on="fiscal_period_end"
            )
            fundamentals["shares_outstanding"].ffill(inplace=True)
        else:
            current_sh = tkr.info.get("sharesOutstanding", np.nan)
            fundamentals = book_df.reset_index().assign(shares_outstanding=current_sh)

        # ---------- 3) FINAL FORMATTING ----------
        fundamentals["ticker"] = ticker_sa
        fundamentals["publish_date"] = pd.NaT
        fundamentals["book_per_share"] = (
            fundamentals["book_equity"] / fundamentals["shares_outstanding"]
        )
        fundamentals.dropna(subset=["book_equity", "shares_outstanding"], inplace=True)
        return fundamentals

    except Exception as e:
        logging.warning(f"Fundamentals fetch failed for {ticker_sa}: {e}")
        return pd.DataFrame()


def create_fundamentals_series(
    tickers: List[str]
) -> pd.DataFrame:
    """
    Orchestrates downloading and compiling quarterly fundamentals for a list of tickers.
    """
    all_fundamentals = []
    print(f"Fetching fundamentals for {len(tickers)} tickers from invest.com...")
    
    for i, ticker in enumerate(tickers):
        time.sleep(1) # Polite scraping
        print(f"({i+1}/{len(tickers)}) Fetching {ticker}...")
        
        ticker_df = _fetch_single_ticker_fundamentals(ticker)
        if not ticker_df.empty:
            all_fundamentals.append(ticker_df)

    if not all_fundamentals:
        print("Warning: Failed to retrieve fundamental data for any tickers.")
        return pd.DataFrame()
        
    final_df = pd.concat(all_fundamentals, ignore_index=True)
    
    final_df = final_df[[
        'fiscal_period_end',
        'publish_date',
        'ticker',
        'shares_outstanding',
        'book_equity',
        'book_per_share'
    ]]
    
    # Convert types for consistency
    final_df['fiscal_period_end'] = pd.to_datetime(final_df['fiscal_period_end'])
    final_df['publish_date'] = pd.to_datetime(final_df['publish_date'])
    final_df['shares_outstanding'] = pd.to_numeric(final_df['shares_outstanding'])
    final_df['book_equity'] = pd.to_numeric(final_df['book_equity'])

    print("\nSuccessfully created fundamentals series.")
    return final_df.sort_values(by=['ticker', 'fiscal_period_end']).reset_index(drop=True)


if __name__ == '__main__':
    # A standalone test to verify the new logic
    TEST_TICKERS = ["PETR4.SA", "VALE3.SA"]
    
    print("--- Running Fundamentals Ingestion Module Standalone Test ---")
    
    try:
        fundamentals = create_fundamentals_series(TEST_TICKERS)
        if not fundamentals.empty:
            print("\n--- Test Results ---")
            print("Shape:", fundamentals.shape)
            print("Columns:", fundamentals.columns.tolist())
            print("\nData for PETR4.SA (shows variation in shares):")
            print(fundamentals[fundamentals['ticker'] == 'PETR4.SA'].tail())
            print("\nData for VALE3.SA (shows variation in shares):")
            print(fundamentals[fundamentals['ticker'] == 'VALE3.SA'].tail())

            # Check that BPS is now calculated
            if fundamentals['book_per_share'].isnull().any():
                print("\nWarning: Found nulls in 'book_per_share'.")
        else:
            print("\nTest failed: No data was returned.")
            
    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/data/ingest_fx.py
###############################################################################
import pandas as pd
import requests
import numpy as np

from ..utils_dates import get_b3_trading_calendar

# The BCB SGS series for USD/BRL closing exchange rate (PTAX) is 1.
BCB_API_BASE_URL = "https://api.bcb.gov.br/dados/serie/bcdata.sgs.{series_id}/dados"

def fetch_fx_from_bcb(
    series_id: int, start_date: pd.Timestamp, end_date: pd.Timestamp
) -> pd.DataFrame:
    """
    Fetches a time series from the BCB's SGS API. Specifically for FX rates.

    Args:
        series_id: The numerical ID of the SGS series (e.g., 1 for USD/BRL).
        start_date: The start date for the data query.
        end_date: The end date for the data query.

    Returns:
        A pandas DataFrame with the raw data from the API.
    """
    url = BCB_API_BASE_URL.format(series_id=series_id)
    params = {
        "formato": "json",
        "dataInicial": start_date.strftime("%d/%m/%Y"),
        "dataFinal": end_date.strftime("%d/%m/%Y"),
    }
    
    print(f"Fetching FX series {series_id} from {start_date.date()} to {end_date.date()}...")
    response = requests.get(url, params=params)
    response.raise_for_status()
    
    data = response.json()
    if not data:
        raise ValueError(f"No FX data returned from BCB API for series {series_id}.")

    df = pd.DataFrame(data)
    return df

def create_fx_series(
    start_date: str, end_date: str, series_id: int = 1, fx_pair: str = "USD_BRL"
) -> pd.DataFrame:
    """
    Constructs the daily FX rate series and its log returns.

    Args:
        start_date: The start date for the series.
        end_date: The end date for the series.
        series_id: The BCB SGS series ID for the FX rate.
        fx_pair: A string name for the FX pair column.

    Returns:
        A pandas DataFrame indexed by trading day, with columns for the
        spot rate and its daily log return.
    """
    start_ts = pd.to_datetime(start_date)
    end_ts = pd.to_datetime(end_date)
    
    # 1. Fetch raw data from BCB
    raw_fx_df = fetch_fx_from_bcb(series_id, start_ts, end_ts)

    # 2. Basic processing
    raw_fx_df["date"] = pd.to_datetime(raw_fx_df["data"], format="%d/%m/%Y")
    raw_fx_df[fx_pair] = pd.to_numeric(raw_fx_df["valor"], errors="coerce")
    
    raw_fx_df = raw_fx_df.set_index("date")[[fx_pair]].sort_index()
    
    # 3. Get the canonical trading calendar
    b3_calendar = get_b3_trading_calendar(start_date, end_date)

    # 4. Align FX data to the trading calendar and forward-fill
    aligned_fx = raw_fx_df.reindex(b3_calendar, method="ffill")
    aligned_fx = aligned_fx.dropna()
    
    # 5. Calculate daily log returns for use as a risk factor
    log_return_col = f"{fx_pair}_log_return"
    # A more direct and efficient way to calculate log returns
    aligned_fx[log_return_col] = np.log(aligned_fx[fx_pair] / aligned_fx[fx_pair].shift(1))
    
    aligned_fx = aligned_fx.dropna() # First row will be NaN
    
    print(f"Successfully created FX series with {len(aligned_fx)} trading days.")
    return aligned_fx

if __name__ == "__main__":
    print("--- Running FX Ingest Module Standalone Test ---")
    
    TEST_START_DATE = "2023-01-01"
    TEST_END_DATE = "2023-06-30"
    
    try:
        fx_df = create_fx_series(
            start_date=TEST_START_DATE,
            end_date=TEST_END_DATE,
            series_id=1,
            fx_pair="USD_BRL"
        )
        
        print("\n--- Results ---")
        print("Shape of the final DataFrame:", fx_df.shape)
        print("\nFirst 5 rows:")
        print(fx_df.head())
        print("\nLast 5 rows:")
        print(fx_df.tail())
        
        # Check for any missing values
        if fx_df.isnull().values.any():
            print("\nERROR: Missing values found in the final series!")
        else:
            print("\nOK: No missing values found.")
            
        # Validate return calculation
       # validate the first *valid* return
        price_t0, price_t1 = fx_df["USD_BRL"].iloc[0:2]
        expected_ret = np.log(price_t1 / price_t0)
        actual_ret   = fx_df["USD_BRL_log_return"].iloc[1]    # second row
        assert np.isclose(expected_ret, actual_ret, rtol=1e-12, atol=1e-12)
        print("\nOK: Log return calculation is validated.")
        
    except Exception as e:
        import traceback  # ← add this
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/data/ingest_prices.py
###############################################################################
# ── standard library ---------------------------------------------------------
from typing import List, Dict
import pandas as pd

# ── third‑party --------------------------------------------------------------
import yfinance as yf
from requests.exceptions import HTTPError
from tenacity import (
    retry,
    wait_exponential,
    stop_after_attempt,
    retry_if_exception_type,
)
from yfinance.exceptions import YFRateLimitError  # runtime > 0.2.57

# Optional global throttle (only exists on yfinance 0.2.38‑0.2.58)
try:
    from yfinance.utils import enable_ratelimit  # type: ignore
    enable_ratelimit()
except (ImportError, AttributeError):
    pass  # rely on chunk_size + tenacity back‑off

# ── project‑local ------------------------------------------------------------
from ..utils_dates import get_b3_trading_calendar

@retry(
    wait=wait_exponential(multiplier=1, min=2, max=60),
    stop=stop_after_attempt(6),
    retry=retry_if_exception_type(
        (HTTPError, ConnectionError, TimeoutError, YFRateLimitError)
    ),
)

def _download_chunk_with_tenacity(chunk: List[str], start: str, end: str) -> pd.DataFrame:
    """
    Downloads a single chunk of tickers using yfinance, wrapped in a tenacity retry decorator.
    """
    print(f"  - Downloading chunk: {chunk}")
    df = yf.download(
        tickers=chunk,
        start=start,
        end=end,
        auto_adjust=False,
        actions=True,
        interval="1d",
        threads=False,
        progress=False,
    )
    # yfinance can return an empty DataFrame with a warning for invalid tickers,
    # so we check if all columns are missing data, which can indicate a deeper issue.
    if df.empty or df.columns.empty:
         # Check if it's just a case of no data in the range for valid tickers
        if yf.Ticker(chunk[0]).history(period="1d").empty:
            print(f"Warning: No data for chunk {chunk}, possibly invalid tickers or no history.")
            return pd.DataFrame()
        # Otherwise, raise to trigger retry for transient issues
        raise ConnectionError(f"Download for chunk {chunk} returned an empty frame.")
        
    return df


def fetch_yfinance_data(
    tickers: List[str],
    start_date: str,
    end_date: str,
    index_ticker: str = "^BVSP",
    max_retries: int = 5,
    backoff_factor: float = 0.5,
    chunk_size: int = 3,        # was 12
) -> Dict[str, pd.DataFrame]:
    """
    Fetches historical market data for a list of tickers and a benchmark index.

    This function downloads daily Open, High, Low, Close, Adjusted Close, and Volume
    data from Yahoo Finance. It also fetches corporate actions (dividends and splits).
    It includes a retry mechanism with exponential backoff to handle rate limits,
    and downloads tickers in chunks to avoid overwhelming the API.

    Args:
        tickers: A list of B3 ticker symbols to download (e.g., ['PETR4.SA', 'VALE3.SA']).
        start_date: The start date for the data query (YYYY-MM-DD).
        end_date: The end date for the data query (YYYY-MM-DD).
        index_ticker: The ticker for the benchmark index (default: ^BVSP for IBOVESPA).
        max_retries: Maximum number of times to retry the download.
        backoff_factor: Factor to determine the delay between retries (delay = backoff_factor * 2**attempt).
        chunk_size: The number of tickers to download in each batch.

    Returns:
        A dictionary containing:
        - 'prices': A multi-index DataFrame with price and volume data for all tickers.
        - 'index': A DataFrame with price and volume data for the index.
        - 'actions': A dictionary of DataFrames, with each ticker as a key,
                     containing its corporate actions.
    """
    tickers = list(dict.fromkeys(tickers))  # dedupe while preserving order
    print(f"Downloading {len(tickers)} tickers in chunks of {chunk_size}...")

    all_price_frames = []
    failed_chunks = []
    # 1. Download equities in manageable chunks
    for idx in range(0, len(tickers), chunk_size):
        chunk = tickers[idx: idx + chunk_size]
        if not chunk:
            continue
        
        try:
            df_chunk = _download_chunk_with_tenacity(chunk, start=start_date, end=end_date)
            if not df_chunk.empty:
                all_price_frames.append(df_chunk)
        except Exception as e:
            print(f"Chunk {chunk} failed after all retries: {e}")
            failed_chunks.append(chunk)

    # Download the benchmark index separately
    try:
        index_df = _download_chunk_with_tenacity([index_ticker], start=start_date, end=end_date)
    except Exception as e:
        print(f"Index {index_ticker} failed after all retries: {e}")
        index_df = pd.DataFrame()

    # If no equity tickers were requested, just return the index and empty frames
    if not all_price_frames:
        if index_df.empty:
            raise ValueError("Yahoo Finance download failed for both tickers and index.")
        # If ONLY the index was requested and successful, prepare a valid return package
        index_data = index_df.copy()
        index_data.columns = index_data.columns.droplevel(1) # Flatten MultiIndex
        return {"prices": pd.DataFrame(), "index": index_data, "actions": {}}

    # Combine all equity chunks horizontally (columns are MultiIndex)
    data_prices_full = pd.concat(all_price_frames, axis=1)

    # Append the index columns (avoid duplicate column names)
    if not index_df.empty:
        data_prices_full = pd.concat([data_prices_full, index_df], axis=1)

    # Separate index from equities (guard if index failed)
    if index_ticker in data_prices_full.columns.get_level_values(1):
        index_data = data_prices_full.loc[:, (slice(None), index_ticker)].copy()
        index_data.columns = index_data.columns.droplevel(1)
        price_data = data_prices_full.drop(index_ticker, axis=1, level=1)
    else:
        print(f"Warning: Index ticker {index_ticker} missing from download.")
        index_data = pd.DataFrame()
        price_data = data_prices_full

    # Separate prices from actions
    price_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
    actions_cols = ['Dividends', 'Stock Splits']
    
    prices = {}
    actions_raw = {}

    for col in price_cols:
        sub_cols = [(col, tk) for tk in tickers if (col, tk) in price_data.columns]
        if sub_cols:
            prices[col] = price_data.loc[:, sub_cols]

    for col in actions_cols:
        sub_cols = [(col, tk) for tk in tickers if (col, tk) in price_data.columns]
        if sub_cols:
            actions_raw[col] = price_data.loc[:, sub_cols]

    # Reformat actions into a more usable dictionary
    actions_dict: Dict[str, pd.DataFrame] = {}
    if actions_raw:
       for tk in tickers:
           frames = []
           for fld, df in actions_raw.items():
               col = (fld, tk)
               if col in df.columns:
                   # select the Series, rename to the field (Dividends / Stock Splits)
                   frames.append(df[col].rename(fld))
           if frames:
               merged = pd.concat(frames, axis=1)
               merged = merged[merged.sum(axis=1) != 0]  # keep only non‑zero rows
               if not merged.empty:
                   actions_dict[tk] = merged

    # Rebuild a clean price DataFrame: rows = date, columns = MultiIndex (ticker, field)
    if price_data.empty:
        raise ValueError("No equity price data available after processing.")

    prices_df = price_data.copy()
    prices_df.columns = prices_df.columns.swaplevel(0, 1)
    prices_df = prices_df.sort_index(axis=1)


    return {"prices": prices_df, "index": index_data, "actions": actions_dict}


def create_equity_price_series(
    tickers: List[str], start_date: str, end_date: str
) -> pd.DataFrame:
    """
    Orchestrates the download and cleaning of equity price data.
    ...
    """
    b3_calendar = get_b3_trading_calendar(start_date, end_date)
    buffered_start = (pd.to_datetime(start_date) - pd.Timedelta(days=5)).strftime('%Y-%m-%d')

    data_bundle = fetch_yfinance_data(tickers, buffered_start, end_date)
    prices_wide = data_bundle['prices']

    # --- SLICE DATA ---
    adj_close = prices_wide.xs('Adj Close', level=1, axis=1)
    volume = prices_wide.xs('Volume', level=1, axis=1)

    # --- ALIGN TO CALENDAR ---
    adj_close_aligned = adj_close.reindex(b3_calendar).ffill()
    volume_aligned = volume.reindex(b3_calendar).fillna(0)

    # --- NAME INDEXES FOR ROBUST CONVERSION ---
    adj_close_aligned.index.name = "date"
    adj_close_aligned.columns.name = "ticker" # Explicitly name columns
    volume_aligned.index.name = "date"
    volume_aligned.columns.name = "ticker" # Explicitly name columns

    # --- WIDE TO LONG (ROBUST METHOD) ---
    long_adj_close = adj_close_aligned.stack().reset_index(name="adj_close")
    long_volume = volume_aligned.stack().reset_index(name="volume")

    # This merge will now work reliably
    final_df = pd.merge(long_adj_close, long_volume, on=['date', 'ticker'])
    
    final_df = final_df.dropna(subset=['adj_close'])
    
    print(f"Successfully created equity price series for {len(tickers)} tickers.")
    return final_df.sort_values(by=['ticker', 'date']).reset_index(drop=True)


def create_index_series(
    start_date: str, end_date: str, index_ticker: str = "^BVSP"
) -> pd.DataFrame:
    """
    Creates the cleaned, aligned daily series for the benchmark index.

    Args:
        start_date: The start date for the series.
        end_date: The end date for the series.
        index_ticker: The Yahoo Finance ticker for the index.

    Returns:
        A DataFrame indexed by date with 'adj_close' and 'volume'.
    """
    b3_calendar = get_b3_trading_calendar(start_date, end_date)
    buffered_start = (pd.to_datetime(start_date) - pd.Timedelta(days=5)).strftime('%Y-%m-%d')
    
    data_bundle = fetch_yfinance_data([], buffered_start, end_date, index_ticker)
    index_data = data_bundle['index'][['Adj Close', 'Volume']]
    index_data = index_data.rename(columns={'Adj Close': 'adj_close'})

    # Align to calendar and forward fill
    index_aligned = index_data.reindex(b3_calendar).ffill()
    index_aligned = index_aligned.dropna()
    
    print(f"Successfully created index series for {index_ticker}.")
    return index_aligned


if __name__ == '__main__':
    # Standalone test for the module
    TEST_TICKERS = ["PETR4.SA", "VALE3.SA", "ITUB4.SA", "BBDC4.SA", "WEGE3.SA"]
    TEST_START = "2023-01-01"
    TEST_END = "2023-12-31"

    print("--- Running Price Ingestion Module Standalone Test ---")

    try:
        # Test equity price series creation
        print("\n--- Testing Equity Prices ---")
        equity_prices = create_equity_price_series(TEST_TICKERS, TEST_START, TEST_END)
        print("Equity prices shape:", equity_prices.shape)
        print("Data for one ticker (PETR4.SA):")
        print(equity_prices[equity_prices['ticker'] == 'PETR4.SA'].head())
        
        # Verify no missing data for a specific ticker within its lifetime
        petr4_data = equity_prices[equity_prices['ticker'] == 'PETR4.SA']
        if petr4_data['adj_close'].isnull().any():
             print("\nERROR: Found nulls in PETR4.SA data!")
        else:
             print("\nOK: No nulls found in PETR4.SA data.")

        # Test index series creation
        print("\n--- Testing Index Prices ---")
        ibov_prices = create_index_series(TEST_START, TEST_END)
        print("IBOV prices shape:", ibov_prices.shape)
        print(ibov_prices.head())

        if ibov_prices['adj_close'].isnull().any():
             print("\nERROR: Found nulls in IBOV data!")
        else:
             print("\nOK: No nulls found in IBOV data.")

    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/data/ingest_selic.py
###############################################################################
import pandas as pd
import requests
from typing import Dict, Any

# Assuming this script is run from a context where 'b3alloc' is in the python path
from ..utils_dates import get_b3_trading_calendar

# The specification refers to BCB SGS series 11 for SELIC.
# API documentation: https://dadosabertos.bcb.gov.br/dataset/11-taxa-de-juros---selic/resource/71bcb420-5503-4a72-b651-70a273574b41
BCB_API_BASE_URL = "https://api.bcb.gov.br/dados/serie/bcdata.sgs.{series_id}/dados"
TRADING_DAYS_PER_YEAR = 252 # As per specification

def fetch_selic_from_bcb(
    series_id: int, start_date: pd.Timestamp, end_date: pd.Timestamp
) -> pd.DataFrame:
    """
    Fetches a time series from the BCB's SGS API.

    Args:
        series_id: The numerical ID of the SGS series (e.g., 11 for SELIC).
        start_date: The start date for the data query.
        end_date: The end date for the data query.

    Returns:
        A pandas DataFrame with the raw data from the API.

    Raises:
        requests.exceptions.HTTPError: If the API request fails.
    """
    url = BCB_API_BASE_URL.format(series_id=series_id)
    params = {
        "formato": "json",
        "dataInicial": start_date.strftime("%d/%m/%Y"),
        "dataFinal": end_date.strftime("%d/%m/%Y"),
    }
    
    print(f"Fetching SELIC series {series_id} from {start_date.date()} to {end_date.date()}...")
    response = requests.get(url, params=params)
    response.raise_for_status()  # Raises an HTTPError for bad responses (4xx or 5xx)
    
    data = response.json()
    if not data:
        raise ValueError(f"No data returned from BCB API for series {series_id} in the given date range.")

    df = pd.DataFrame(data)
    return df


def create_risk_free_series(
    start_date: str, end_date: str, series_id: int = 11
) -> pd.DataFrame:
    """
    Constructs the daily risk-free rate series for the B3 market.

    This function orchestrates fetching the SELIC rate, processing it, and
    aligning it with the official B3 trading calendar.

    Args:
        start_date: The start date for the series (e.g., '2010-01-01').
        end_date: The end date for the series (e.g., '2025-01-01').
        series_id: The BCB SGS series ID for the risk-free rate.
                   Defaults to 11 (SELIC).

    Returns:
        A pandas DataFrame indexed by trading day, with columns for the
        daily risk-free rate and its annualized equivalent.
    """
    start_ts = pd.to_datetime(start_date)
    end_ts = pd.to_datetime(end_date)
    
    # 1. Fetch raw data from BCB
    raw_selic_df = fetch_selic_from_bcb(series_id, start_ts, end_ts)

    # 2. Basic processing
    # The 'valor' is the annual rate as a percentage. Convert to decimal.
    raw_selic_df["date"] = pd.to_datetime(raw_selic_df["data"], format="%d/%m/%Y")
    raw_selic_df["selic_annualized"] = pd.to_numeric(raw_selic_df["valor"], errors="coerce") / 100.0
    
    # Set date as index for alignment
    raw_selic_df = raw_selic_df.set_index("date")[["selic_annualized"]]
    raw_selic_df = raw_selic_df.sort_index()
    
    # 3. Get the canonical trading calendar
    b3_calendar = get_b3_trading_calendar(start_date, end_date)

    # 4. Align SELIC data to the trading calendar
    # Reindex with the trading calendar and forward-fill missing values
    aligned_rf = raw_selic_df.reindex(b3_calendar, method="ffill")
    
    # Drop any initial NaNs if the backtest starts before the SELIC data
    aligned_rf = aligned_rf.dropna()

    # 5. Convert annualized rate to daily rate for modeling
    # Formula: daily_rate = (1 + annual_rate)^(1/252) - 1
    aligned_rf["rf_daily"] = (
        (1 + aligned_rf["selic_annualized"]) ** (1 / TRADING_DAYS_PER_YEAR)
    ) - 1
    
    aligned_rf.index.name = "date"
    
    print(f"Successfully created risk-free series with {len(aligned_rf)} trading days.")
    return aligned_rf


if __name__ == "__main__":
    # Example of how to run this module directly for testing
    TEST_START_DATE = "2022-01-01"
    TEST_END_DATE = "2022-12-31"
    
    print("--- Running SELIC Ingestion Module Standalone Test ---")
    
    try:
        risk_free_df = create_risk_free_series(
            start_date=TEST_START_DATE,
            end_date=TEST_END_DATE,
            series_id=11
        )
        
        print("\n--- Results ---")
        print("Shape of the final DataFrame:", risk_free_df.shape)
        print("\nFirst 5 rows:")
        print(risk_free_df.head())
        print("\nLast 5 rows:")
        print(risk_free_df.tail())
        
        # Validation
        # Check if the conversion from annual to daily is correct for one entry
        last_entry = risk_free_df.iloc[-1]
        annual_rate = last_entry['selic_annualized']
        daily_rate = last_entry['rf_daily']
        expected_daily_rate = (1 + annual_rate)**(1/TRADING_DAYS_PER_YEAR) - 1
        
        assert abs(daily_rate - expected_daily_rate) < 1e-9, "Daily rate calculation is incorrect."
        print("\nOK: Daily rate calculation is validated.")

        # Check for any missing values, which would be an error
        if risk_free_df.isnull().values.any():
            print("\nERROR: Missing values found in the final series!")
        else:
            print("\nOK: No missing values found in the final series.")

    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/factors/__init__.py
###############################################################################
# This file marks the 'b3alloc' directory as a Python package.
# It can also be used to define package-level variables or import key functions.

__version__ = "0.1.0"


###############################################################################
### FILE: src/b3alloc/factors/fama_french_b3.py
###############################################################################
import pandas as pd
import numpy as np
from typing import Dict

def _get_factor_portfolio_assignments(
    data_for_date: pd.DataFrame
) -> pd.Series:
    """
    Assigns each stock to one of six portfolios based on size and B/M ratio.

    This function is called for a single date (the formation date) and uses
    the cross-section of all stocks at that point in time.

    Args:
        data_for_date: A DataFrame with 'market_cap' and 'book_to_market'
                       for all tickers on a single formation date.

    Returns:
        A pandas Series with tickers as index and portfolio assignment
        (e.g., 'Small_High') as values.
    """
    # Filter out ineligible stocks for factor construction, as per spec
    eligible = data_for_date[
        (data_for_date['book_to_market'].notna()) &
        (data_for_date['book_to_market'] > 0) & # Positive book equity
        (data_for_date['market_cap'] > 0)
    ].copy()

    if eligible.empty:
        return pd.Series(dtype=str)

    # 1. Size Breakpoint (SMB): Median market cap
    size_breakpoint = eligible['market_cap'].median()
    eligible['size_bucket'] = np.where(eligible['market_cap'] >= size_breakpoint, 'Big', 'Small')

    # 2. Value Breakpoints (HML): 30th and 70th percentiles of B/M
    bm_p30, bm_p70 = eligible['book_to_market'].quantile([0.3, 0.7])
    eligible['value_bucket'] = 'Mid'
    eligible.loc[eligible['book_to_market'] <= bm_p30, 'value_bucket'] = 'Low' # Growth
    eligible.loc[eligible['book_to_market'] >= bm_p70, 'value_bucket'] = 'High' # Value

    # 3. Combine to form the six portfolios
    assignments = eligible['size_bucket'] + '_' + eligible['value_bucket']
    return assignments


def build_fama_french_factors(
    daily_fundamentals_df: pd.DataFrame,
    prices_df: pd.DataFrame,
    returns_df: pd.DataFrame,
    market_excess_returns: pd.Series
) -> pd.DataFrame:
    """
    Constructs the daily Fama-French 3 factors (MKT, SMB, HML) for the B3 market.

    Args:
        daily_fundamentals_df: Long-format DataFrame from the align module.
        prices_df: Wide-format DataFrame of daily prices.
        returns_df: Wide-format DataFrame of daily simple returns.
        market_excess_returns: Series of daily IBOV excess returns.

    Returns:
        A DataFrame with a daily time series of the Mkt_excess, SMB, and HML factors.
    """
    print("Building Fama-French 3 factors for B3...")

    # 1. Create a daily panel with all necessary metrics
    market_cap_df = prices_df * daily_fundamentals_df.pivot(index='date', columns='ticker', values='shares_outstanding')
    book_to_market_df = daily_fundamentals_df.pivot(index='date', columns='ticker', values='book_equity') / market_cap_df
    
    # 2. Determine portfolio assignments at the beginning of each month
    formation_dates = returns_df.resample('M').first().index
    
    all_assignments = pd.DataFrame(index=returns_df.index, columns=returns_df.columns)

    print("Determining monthly portfolio assignments...")
    for date in formation_dates:
        # Ensure the date exists in our market cap index before proceeding
        if date not in market_cap_df.index:
            closest_date = market_cap_df.index.asof(date)
            if closest_date is pd.NaT: continue
            date = closest_date

        data_for_formation = pd.DataFrame({
            'market_cap': market_cap_df.loc[date],
            'book_to_market': book_to_market_df.loc[date]
        })
        assignments = _get_factor_portfolio_assignments(data_for_formation)
        all_assignments.loc[date] = assignments
    
    # Forward-fill the assignments for the rest of each month
    all_assignments = all_assignments.ffill()

    # 3. Calculate daily returns for the 6 value-weighted portfolios
    portfolio_returns = pd.DataFrame(index=returns_df.index)
    
    # Previous day's market cap is used for weighting today's returns
    mkt_cap_lagged = market_cap_df.shift(1)

    print("Calculating daily value-weighted portfolio returns...")
    for portfolio_name in ['Small_Low', 'Small_Mid', 'Small_High', 'Big_Low', 'Big_Mid', 'Big_High']:
        # Create a boolean mask for stocks in this portfolio on each day
        mask = (all_assignments == portfolio_name)
        
        # Calculate weights for each stock within the portfolio
        portfolio_mkt_cap = (mkt_cap_lagged * mask).sum(axis=1)
        
        # Replace 0 with NaN to avoid division errors; NaN/NaN results in NaN which we fill with 0.
        weights = (mkt_cap_lagged * mask).div(portfolio_mkt_cap.replace(0, np.nan), axis=0).fillna(0)
        
        # Portfolio return is the sum of weighted constituent returns
        portfolio_returns[portfolio_name] = (returns_df * weights).sum(axis=1)

    # 4. Calculate SMB and HML factor returns
    # SMB = (Small/Low + Small/Mid + Small/High)/3 - (Big/Low + Big/Mid + Big/High)/3
    smb = (
        portfolio_returns[['Small_Low', 'Small_Mid', 'Small_High']].mean(axis=1) -
        portfolio_returns[['Big_Low', 'Big_Mid', 'Big_High']].mean(axis=1)
    )
    
    # HML = (Small/High + Big/High)/2 - (Small/Low + Big/Low)/2
    hml = (
        portfolio_returns[['Small_High', 'Big_High']].mean(axis=1) -
        portfolio_returns[['Small_Low', 'Big_Low']].mean(axis=1)
    )

    # 5. Combine into the final factor panel
    factor_panel = pd.DataFrame({
        'mkt_excess': market_excess_returns,
        'smb': smb,
        'hml': hml
    }).dropna()
    
    print("Successfully built Fama-French factor panel.")
    return factor_panel


if __name__ == '__main__':
    print("--- Running Fama-French Module Standalone Test ---")

    # 1. Create dummy input data for a 2-month period
    dates = pd.date_range("2023-01-01", "2023-02-28", freq="B")
    tickers = ['S_L', 'S_H', 'B_L', 'B_H'] # Small/Low, Small/High, Big/Low, Big/High
    
    # Setup characteristics
    shares = pd.DataFrame({'S_L': 1e6, 'S_H': 1e6, 'B_L': 5e6, 'B_H': 5e6}, index=dates)
    prices = pd.DataFrame({
        'S_L': np.linspace(10, 11, len(dates)), 'S_H': np.linspace(12, 13, len(dates)),
        'B_L': np.linspace(50, 52, len(dates)), 'B_H': np.linspace(60, 61, len(dates)),
    }, index=dates)
    book_equity = pd.DataFrame({ # Low B/M for _L, High B/M for _H
        'S_L': 1e6, 'S_H': 10e6, 'B_L': 20e6, 'B_H': 300e6
    }, index=dates)

    daily_fundamentals = pd.concat([
        shares.stack().rename('shares_outstanding'),
        book_equity.stack().rename('book_equity')
    ], axis=1).reset_index().rename(columns={'level_0':'date', 'level_1':'ticker'})
    
    returns = prices.pct_change().dropna()
    mkt_excess = pd.Series(np.random.randn(len(dates)) * 0.01, index=dates)
    
    # Align indices for the test
    prices, mkt_excess = prices.align(returns, join='right', axis=0)[0], mkt_excess.align(returns, join='right', axis=0)[0]
    daily_fundamentals = daily_fundamentals[daily_fundamentals.date.isin(returns.index)]

    # 2. Run the builder
    try:
        factor_panel = build_fama_french_factors(daily_fundamentals, prices, returns, mkt_excess)
        
        print("\n--- Output Factor Panel ---")
        print(factor_panel.head())
        print("\n--- Diagnostics ---")
        print(factor_panel.describe())
        
        # 3. Validation
        # On a day where small caps outperform big caps, SMB should be positive.
        # Let's engineer a return for one day
        test_date = pd.to_datetime('2023-02-15')
        if test_date in returns.index:
            returns.loc[test_date, ['S_L', 'S_H']] = 0.02  # Small caps jump 2%
            returns.loc[test_date, ['B_L', 'B_H']] = -0.01 # Big caps drop 1%
            
            # Re-run with the specific return shock
            factor_panel_shock = build_fama_french_factors(daily_fundamentals, prices, returns, mkt_excess)
            smb_on_shock_day = factor_panel_shock.loc[test_date, 'smb']
            
            print(f"\nSMB on shock day ({test_date.date()}): {smb_on_shock_day:.4f}")
            assert smb_on_shock_day > 0, "SMB should be positive when small caps outperform."
            print("OK: SMB sign responds correctly to return shocks.")

    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/optimize/__init__.py
###############################################################################
# This file marks the 'b3alloc' directory as a Python package.
# It can also be used to define package-level variables or import key functions.

__version__ = "0.1.0"


###############################################################################
### FILE: src/b3alloc/optimize/constraints.py
###############################################################################
import cvxpy as cp
import pandas as pd
from typing import List, Dict, Optional
from ..config import OptimizerConfig

def build_optimizer_constraints(
    w_variable: cp.Variable,
    config: OptimizerConfig,
    asset_universe: List[str],
    sector_map: Optional[Dict[str, str]] = None
) -> List[cp.constraints.constraint.Constraint]:
    """
    Builds a list of cvxpy constraints based on the provided configuration.

    Args:
        w_variable: The cvxpy Variable representing the portfolio weights.
        config: The OptimizerConfig Pydantic model.
        asset_universe: The list of assets in the order they appear in w_variable.
        sector_map: A dictionary mapping tickers to their sectors, required for
                    sector constraints.

    Returns:
        A list of cvxpy constraint objects.
    """
    constraints = []
    
    # --- Core Constraints ---
    constraints.append(cp.sum(w_variable) == 1)
    
    if config.long_only:
        constraints.append(w_variable >= 0)
        
    # --- Weight Cap Constraints ---
    if config.name_cap is not None:
        constraints.append(w_variable <= config.name_cap)
    
    if config.sector_cap is not None:
        if not sector_map:
            raise ValueError("A sector_map is required for sector cap constraints.")
            
        sectors = {}
        for i, ticker in enumerate(asset_universe):
            sector = sector_map.get(ticker, 'Unknown')
            if sector not in sectors:
                sectors[sector] = []
            sectors[sector].append(w_variable[i])
            
        for sector, weights_in_sector in sectors.items():
            if sector != 'Unknown':
                constraints.append(cp.sum(weights_in_sector) <= config.sector_cap)

    print(f"Built {len(constraints)} constraints for the optimizer.")
    return constraints


if __name__ == '__main__':
    print("--- Running Constraints Module Standalone Test ---")
    
    # --- GIVEN ---
    test_universe = ['PETR4.SA', 'VALE3.SA', 'ITUB4.SA', 'BBDC4.SA']
    test_sector_map = {
        'PETR4.SA': 'Energy',
        'VALE3.SA': 'Materials',
        'ITUB4.SA': 'Financials',
        'BBDC4.SA': 'Financials'
    }
    # Use the Pydantic model for config
    test_config = OptimizerConfig(
        objective='max_sharpe', # Objective is not used here, but required by model
        long_only=True,
        name_cap=0.10,
        sector_cap=0.15,
        turnover_penalty_bps=0
    )
    w = cp.Variable(len(test_universe))
    
    # --- WHEN ---
    try:
        constraints_list = build_optimizer_constraints(w, test_config, test_universe, test_sector_map)
        
        # --- THEN ---
        print(f"\nGenerated {len(constraints_list)} constraint objects.")
        
        # Validation
        assert len(constraints_list) == 6, "Incorrect number of constraints generated."
        
        print("\nOK: Correct number of constraints created.")
        
        sector_constraint_str = str(constraints_list[-1])
        assert '<=' in sector_constraint_str
        assert str(test_config.sector_cap) in sector_constraint_str
        print("OK: Sector constraint appears to be correctly formulated.")
        
    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/optimize/mean_variance.py
###############################################################################
import pandas as pd
import numpy as np
import cvxpy as cp
from typing import Optional, List

def run_mean_variance_optimization(
    mu: pd.Series,
    sigma: pd.DataFrame,
    risk_aversion: float,
    constraints: List[cp.constraints.constraint.Constraint],
    solver: str = 'OSQP'
) -> Optional[pd.Series]:
    """
    Solves the mean-variance optimization problem for a given risk aversion.

    This function finds the portfolio weights 'w' that maximize the quadratic utility:
    Objective: w.T * mu - (gamma / 2) * w.T * Sigma * w
    where gamma is the risk aversion parameter.

    This is the standard formulation for mean-variance optimization and is a
    convex quadratic program, which can be solved efficiently.

    Args:
        mu: A pandas Series of expected excess returns.
        sigma: A pandas DataFrame of the asset covariance matrix.
        risk_aversion: The scalar risk aversion parameter (gamma). A higher
                       value leads to a more conservative (lower-risk) portfolio.
        constraints: A list of pre-constructed cvxpy constraint objects.

    Returns:
        A pandas Series of optimal portfolio weights, indexed by ticker.
        Returns None if the optimization fails.
    """
    # Ensure mu and sigma are aligned
    tickers = mu.index.tolist()
    sigma = sigma.reindex(index=tickers, columns=tickers)
    
    n = len(tickers)
    w = cp.Variable(n) # The portfolio weights vector to be solved for

    # Define the objective function
    expected_return = mu.values @ w
    risk_term = cp.quad_form(w, sigma.values) # Efficiently calculates w.T * Sigma * w
    
    utility = expected_return - (risk_aversion / 2) * risk_term
    objective = cp.Maximize(utility)
    
    # Define the problem
    problem = cp.Problem(objective, constraints)
    
    # Solve the problem
    print("Solving mean-variance optimization problem...")
    try:
        # OSQP is a good, fast solver for QPs. ECOS is another option.
        # It's important to handle cases where the problem is infeasible or unbounded.
        problem.solve(solver=solver, verbose=False)

        if problem.status not in ["optimal", "optimal_inaccurate"]:
            print(f"Warning: Optimizer failed or found a non-optimal solution. Status: {problem.status}")
            return None
        
        optimal_weights = pd.Series(w.value, index=tickers, name="weights")
        # Due to numerical precision, clip very small negative weights to zero
        optimal_weights[optimal_weights < 0] = 0
        # Re-normalize to ensure sum-to-one constraint holds perfectly
        optimal_weights /= optimal_weights.sum()

        print("Successfully found optimal weights.")
        return optimal_weights

    except cp.SolverError as e:
        print(f"CVXPY SolverError: {e}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred during optimization: {e}")
        return None

def build_basic_constraints(n_assets: int, w_variable: cp.Variable) -> list:
    """Helper function to create a standard set of long-only, fully-invested constraints."""
    return [
        cp.sum(w_variable) == 1, # Fully invested
        w_variable >= 0          # Long only (no shorting)
    ]

if __name__ == '__main__':
    print("--- Running Mean-Variance Optimizer Module Standalone Test ---")

    # --- GIVEN ---
    # 1. A 3-asset universe with clear characteristics
    tickers = ['HIGH_SHARPE', 'MID_SHARPE', 'LOW_SHARPE']
    
    # Asset 1 has high return, low vol (high Sharpe)
    # Asset 3 has low return, high vol (low Sharpe)
    mu_test = pd.Series([0.15, 0.10, 0.05], index=tickers)
    sigma_test = pd.DataFrame(
        [
            [0.02, 0.01, 0.005], # Vol of HIGH_SHARPE = sqrt(0.02) = 14%
            [0.01, 0.04, 0.015], # Vol of MID_SHARPE = sqrt(0.04) = 20%
            [0.005, 0.015, 0.06] # Vol of LOW_SHARPE = sqrt(0.06) = 24%
        ],
        columns=tickers, index=tickers
    )
    
    # 2. A moderate risk aversion. A value similar to the one calculated
    # from the market is a good starting point.
    gamma = 2.5
    
    # 3. Standard constraints: long-only, fully invested
    w_test = cp.Variable(len(tickers))
    constraints_test = build_basic_constraints(len(tickers), w_test)

    try:
        # --- WHEN ---
        optimal_w = run_mean_variance_optimization(
            mu_test, sigma_test, gamma, constraints_test, solver='OSQP'
        )
        
        # --- THEN ---
        assert optimal_w is not None, "Optimization failed on a simple, valid problem."
        
        print("\n--- Optimizer Inputs ---")
        print("Expected Returns (mu):\n", mu_test)
        print("\nCovariance Matrix (Sigma):\n", sigma_test)
        print(f"\nRisk Aversion (gamma): {gamma}")
        
        print("\n--- Optimal Weights ---")
        print(optimal_w)
        
        # --- Validation ---
        # 1. Check if constraints are met
        assert np.isclose(optimal_w.sum(), 1.0), "Weights do not sum to 1."
        assert (optimal_w >= -1e-6).all(), "Negative weights found (violates long-only)." # Allow for tiny numerical error
        print("\nOK: Basic constraints (sum-to-one, long-only) are satisfied.")

        # 2. Check if the allocation makes economic sense
        # The asset with the highest Sharpe ratio should get the highest allocation.
        assert optimal_w.idxmax() == 'HIGH_SHARPE', "Allocation should be highest for the highest Sharpe asset."
        assert optimal_w['HIGH_SHARPE'] > optimal_w['MID_SHARPE'] > optimal_w['LOW_SHARPE']
        print("OK: Portfolio allocation is intuitive (favors higher Sharpe assets).")

    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/preprocess/__init__.py
###############################################################################
# This file marks the 'b3alloc' directory as a Python package.
# It can also be used to define package-level variables or import key functions.

__version__ = "0.1.0"


###############################################################################
### FILE: src/b3alloc/preprocess/align.py
###############################################################################
import pandas as pd
import logging

from ..utils_dates import get_b3_trading_calendar, apply_publication_lag

def align_fundamentals_to_prices(
    fundamentals_df: pd.DataFrame,
    price_dates: pd.DatetimeIndex,
    publish_lag_days: int
) -> pd.DataFrame:
    """
    Aligns quarterly fundamental data to a daily price calendar.

    This function performs two critical alignment steps:
    1. It calculates the 'actionable_date' for each fundamental report by
       applying a trading day lag to the 'fiscal_period_end' date. This
       simulates the delay in information publication.
    2. It forward-fills this quarterly data to create a daily panel that
       matches the price series, ensuring that at any point in time, we are
       using the most recently available fundamental data.

    Args:
        fundamentals_df: A long-format DataFrame of quarterly fundamentals with
                         columns ['ticker', 'fiscal_period_end', ...].
        price_dates: A DatetimeIndex of all trading days for which price
                     data is available.
        publish_lag_days: The number of trading days to wait after a fiscal
                          period ends before the data is considered public.

    Returns:
        A daily, long-format DataFrame with tickers and dates as the index,
        containing the correctly lagged and forward-filled fundamental data.
    """
    if fundamentals_df.empty:
        raise ValueError("Input fundamentals_df cannot be empty.")
    if price_dates.empty:
        raise ValueError("Input price_dates cannot be empty.")

    # 1. Determine the full trading calendar for the alignment period
    # This must encompass both fundamental and price date ranges.
    cal_start = min(fundamentals_df['fiscal_period_end'].min(), price_dates.min())
    cal_end = max(fundamentals_df['fiscal_period_end'].max(), price_dates.max())
    calendar = get_b3_trading_calendar(cal_start, cal_end)
    
    # 2. Calculate the 'actionable_date' for each fundamental report
    # We group by ticker to handle cases where different tickers have different
    # fiscal period end dates.
    
    # The 'publish_date' column is often missing or unreliable from scrapers.
    # The project spec dictates using 'fiscal_period_end' + a fixed lag as the
    # reliable point-in-time marker.
    
    # Drop any existing publish_date column to avoid confusion
    if 'publish_date' in fundamentals_df.columns:
        fundamentals_df = fundamentals_df.drop(columns=['publish_date'])
        
    def _process_group(group):
        """Processes each ticker group to add the actionable_date."""
        group = group.copy()
        ticker = group['ticker'].iloc[0]
        # logging.info(f"Processing group for ticker: {ticker}")
        
        # Get unique fiscal period end dates to avoid redundant calculations
        unique_fiscal_dates = pd.Series(group['fiscal_period_end'].unique()).dropna()
        if unique_fiscal_dates.empty:
            # logging.warning(f"No valid fiscal dates for {ticker}, skipping.")
            return None
        
        # logging.info(f"  - Ticker {ticker}: Found {len(unique_fiscal_dates)} unique fiscal dates.")
        
        # Calculate the actionable dates for these unique fiscal dates
        actionable_dates = apply_publication_lag(
            unique_fiscal_dates,
            publish_lag_days,
            calendar
        )
        # logging.info(f"  - Ticker {ticker}: Got {len(actionable_dates)} actionable dates.")
        
        # Create a dictionary to map fiscal dates to actionable dates.
        # This handles cases where some dates are dropped by the lag function.
        date_map = dict(zip(unique_fiscal_dates, actionable_dates))
        
        # Map the actionable dates back to the original group's index
        group['actionable_date'] = group['fiscal_period_end'].map(date_map)
        return group.dropna(subset=['actionable_date'])

    # Apply the processing function to each ticker group
    processed_groups = fundamentals_df.groupby('ticker', group_keys=False).apply(_process_group)

    if not processed_groups.empty:
        processed_groups = processed_groups.dropna(subset=['actionable_date'])
    else:
        raise ValueError("Could not calculate actionable dates for any tickers.")
        
    # 3. Create the daily panel by forward-filling
    # We set a multi-index on the data we want to fill forward.
    processed_groups = processed_groups.set_index(['ticker', 'actionable_date'])
    processed_groups = processed_groups.drop(columns=['fiscal_period_end'])
    processed_groups = processed_groups.sort_index()

    # Create the target daily index grid (all tickers for all price dates)
    tickers = fundamentals_df['ticker'].unique()
    daily_panel_index = pd.MultiIndex.from_product(
        [tickers, price_dates], names=['ticker', 'date']
    )
    
    # Reindex our lagged data onto this daily grid.
    # The `groupby('ticker').ffill()` is the key step. It ensures that we
    # forward-fill data for each ticker independently.
    daily_fundamentals = processed_groups.reindex(daily_panel_index)
    daily_fundamentals = daily_fundamentals.groupby(level='ticker').ffill()
    
    # Drop any rows that still have NaNs (e.g., assets that IPO'd mid-period)
    daily_fundamentals = daily_fundamentals.dropna()

    print("Successfully aligned fundamental data to daily price calendar.")
    return daily_fundamentals.reset_index()


if __name__ == '__main__':
    print("--- Running Align Module Standalone Test ---")

    # 1. Create dummy input data
    tickers = ['TICKER_A', 'TICKER_B']
    
    # Quarterly fundamentals for 2023
    dummy_fundamentals = pd.DataFrame({
        'ticker': ['TICKER_A', 'TICKER_A', 'TICKER_B', 'TICKER_B'],
        'fiscal_period_end': pd.to_datetime([
            "2022-12-31", "2023-03-31", "2022-12-31", "2023-03-31"
        ]),
        'book_equity': [1000, 1050, 2000, 1950],
        'shares_outstanding': [100, 100, 50, 50]
    })

    # Daily prices for part of 2023
    price_dates = get_b3_trading_calendar("2023-03-01", "2023-04-30")
    
    PUBLISH_LAG_DAYS = 3 # As per spec

    print("--- Input Data ---")
    print("Fundamentals:")
    print(dummy_fundamentals)
    print(f"\nPrice Dates Range: {price_dates.min().date()} to {price_dates.max().date()}")
    print(f"Publication Lag: {PUBLISH_LAG_DAYS} trading days")
    
    # 2. Run the alignment function
    try:
        daily_panel = align_fundamentals_to_prices(
            dummy_fundamentals, price_dates, PUBLISH_LAG_DAYS
        )
        
        print("\n--- Output ---")
        print("Shape of daily fundamental panel:", daily_panel.shape)
        
        # 3. Validation
        print("\n--- Validation ---")
        
        # Check TICKER_A around the Q1-2023 earnings release
        # Fiscal end: 2023-03-31 (Friday)
        # B3 Calendar around that time: Mar-31, Apr-03, Apr-04, Apr-05, Apr-06 (Good Friday), Apr-10
        # Actionable date should be 3 trading days AFTER Mar-31, which is Apr-05.
        
        data_before_release = daily_panel[
            (daily_panel['ticker'] == 'TICKER_A') &
            (daily_panel['date'] == pd.to_datetime('2023-04-04'))
        ]
        
        data_on_release_day = daily_panel[
            (daily_panel['ticker'] == 'TICKER_A') &
            (daily_panel['date'] == pd.to_datetime('2023-04-05'))
        ]

        print("Data available on April 4th (should be from Q4-2022):")
        print(data_before_release[['date', 'book_equity']])
        assert data_before_release['book_equity'].iloc[0] == 1000

        print("\nData available on April 5th (should be from Q1-2023):")
        print(data_on_release_day[['date', 'book_equity']])
        assert data_on_release_day['book_equity'].iloc[0] == 1050
        
        print("\nOK: Lookahead bias prevention is working correctly.")
        
    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/preprocess/clean.py
###############################################################################
import pandas as pd
from typing import List, Dict

def flag_price_outliers(
    returns_df: pd.DataFrame,
    window: int = 63,
    z_score_threshold: float = 5.0
) -> pd.DataFrame:
    """
    Flags extreme daily returns based on a rolling Z-score.

    This function helps identify potential data errors or extreme events.
    It does not modify the data but adds boolean flag columns.

    Args:
        returns_df: A wide-format DataFrame of daily returns (tickers as columns).
        window: The rolling window length in trading days (e.g., 63 for ~3 months).
        z_score_threshold: The number of standard deviations to use as the outlier cutoff.

    Returns:
        A DataFrame of the same shape as input, with boolean flags where
        returns are considered outliers.
    """
    rolling_mean = returns_df.rolling(window=window, min_periods=window // 2).mean()
    rolling_std = returns_df.rolling(window=window, min_periods=window // 2).std()

    # Calculate Z-scores, handling potential division by zero
    z_scores = (returns_df - rolling_mean) / (rolling_std + 1e-9)
    
    outlier_flags = z_scores.abs() > z_score_threshold
    
    num_outliers = outlier_flags.sum().sum()
    print(f"Flagged {num_outliers} return outliers (Z-score > {z_score_threshold}).")
    
    return outlier_flags

def get_liquid_universe(
    prices_df: pd.DataFrame,
    volume_df: pd.DataFrame,
    rebalance_date: pd.Timestamp,
    lookback_days: int,
    min_trading_ratio: float = 0.90,
    min_avg_volume: int = 1000000
) -> List[str]:
    """
    Filters the asset universe based on liquidity criteria at a specific rebalance date.

    As per the specification, this function ensures that only assets with
    sufficient trading activity and history are included in the portfolio
    construction for a given period.

    Args:
        prices_df: A wide-format DataFrame of daily prices (tickers as columns).
        volume_df: A wide-format DataFrame of daily volumes (in number of shares).
        rebalance_date: The date on which the universe is being screened.
        lookback_days: The number of trading days to look back for evaluation.
        min_trading_ratio: The minimum fraction of days an asset must have a
                           valid price in the lookback window.
        min_avg_volume: The minimum average daily traded financial volume in BRL.
                        Calculated as the average of (daily_price * daily_volume).

    Returns:
        A list of ticker symbols that are eligible for inclusion in the model.
    """
    # 1. Define the lookback window
    lookback_start_date = rebalance_date - pd.Timedelta(days=lookback_days)
    
    prices_window = prices_df.loc[lookback_start_date:rebalance_date]
    volume_window = volume_df.loc[lookback_start_date:rebalance_date]

    if prices_window.empty or volume_window.empty:
        print("Warning: Not enough data in the lookback window to determine universe.")
        return []

    # 2. Minimum History / Trading Day Ratio
    # Count non-NaN prices for each asset in the window
    trading_days_count = prices_window.notna().sum()
    required_trading_days = len(prices_window) * min_trading_ratio
    
    traded_enough = trading_days_count >= required_trading_days
    
    # 3. Minimum Average Volume
    # Calculate average daily traded value (price * volume)
    # Using the average price over the window to be robust to price changes
    avg_price = prices_window.mean()
    avg_volume = volume_window.mean()
    avg_traded_value = avg_price * avg_volume
    
    enough_volume = avg_traded_value >= min_avg_volume
    
    # 4. Combine filters
    eligible_tickers = traded_enough & enough_volume
    
    liquid_universe = eligible_tickers[eligible_tickers].index.tolist()
    
    total_assets = len(eligible_tickers)
    print(f"Screened universe on {rebalance_date.date()}: {len(liquid_universe)} of {total_assets} assets passed liquidity filters.")
    
    return liquid_universe


if __name__ == '__main__':
    print("--- Running Clean Module Standalone Test ---")

    # 1. Create dummy input data
    dates = pd.to_datetime(pd.date_range("2023-01-01", "2023-06-30", freq="B"))
    
    prices_data = {
        # Liquid asset, always trades
        'LIQUID_A': np.linspace(100, 110, len(dates)),
        # Illiquid asset, many missing prices
        'ILLIQUID_B': np.linspace(50, 55, len(dates)),
        # Low volume asset
        'LOWVOL_C': np.linspace(20, 22, len(dates)),
        # Asset with a huge price spike (outlier)
        'OUTLIER_D': np.linspace(30, 33, len(dates)),
    }
    
    # Introduce NaNs for the illiquid asset
    prices_data['ILLIQUID_B'][10:40] = np.nan
    # Introduce an outlier
    prices_data['OUTLIER_D'][50] = 50 
    
    dummy_prices = pd.DataFrame(prices_data, index=dates)

    volume_data = {
        'LIQUID_A': [200000] * len(dates),
        'ILLIQUID_B': [150000] * len(dates),
        'LOWVOL_C': [10000] * len(dates), # Avg value will be ~20*10000 = 200k, below threshold
        'OUTLIER_D': [180000] * len(dates),
    }
    dummy_volume = pd.DataFrame(volume_data, index=dates)


    # --- Test 1: Outlier Flagging ---
    print("\n--- Testing flag_price_outliers ---")
    try:
        dummy_returns = dummy_prices.pct_change()
        outlier_flags = flag_price_outliers(dummy_returns, window=20, z_score_threshold=3.0)
        
        print("\nOutlier flags (showing only non-zero rows):")
        print(outlier_flags[outlier_flags.any(axis=1)])
        
        # Validation
        assert outlier_flags.loc['2023-03-15', 'OUTLIER_D'], "Expected outlier was not flagged!"
        print("\nOK: Outlier correctly identified.")
        
    except Exception as e:
        print(f"An error occurred: {e}")

    # --- Test 2: Liquidity Filtering ---
    print("\n--- Testing get_liquid_universe ---")
    try:
        rebalance_date = pd.to_datetime("2023-06-30")
        lookback_days = 180
        
        liquid_assets = get_liquid_universe(
            prices_df=dummy_prices,
            volume_df=dummy_volume,
            rebalance_date=rebalance_date,
            lookback_days=lookback_days,
            min_trading_ratio=0.85,  # 85% of days must have price
            min_avg_volume=1000000 # 1 Million BRL
        )
        
        print(f"\nEligible assets on {rebalance_date.date()}: {liquid_assets}")
        
        # Validation
        # LIQUID_A: Should pass (avg value is well > 1M BRL)
        # ILLIQUID_B: Should fail (too many NaNs)
        # LOWVOL_C: Should fail (avg value ~21*10k = 210k < 1M)
        # OUTLIER_D: Should pass (avg value ~31*180k > 1M)
        expected_universe = ['LIQUID_A', 'OUTLIER_D']
        assert set(liquid_assets) == set(expected_universe), f"Expected {expected_universe} but got {liquid_assets}"
        print("\nOK: Universe filtering works as expected.")
        
    except Exception as e:
        print(f"An error occurred: {e}")


###############################################################################
### FILE: src/b3alloc/preprocess/returns.py
###############################################################################
import pandas as pd
import numpy as np

def compute_returns(
    prices_df: pd.DataFrame,
    risk_free_df: pd.DataFrame,
    benchmark_df: pd.DataFrame,
    price_col: str = 'adj_close'
) -> pd.DataFrame:
    """
    Computes various return series from wide-format price data.

    Args:
        prices_df: Wide-format DataFrame of daily prices, index=date, columns=ticker.
        risk_free_df: DataFrame of daily and annualized risk-free rates.
        benchmark_df: DataFrame of the benchmark index price series.
        price_col: The column to use for price data (default: 'adj_close').

    Returns:
        A dictionary of DataFrames containing simple, log, excess simple,
        and excess log returns, plus market returns.
    """
    
    # The input prices_df is already in wide format, no pivot needed.
    prices_wide = prices_df

    # --- Equity Returns ---
    simple_returns = prices_wide.pct_change()
    log_returns = np.log(prices_wide / prices_wide.shift(1))
    
    # Align risk-free rate to the returns index
    rf_daily = risk_free_df['rf_daily'].reindex(simple_returns.index).ffill()
    
    # Calculate excess returns
    excess_simple_returns = simple_returns.subtract(rf_daily, axis=0)
    excess_log_returns = log_returns.subtract(rf_daily, axis=0)
    
    # --- Benchmark Market Returns ---
    market_simple_returns = benchmark_df[price_col].pct_change()
    market_log_returns = np.log(benchmark_df[price_col] / benchmark_df[price_col].shift(1))
    market_excess_returns = market_simple_returns - rf_daily
    
    returns_bundle = {
        "simple": simple_returns.dropna(how='all'),
        "log": log_returns.dropna(how='all'),
        "simple_excess": excess_simple_returns.dropna(how='all'),
        "log_excess": excess_log_returns.dropna(how='all'),
        "market_simple": market_simple_returns.dropna(),
        "market_log": market_log_returns.dropna(),
        "market_excess": market_excess_returns.dropna()
    }
    
    print("Successfully computed all return series.")
    return returns_bundle

if __name__ == '__main__':
    # Standalone test for the returns module
    print("--- Running Returns Module Standalone Test ---")

    # 1. Create dummy input data
    dates = pd.to_datetime(pd.date_range("2023-01-01", "2023-01-10", freq="B"))
    
    dummy_prices_long = pd.DataFrame({
        'date': dates.repeat(2),
        'ticker': ['TICKER_A', 'TICKER_B'] * len(dates),
        'adj_close': [
            100, 200, 101, 200, 102, 199, 103, 201, 102, 202,
            104, 203, 105, 202
        ]
    })
    
    dummy_rf_daily = pd.DataFrame({
        'rf_daily': [0.0005] * len(dates) # 0.05% per day
    }, index=dates)

    print("--- Input Data ---")
    print("Long Prices:")
    print(dummy_prices_long)
    print("\nDaily Risk-Free Rate:")
    print(dummy_rf_daily)

    # 2. Run the computation function
    try:
        returns_dict = compute_returns(dummy_prices_long, dummy_rf_daily)

        print("\n--- Output ---")
        
        print("\nLog Returns (for GARCH/VAR modeling):")
        print(returns_dict['log'].head())

        print("\nExcess Simple Returns (for performance calculation):")
        print(returns_dict['excess_simple'].head())

        # 3. Validation
        # Manual check for TICKER_A on 2023-01-03
        # Price change from 100 to 101
        expected_log_return = np.log(101/100)
        actual_log_return = returns_dict['log'].loc['2023-01-03', 'TICKER_A']
        
        expected_excess_simple = (101/100 - 1) - 0.0005
        actual_excess_simple = returns_dict['excess_simple'].loc['2023-01-03', 'TICKER_A']
        
        print("\n--- Validation ---")
        print(f"Expected Log Return for TICKER_A on 2023-01-03: {expected_log_return:.6f}")
        print(f"Actual Log Return: {actual_log_return:.6f}")
        assert np.isclose(expected_log_return, actual_log_return), "Log return mismatch!"

        print(f"Expected Excess Simple Return for TICKER_A on 2023-01-03: {expected_excess_simple:.6f}")
        print(f"Actual Excess Simple Return: {actual_excess_simple:.6f}")
        assert np.isclose(expected_excess_simple, actual_excess_simple), "Excess simple return mismatch!"
        
        print("\nOK: Test calculations match expected values.")

    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/returns/__init__.py
###############################################################################
# This file marks the 'b3alloc' directory as a Python package.
# It can also be used to define package-level variables or import key functions.

__version__ = "0.1.0"


###############################################################################
### FILE: src/b3alloc/returns/ff_view.py
###############################################################################
import pandas as pd
import numpy as np
import statsmodels.api as sm
from typing import Tuple, Dict, List, Optional
from joblib import Parallel, delayed

from ..config import FactorViewConfig

def estimate_factor_betas(
    asset_excess_returns: pd.Series,
    base_factor_returns: pd.DataFrame,
    fx_factor_return: Optional[pd.Series] = None
) -> Tuple[Optional[pd.Series], Optional[Dict]]:
    """
    Estimates factor betas for a single asset via OLS regression.

    Dynamically includes an FX factor in the regression if it is provided.

    Args:
        asset_excess_returns: A Series of daily excess returns for one asset.
        base_factor_returns: A DataFrame with daily returns for Mkt_excess, SMB, and HML.
        fx_factor_return: An optional Series of daily FX factor returns.

    Returns:
        A tuple containing:
        - A Series of estimated betas (including alpha and potentially fx_beta).
        - A dictionary of regression diagnostics (R-squared, p-values).
    """
    # Combine all potential regressors
    all_factors = [base_factor_returns]
    if fx_factor_return is not None:
        all_factors.append(fx_factor_return)
        
    # Align all data and drop any days with missing values
    data = pd.concat([asset_excess_returns] + all_factors, axis=1).dropna()
    
    if data.shape[0] < 60: # Not enough data for a meaningful regression
        return None, None

    Y = data.iloc[:, 0]
    X = data.iloc[:, 1:]
    X = sm.add_constant(X) # Add a constant to estimate alpha

    model = sm.OLS(Y, X)
    results = model.fit()
    
    betas = results.params
    betas = betas.rename({'const': 'alpha'})
    
    diagnostics = {
        'r_squared': results.rsquared_adj,
        'p_values': results.pvalues,
        'residual_variance': results.resid.var()
    }
    
    return betas, diagnostics


def estimate_factor_premia(
    factor_returns: pd.DataFrame,
    estimator: str = "long_term_mean"
) -> pd.Series:
    """
    Estimates the forward-looking factor premia.
    Now handles an arbitrary number of factors.
    """
    TRADING_DAYS_PER_YEAR = 252
    
    if estimator == "long_term_mean":
        daily_premia = factor_returns.mean()
        annual_premia = daily_premia * TRADING_DAYS_PER_YEAR
        return annual_premia
    else:
        raise NotImplementedError(f"Estimator '{estimator}' not implemented.")


def create_fama_french_view(
    asset_returns_df: pd.DataFrame,
    factor_returns_df: pd.DataFrame,
    config: FactorViewConfig,
    fx_returns_df: Optional[pd.DataFrame] = None,
    asset_fx_sensitivity: Optional[Dict[str, bool]] = None
) -> Tuple[pd.DataFrame, pd.Series]:
    """
    Generates expected excess returns based on a dynamic factor model.
    """
    print("Generating return views from factor model (FX-aware)...")
    n_jobs = -1
    asset_fx_sensitivity = asset_fx_sensitivity or {}

    # --- 1. Estimate betas for all assets in parallel ---
    print(f"Estimating factor betas for {asset_returns_df.shape[1]} assets...")
    beta_results = Parallel(n_jobs=n_jobs)(
        delayed(estimate_factor_betas)(
            asset_returns_df[ticker],
            factor_returns_df,
            fx_returns_df.iloc[:, 0] if fx_returns_df is not None and asset_fx_sensitivity.get(ticker, False) else None
        )
        for ticker in asset_returns_df.columns
    )
    
    all_betas = {}
    for ticker, (betas, diags) in zip(asset_returns_df.columns, beta_results):
        if betas is not None:
            all_betas[ticker] = betas
            
    if not all_betas:
        raise RuntimeError("Failed to estimate betas for any asset.")
        
    betas_df = pd.DataFrame(all_betas).T.fillna(0) # Fill missing betas (e.g., FX beta for non-sensitive assets) with 0

    # --- 2. Estimate historical factor premia ---
    print("Estimating historical factor premia (including FX)...")
    # Combine all possible factors for premia calculation
    full_factor_panel = factor_returns_df.copy()
    if fx_returns_df is not None:
        full_factor_panel = pd.concat([full_factor_panel, fx_returns_df], axis=1)

    factor_premia = estimate_factor_premia(
        full_factor_panel.dropna(), config.premium_estimator
    )
    
    # --- 3. Project expected returns ---
    print("Projecting expected returns from betas and premia...")
    factor_betas = betas_df.drop(columns=['alpha'], errors='ignore')
    
    # Align factor betas and premia columns before dot product
    aligned_betas, aligned_premia = factor_betas.align(factor_premia, join='left', axis=1)
    aligned_betas = aligned_betas.fillna(0) # Ensure any non-estimated betas are 0
    aligned_premia = aligned_premia.fillna(0)
    
    expected_returns = aligned_betas.dot(aligned_premia)
    
    if config.include_alpha and 'alpha' in betas_df.columns:
        expected_returns += betas_df['alpha']
    
    expected_returns.name = "ff_expected_returns"
    
    print("Successfully generated FX-aware Fama-French return view.")
    return betas_df, expected_returns


if __name__ == '__main__':
    from ..config import load_config
    from pathlib import Path
    import shutil

    # Create dummy config
    # ... (same as before, FactorViewConfig is sufficient) ...
    # (assuming previous test setup code for config)

    # --- GIVEN ---
    np.random.seed(42)
    n_obs, n_assets = 1000, 2
    dates = pd.date_range("2018-01-01", periods=n_obs)
    tickers = ['PETR4.SA', 'AAPL34.SA'] # A domestic stock and a BDR
    
    factors = pd.DataFrame(np.random.randn(n_obs, 3) * 0.01, index=dates, columns=['mkt_excess', 'smb', 'hml'])
    fx_factor = pd.DataFrame(np.random.randn(n_obs, 1) * 0.005, index=dates, columns=['USD_BRL_log_return'])
    
    # Create asset returns with specific sensitivities
    asset_returns = pd.DataFrame(index=dates, columns=tickers)
    asset_returns['PETR4.SA'] = 1.2 * factors['mkt_excess'] + np.random.randn(n_obs) * 0.015 # No FX beta
    asset_returns['AAPL34.SA'] = 0.8 * factors['mkt_excess'] + 1.0 * fx_factor['USD_BRL_log_return'] + np.random.randn(n_obs) * 0.01 # High FX beta
    
    # Define which assets are sensitive
    fx_sensitivity_map = {'AAPL34.SA': True}
    
    # Dummy config setup
    dummy_yaml = """
    return_engine:
      factor:
        lookback_days: 756
        include_alpha: false
        premium_estimator: long_term_mean
      var: {max_lag: 1, criterion: bic, log_returns: true}
    data: {start: '', end: '', tickers_file: '', selic_series: 0, publish_lag_days: 0}
    risk_engine: {garch: {dist: gaussian, min_obs: 50, refit_freq_days: 21}, dcc: {a_init: 0.02, b_init: 0.97}, shrinkage: {method: ledoit_wolf, floor: 0.0}}
    black_litterman: {tau: 0.05, confidence: {method: rmse_based, factor_scaler: 1.0, var_scaler: 1.0}}
    optimizer: {objective: max_sharpe, long_only: true, name_cap: 0.1, sector_cap: 0.25, turnover_penalty_bps: 0}
    backtest: {lookback_years: 1, rebalance: monthly, start: '', end: '', costs_bps: 0}
    """
    temp_dir = Path("./temp_ff_view_config"); temp_dir.mkdir(exist_ok=True)
    dummy_config_path = temp_dir / "config.yaml"
    dummy_config_path.write_text(dummy_yaml)
    config = load_config(dummy_config_path)


    # --- WHEN ---
    betas, mu_view = create_fama_french_view(
        asset_returns,
        factors,
        config.return_engine.factor,
        fx_returns_df=fx_factor,
        asset_fx_sensitivity=fx_sensitivity_map
    )
    
    # --- THEN ---
    print("\n--- Estimated Betas (including FX) ---")
    print(betas)
    
    print("\n--- Final Expected Returns View (Annualized) ---")
    print(mu_view)
    
    # --- Validation ---
    # 1. Betas for the BDR should include a non-zero FX beta
    fx_beta_col_name = fx_factor.columns[0]
    assert fx_beta_col_name in betas.columns
    assert betas.loc['AAPL34.SA', fx_beta_col_name] > 0.8 # Should be close to 1.0
    
    # 2. Betas for the domestic stock should have a zero FX beta
    assert np.isclose(betas.loc['PETR4.SA', fx_beta_col_name], 0.0)
    
    print("\nOK: Model correctly estimates FX beta for sensitive assets and zero for others.")
    
    # 3. Check mu_view calculation
    # Expected return for AAPL34.SA should be influenced by the FX premium
    combined_factors = pd.concat([factors, fx_factor], axis=1)
    premia = estimate_factor_premia(combined_factors)
    
    expected_mu_aapl = (betas.loc['AAPL34.SA'][premia.index] * premia).sum()
    assert np.isclose(mu_view['AAPL34.SA'], expected_mu_aapl)
    print("OK: Expected return calculation correctly incorporates the FX premium.")
    
    # Cleanup
    shutil.rmtree(temp_dir)


###############################################################################
### FILE: src/b3alloc/returns/var_view.py
###############################################################################
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.api import VAR
from typing import Tuple, Dict, Optional

from ..config import VarViewConfig

def create_var_view(
    log_returns_df: pd.DataFrame,
    config: VarViewConfig
) -> Tuple[Optional[pd.Series], Dict]:
    """
    Generates expected excess returns based on a Vector Autoregression (VAR) model.

    This function fits a VAR(p) model to the multivariate time series of
    log returns to produce a one-step-ahead forecast.

    Args:
        log_returns_df: A wide-format DataFrame of daily log excess returns for
                        all assets in the universe.
        config: The VarViewConfig object with model parameters.

    Returns:
        A tuple containing:
        - A Series of the final annualized expected excess returns (the "view").
        - A dictionary of model diagnostics (selected lag order, stability).
    """
    print("Generating return views from VAR model...")
    
    # VAR models are sensitive to NaNs. We must use a common, non-null history.
    data = log_returns_df.dropna()
    
    if data.shape[0] < 100 or data.shape[1] < 2:
        print("Warning: Not enough data or assets for a meaningful VAR model. Skipping.")
        return None, {"error": "Insufficient data"}
        
    diagnostics = {}
    
    # 1. Fit the VAR model
    # The statsmodels VAR implementation automatically selects the lag order if
    # maxlags is provided to the fit method.
    model = VAR(data)
    
    try:
        # The `fit` method can select the best lag order automatically
        results = model.fit(maxlags=config.max_lag, ic=config.criterion)
        
        diagnostics['selected_lag_order'] = results.k_ar
        diagnostics['is_stable'] = results.is_stable()
        # Per spec, the forecast error variance is crucial for BL confidence
        diagnostics['sigma_u'] = results.sigma_u
        
        if not diagnostics['is_stable']:
            print("Warning: Fitted VAR model is unstable. Forecasts may be unreliable.")
            # Proceeding anyway as per spec, but flagging it.
            
    except Exception as e:
        print(f"Error fitting VAR model: {e}")
        diagnostics['error'] = str(e)
        return None, diagnostics

    # 2. Generate one-step-ahead forecast
    # We need the last `p` observations to make the next forecast.
    p = results.k_ar
    last_observations = data.values[-p:]
    
    forecast_log_daily = results.forecast(y=last_observations, steps=1)
    
    # The forecast is a numpy array; convert it to a pandas Series
    forecast_log_daily_s = pd.Series(forecast_log_daily[0], index=data.columns)

    # 3. Convert forecast to the required format (annualized simple excess return)
    # The VAR model was fit on daily log returns.
    # Step 3a: Convert daily log return to daily simple return
    # simple_return = exp(log_return) - 1
    forecast_simple_daily = np.exp(forecast_log_daily_s) - 1
    
    # Step 3b: Annualize the daily simple return
    TRADING_DAYS_PER_YEAR = 252
    mu_view = forecast_simple_daily * TRADING_DAYS_PER_YEAR
    mu_view.name = "var_expected_returns"

    print(f"Successfully generated VAR view with lag order p={p}.")
    return mu_view, diagnostics


if __name__ == '__main__':
    from ..config import load_config
    from pathlib import Path
    import shutil

    # Create dummy config
    dummy_yaml = """
    return_engine:
      var:
        max_lag: 10
        criterion: bic
        log_returns: true
      factor: {lookback_days: 756, include_alpha: false, premium_estimator: long_term_mean}
    # Add other sections to satisfy pydantic
    data: {start: '', end: '', tickers_file: '', selic_series: 0, publish_lag_days: 0}
    risk_engine: {garch: {dist: gaussian, min_obs: 50, refit_freq_days: 21}, dcc: {a_init: 0.02, b_init: 0.97}, shrinkage: {method: ledoit_wolf, floor: 0.0}}
    black_litterman: {tau: 0.05, confidence: {method: rmse_based, factor_scaler: 1.0, var_scaler: 1.0}}
    optimizer: {objective: max_sharpe, long_only: true, name_cap: 0.1, sector_cap: 0.25, turnover_penalty_bps: 0}
    backtest: {lookback_years: 1, rebalance: monthly, start: '', end: '', costs_bps: 0}
    """
    temp_dir = Path("./temp_var_config"); temp_dir.mkdir(exist_ok=True)
    dummy_config_path = temp_dir / "var_config.yaml"
    dummy_config_path.write_text(dummy_yaml)
    config = load_config(dummy_config_path)

    # Generate synthetic data exhibiting mean reversion
    np.random.seed(42)
    n_obs, n_assets = 500, 2
    dates = pd.date_range("2020-01-01", periods=n_obs)
    tickers = ['MEAN_REVERTER', 'MOMENTUM_STOCK']
    
    returns = pd.DataFrame(np.random.randn(n_obs, n_assets) * 0.01, index=dates, columns=tickers)
    
    # Create mean-reverting behavior in the first asset
    for t in range(1, n_obs):
        returns.iloc[t, 0] = -0.3 * returns.iloc[t-1, 0] + 0.1 * returns.iloc[t-1, 1] + np.random.randn() * 0.01
    
    # Add a recent negative shock to the mean-reverting asset
    returns.iloc[-1, 0] = -0.05
    
    print("\n--- Last observation (input to forecast) ---")
    print(returns.tail(1))
    
    try:
        mu_view, diags = create_var_view(returns, config.return_engine.var)
        
        print("\n--- Diagnostics ---")
        if diags.get("error") is None:
            print(diags)
            
            print("\n--- Final Expected Returns View (Annualized) ---")
            print(mu_view)
            
            # Validation
            assert diags['selected_lag_order'] > 0
            
            # Because the last return of MEAN_REVERTER was very negative, a mean-reverting
            # model should predict a positive return for the next period.
            assert mu_view['MEAN_REVERTER'] > 0, "VAR should predict a positive return after a negative shock for a mean-reverting asset."
            print("\nOK: VAR forecast is consistent with mean-reverting dynamics.")
        else:
            print(f"Test failed with error: {diags['error']}")

    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()
    finally:
        shutil.rmtree(temp_dir)


###############################################################################
### FILE: src/b3alloc/risk/__init__.py
###############################################################################
# This file marks the 'b3alloc' directory as a Python package.
# It can also be used to define package-level variables or import key functions.

__version__ = "0.1.0"


###############################################################################
### FILE: src/b3alloc/risk/dcc.py
###############################################################################
import pandas as pd
import numpy as np
from typing import Tuple, Optional
from arch import arch_model

# Define type hints for clarity
DccFitResult = any # The specific type is complex
ForecastedCorrMatrix = Optional[np.ndarray]

def fit_dcc_model(
    standardized_residuals: pd.DataFrame
) -> Tuple[Optional[DccFitResult], ForecastedCorrMatrix]:
    """
    Fits a DCC(1,1)-GARCH(1,1) model to a set of standardized residuals.

    This implements the second stage of the DCC-GARCH process. By feeding the
    model pre-standardized residuals, we focus the estimation on the DCC
    parameters that govern the time-varying correlations.

    Args:
        standardized_residuals: A DataFrame where each column is an asset's
                                time series of standardized residuals (returns
                                divided by conditional volatility from GARCH).
                                The index is the date.

    Returns:
        A tuple containing:
        - The fitted DCC model result object (or None if convergence fails).
        - The one-step-ahead forecasted NxN correlation matrix (or None if fails).
    """
    if standardized_residuals.isnull().values.any():
        print("Warning: DCC input contains NaNs. Filling with 0 before fitting.")
        standardized_residuals = standardized_residuals.fillna(0)

    num_assets = standardized_residuals.shape[1]
    if num_assets < 2:
        print("Warning: DCC model requires at least 2 assets. Skipping.")
        return None, None
    
    # Define the GARCH specification for the DCC model.
    # The 'arch' library is designed to handle multiple series by passing
    # the entire DataFrame of residuals to the model constructor.
    # The original loop-based construction was incorrect.
    vol_model = arch_model(
        standardized_residuals, 
        vol='Garch', 
        p=1, q=1, 
        cov_type='DCC', 
        cov_p=1, cov_q=1
    )

    try:
        # Fit the model. Scaling is not needed as residuals are already standardized.
        # Options can be tuned for better convergence if needed.
        fit_result = vol_model.fit(disp='off', options={'maxiter': 500})
        
        if not fit_result.convergence_flag == 0:
            print(f"Warning: DCC model did not converge. Status: {fit_result.convergence_flag}")
            return None, None

    except Exception as e:
        print(f"Warning: DCC model failed to fit. Error: {e}")
        return None, None
        
    # Forecast one step ahead
    forecast = fit_result.forecast(horizon=1, reindex=False)
    
    # The forecasted correlation matrix is nested in the output.
    # The structure is forecast.correlation['h.1'][date_index]
    # We want the NxN matrix for the single forecasted date.
    forecast_date = forecast.correlation.index[0]
    corr_matrix = forecast.correlation.loc[forecast_date].values
    
    return fit_result, corr_matrix


if __name__ == '__main__':
    print("--- Running DCC Module Standalone Test ---")

    # 1. Generate synthetic standardized residuals with a changing correlation structure
    np.random.seed(42)
    n_obs = 1500
    dates = pd.date_range("2015-01-01", periods=n_obs)
    
    # Regime 1: High positive correlation (first 750 days)
    corr1 = np.array([[1.0, 0.8], [0.8, 1.0]])
    # Regime 2: Negative correlation (last 750 days)
    corr2 = np.array([[1.0, -0.6], [-0.6, 1.0]])
    
    chol1 = np.linalg.cholesky(corr1)
    chol2 = np.linalg.cholesky(corr2)
    
    # Independent random variables
    innovations = np.random.normal(0, 1, size=(n_obs, 2))
    
    # Create correlated residuals
    residuals1 = (chol1 @ innovations[:750].T).T
    residuals2 = (chol2 @ innovations[750:].T).T
    
    residuals = np.vstack([residuals1, residuals2])
    residuals_df = pd.DataFrame(residuals, index=dates, columns=['ASSET_X', 'ASSET_Y'])

    print("\n--- Testing with synthetic data ---")
    print(f"True correlation in first half: {corr1[0,1]}")
    print(f"True correlation in second half: {corr2[0,1]}")
    
    try:
        dcc_fit, forecasted_corr = fit_dcc_model(residuals_df)
        
        assert dcc_fit is not None, "DCC model failed to fit on good data."
        assert forecasted_corr is not None, "DCC forecast should not be None."
        
        print("\nDCC model fitted successfully.")
        
        print("\nForecasted Correlation Matrix (t+1):")
        print(forecasted_corr)
        
        # Validation
        # The forecasted correlation should be much closer to the second regime's
        # correlation (-0.6) than the first's (0.8).
        forecasted_xy_corr = forecasted_corr[0, 1]
        
        print(f"\nForecasted correlation between X and Y: {forecasted_xy_corr:.4f}")
        assert forecasted_xy_corr < 0, "Forecasted correlation should be negative."
        assert abs(forecasted_xy_corr - (-0.6)) < 0.2, "Forecasted correlation is not close to the recent regime."
        print("OK: Forecasted correlation correctly reflects the most recent correlation regime.")

        # Check matrix properties
        assert forecasted_corr.shape == (2, 2), "Matrix shape is incorrect."
        assert np.allclose(np.diag(forecasted_corr), [1.0, 1.0]), "Diagonal must be all ones."
        assert np.allclose(forecasted_corr, forecasted_corr.T), "Matrix must be symmetric."
        print("OK: Forecasted matrix has valid properties.")

    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/risk/garch.py
###############################################################################
import pandas as pd
import numpy as np
from typing import Tuple, Optional
from arch import arch_model
from arch.univariate import GARCH, StudentsT, Normal

# Define type hints for clarity
FitResult = any # The specific type is complex, 'any' is sufficient here
ForecastResult = Optional[float]

def fit_garch_model(
    returns_series: pd.Series,
    dist: str = 'gaussian'
) -> Tuple[Optional[FitResult], ForecastResult]:
    """
    Fits a GARCH(1,1) model to a single asset's return series.

    Args:
        returns_series: A pandas Series of demeaned log excess returns for one asset.
        dist: The distribution to use for the innovations ('gaussian' or 'student-t').

    Returns:
        A tuple containing:
        - The fitted model result object (or None if convergence fails).
        - The one-step-ahead forecasted variance (or None if convergence fails).
    """
    if returns_series.isnull().any() or returns_series.empty:
        # print(f"Warning: GARCH input for {returns_series.name} contains NaNs or is empty. Skipping.")
        return None, None

    # GARCH models work best on demeaned series. We assume returns are already excess returns.
    # The model automatically estimates a mean, so we don't need to subtract it manually.
    # Rescaling by 100 can help with optimizer convergence.
    scaled_returns = returns_series * 100

    # The arch library is very particular about the distribution name.
    dist_name = str(dist).lower()
    if 'student' in dist_name:
        dist_name = 'student-t'
    elif 'gaussian' in dist_name or 'normal' in dist_name:
        dist_name = 'gaussian'
    else:
        dist_name = 'student-t' # Default

    # Define the GARCH(1,1) model
    model = arch_model(
        scaled_returns,
        vol='Garch',
        p=1,
        o=0,
        q=1,
        dist=dist_name
    )
    
    try:
        # Fit the model. disp='off' suppresses the output.
        fit_result = model.fit(update_freq=0, disp='off')
        
        # Check for successful convergence
        if not fit_result.convergence_flag == 0:
            # print(f"Warning: GARCH for {returns_series.name} did not converge. Status: {fit_result.convergence_flag}")
            return None, None
            
    except Exception as e:
        # print(f"Warning: GARCH for {returns_series.name} failed to fit. Error: {e}")
        return None, None

    # Forecast one step ahead
    forecast = fit_result.forecast(horizon=1, reindex=False)
    
    # The result is in a DataFrame. We need to extract the variance value.
    # The column name is 'h.1' for one-step-ahead variance.
    # Remember to scale it back down since we scaled the inputs by 100.
    forecasted_variance = forecast.variance.iloc[0, 0] / (100**2)
    
    return fit_result, forecasted_variance


if __name__ == '__main__':
    print("--- Running GARCH Module Standalone Test ---")

    # 1. Generate a synthetic return series with volatility clustering
    np.random.seed(42)
    n_obs = 1000
    dates = pd.date_range("2010-01-01", periods=n_obs)
    
    # Simulate a GARCH(1,1) process
    omega = 0.01
    alpha = 0.1
    beta = 0.88
    
    true_vol = np.zeros(n_obs)
    returns = np.zeros(n_obs)
    true_vol[0] = np.sqrt(omega / (1 - alpha - beta))
    returns[0] = np.random.normal(0, true_vol[0])
    
    for t in range(1, n_obs):
        true_vol[t] = np.sqrt(omega + alpha * returns[t-1]**2 + beta * true_vol[t-1]**2)
        returns[t] = np.random.normal(0, true_vol[t])
        
    returns_series = pd.Series(returns, index=dates, name="SYNTHETIC_GARCH")

    print("\n--- Testing with synthetic GARCH data (Gaussian) ---")
    try:
        garch_fit, variance_forecast = fit_garch_model(returns_series, dist='gaussian')

        assert garch_fit is not None, "Model failed to fit on good data."
        assert variance_forecast is not None, "Forecast should not be None."
        
        print("Model fitted successfully.")
        print(garch_fit.summary())
        print(f"\nOne-step-ahead variance forecast: {variance_forecast:.8f}")
        
        # Check if estimated params are reasonable
        est_alpha = garch_fit.params['alpha[1]']
        est_beta = garch_fit.params['beta[1]']
        assert abs(est_alpha - alpha) < 0.05, f"Alpha estimate {est_alpha} is too far from true value {alpha}."
        assert abs(est_beta - beta) < 0.05, f"Beta estimate {est_beta} is too far from true value {beta}."
        print("\nOK: Estimated parameters are close to true simulation parameters.")

    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()

    print("\n--- Testing failure case (constant series) ---")
    constant_series = pd.Series([0.001] * 200, name="CONSTANT")
    garch_fit_fail, var_fail = fit_garch_model(constant_series)
    
    assert garch_fit_fail is None, "Model should fail on constant series."
    assert var_fail is None, "Forecast should be None on failure."
    print("OK: Model correctly failed to converge on a constant series.")


###############################################################################
### FILE: src/b3alloc/risk/risk_engine.py
###############################################################################
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from joblib import Parallel, delayed

from .garch import fit_garch_model
from .dcc import fit_dcc_model
from .shrinkage import apply_ledoit_wolf_shrinkage
from ..config import RiskEngineConfig

def build_covariance_matrix(
    returns_df: pd.DataFrame,
    config: RiskEngineConfig,
    fx_returns_df: Optional[pd.DataFrame] = None
) -> Tuple[pd.DataFrame, Dict]:
    """
    Orchestrates the construction of the forward-looking covariance matrix, now
    with optional FX factor integration.

    If an FX return series is provided, it is included in the GARCH-DCC
    estimation to capture its volatility and correlations with other assets.
    It is then removed before the final matrix is returned to the optimizer.

    Args:
        returns_df: A wide-format DataFrame of log excess returns for the assets.
        config: The RiskEngineConfig object.
        fx_returns_df: An optional single-column DataFrame of FX log returns.

    Returns:
        A tuple containing:
        - A pandas DataFrame representing the final, shrunk NxN asset covariance matrix.
        - A dictionary containing diagnostic information.
    """
    print("Building dynamic covariance matrix (FX-aware)...")
    if returns_df.isnull().all().any():
        raise ValueError("A column in the returns_df is all NaN. Check input data.")

    # --- 1. Combine Asset and FX Returns ---
    model_input_returns = returns_df.copy()
    fx_col_name = None
    if fx_returns_df is not None and not fx_returns_df.empty:
        # Align and combine
        fx_col_name = fx_returns_df.columns[0]
        model_input_returns = pd.concat([returns_df, fx_returns_df], axis=1).dropna(axis=0, how='any')
        print(f"  -> Including '{fx_col_name}' in risk estimation.")

    n_jobs = -1
    
    # --- 2. Univariate GARCH Fitting (Parallelized) ---
    print(f"Fitting GARCH(1,1) for {model_input_returns.shape[1]} series (assets + FX)...")
    # Fit GARCH models in parallel
    garch_results = Parallel(n_jobs=n_jobs)(
        delayed(fit_garch_model)(
            model_input_returns[ticker].dropna(),
            "student-t"  # Hardcode the distribution to isolate the error
        )
        for ticker in model_input_returns.columns
    )
    
    # --- 3. Process GARCH results ---
    successful_series = []
    standardized_residuals = {}
    variance_forecasts = {}

    for series_name, (fit_result, var_forecast) in zip(model_input_returns.columns, garch_results):
        if fit_result and var_forecast:
            successful_series.append(series_name)
            std_resid = fit_result.resid / fit_result.conditional_volatility
            standardized_residuals[series_name] = std_resid
            variance_forecasts[series_name] = var_forecast
        else:
            print(f"  -> GARCH for {series_name} failed. Excluding from covariance matrix.")

    if len(successful_series) < 2:
        raise RuntimeError("Covariance matrix requires at least 2 series with successful GARCH fits.")
        
    residuals_df = pd.DataFrame(standardized_residuals).reindex(model_input_returns.index).loc[:, successful_series]

    # --- 4. Multivariate DCC Fitting ---
    print(f"Fitting DCC(1,1) on {residuals_df.shape[1]} standardized residual series...")
    dcc_fit, corr_forecast = fit_dcc_model(residuals_df)
    
    if corr_forecast is None:
        raise RuntimeError("DCC model failed to converge. Cannot build covariance matrix.")

    corr_forecast_df = pd.DataFrame(corr_forecast, index=successful_series, columns=successful_series)

    # --- 5. Reconstruct Dynamic Covariance (H_t) ---
    vol_forecasts = np.sqrt(pd.Series(variance_forecasts))
    D_t = np.diag(vol_forecasts)
    H_t = D_t @ corr_forecast_df.values @ D_t
    H_t_df = pd.DataFrame(H_t, index=successful_series, columns=successful_series)

    # --- 6. Ledoit-Wolf Shrinkage Integration ---
    print("Applying Ledoit-Wolf shrinkage to stabilize the GARCH-DCC matrix...")
    # We shrink the GARCH-DCC forecast (H_t) towards a stable target,
    # using the historical returns to estimate the optimal shrinkage intensity (delta).
    shrunk_cov_matrix, shrinkage_delta = apply_ledoit_wolf_shrinkage(
        sample_cov=H_t_df.values,
        returns_array=model_input_returns[successful_series].values
    )
    final_shrunk_cov_full_df = pd.DataFrame(
        shrunk_cov_matrix, 
        index=successful_series, 
        columns=successful_series
    )
    
    # --- 7. Finalize and Drop FX Factor ---
    # As per spec, drop the FX factor row/col from the final matrix
    # that goes to the optimizer.
    final_asset_cov_df = final_shrunk_cov_full_df
    if fx_col_name and fx_col_name in final_asset_cov_df.columns:
        final_asset_cov_df = final_asset_cov_df.drop(index=fx_col_name, columns=fx_col_name)
        print(f"  -> Dropping '{fx_col_name}' from final optimizer covariance matrix.")
    
    print("Successfully built final covariance matrix.")
    
    diagnostics = {
        'num_series_in': model_input_returns.shape[1],
        'num_assets_out': final_asset_cov_df.shape[1],
        'shrinkage_intensity_delta': shrinkage_delta,
        'full_covariance_matrix_with_fx': final_shrunk_cov_full_df # Store for attribution
    }
    
    return final_asset_cov_df, diagnostics

if __name__ == '__main__':
    from ..config import load_config
    from pathlib import Path
    import shutil

    # Create a dummy config
    # ... (assuming previous test setup code for config) ...
    # ... as the RiskEngineConfig itself doesn't need to change ...
    
    # --- GIVEN ---
    np.random.seed(42)
    n_obs, n_assets = 500, 3
    dates = pd.date_range("2020-01-01", periods=n_obs)
    tickers = [f"ASSET_{i}" for i in range(n_assets)]
    test_returns = pd.DataFrame(np.random.randn(n_obs, n_assets) * 0.02, index=dates, columns=tickers)
    
    # Create an FX series that is correlated with one of the assets
    fx_returns = pd.DataFrame(
        0.5 * test_returns['ASSET_1'] + np.random.randn(n_obs) * 0.005,
        columns=['USD_BRL_ret']
    )
    
    # Mock config object
    mock_config = RiskEngineConfig(
        garch={'dist': 'gaussian', 'min_obs': 250, 'refit_freq_days': 21},
        dcc={'a_init': 0.02, 'b_init': 0.97},
        shrinkage={'method': 'ledoit_wolf', 'floor': 0}
    )

    try:
        # --- WHEN ---
        # Run the engine with the FX factor
        sigma, diags = build_covariance_matrix(test_returns, mock_config, fx_returns_df=fx_returns)

        # --- THEN ---
        print("\n--- Output ---")
        print("Final Asset Covariance Matrix (Sigma) shape:", sigma.shape)
        print("Columns:", sigma.columns.tolist())
        
        # --- Validation ---
        # 1. Final matrix should only contain assets, not the FX factor
        assert 'USD_BRL_ret' not in sigma.columns
        assert sigma.shape == (n_assets, n_assets)
        
        # 2. The full matrix (with FX) should be available in diagnostics
        full_sigma = diags['full_covariance_matrix_with_fx']
        assert 'USD_BRL_ret' in full_sigma.columns
        assert full_sigma.shape == (n_assets + 1, n_assets + 1)
        
        # 3. Check the correlation
        # The correlation between ASSET_1 and USD_BRL should be high
        full_corr = np.linalg.inv(np.diag(np.sqrt(np.diag(full_sigma)))) @ full_sigma @ np.linalg.inv(np.diag(np.sqrt(np.diag(full_sigma))))
        fx_asset1_corr = full_corr.loc['ASSET_1', 'USD_BRL_ret']
        print(f"\nEstimated correlation between ASSET_1 and USD_BRL: {fx_asset1_corr:.4f}")
        assert fx_asset1_corr > 0.5
        
        print("\nOK: Risk engine correctly incorporates FX correlations and provides the correctly shaped final matrix.")

    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/risk/shrinkage.py
###############################################################################
import pandas as pd
import numpy as np
from typing import Tuple
from sklearn.covariance import ledoit_wolf

def apply_ledoit_wolf_shrinkage(
    sample_cov: np.ndarray,
    returns_array: np.ndarray,
    shrinkage_target: str = 'constant_correlation'
) -> Tuple[np.ndarray, float]:
    """
    Applies Ledoit-Wolf shrinkage to an empirical covariance matrix.

    This implementation follows the direct formula for shrinkage, allowing the
    GARCH-DCC covariance forecast to be used as the "sample_cov".

    Args:
        sample_cov: The NxN sample covariance matrix (e.g., from GARCH-DCC).
        returns_array: The NxT array of returns used to estimate the shrinkage intensity.
        shrinkage_target: The structured matrix to shrink towards.
                         'constant_correlation' is specified in the blueprint.

    Returns:
        A tuple containing:
        - The shrunken NxN covariance matrix (as a numpy array).
        - The estimated optimal shrinkage coefficient (delta).
    """
    if sample_cov.shape[0] != sample_cov.shape[1]:
        raise ValueError("Sample covariance must be a square matrix.")
    if sample_cov.shape[0] != returns_array.shape[1]:
        raise ValueError("Dimensions of sample_cov and returns_array do not match.")

    n_assets = sample_cov.shape[0]
    n_obs = returns_array.shape[0]

    # --- 1. Define the shrinkage target (F) ---
    if shrinkage_target == 'constant_correlation':
        asset_variances = np.diag(sample_cov).reshape(-1, 1)
        sqrt_var = np.sqrt(asset_variances)
        corr_matrix = sample_cov / (sqrt_var @ sqrt_var.T)
        
        # Average correlation
        upper_triangle_indices = np.triu_indices(n_assets, k=1)
        avg_corr = np.mean(corr_matrix[upper_triangle_indices])
        
        # Build target matrix F
        F = np.full((n_assets, n_assets), avg_corr)
        np.fill_diagonal(F, 1.0)
        F = np.diag(np.sqrt(asset_variances).flatten()) @ F @ np.diag(np.sqrt(asset_variances).flatten())
    else:
        raise NotImplementedError(f"Shrinkage target '{shrinkage_target}' not implemented.")

    # --- 2. Estimate the shrinkage intensity (delta) ---
    # This is a simplified version of the formula from Ledoit & Wolf (2004)
    # delta = sum(Var(s_ij)) / sum((s_ij - f_ij)^2)
    
    # Estimate pi-hat (sum of variances of sample covariance entries)
    y_t = returns_array
    y_t_centered = y_t - y_t.mean(axis=0)
    pi_hat_mat = np.zeros_like(sample_cov)
    for t in range(n_obs):
        pi_hat_mat += np.outer(y_t_centered[t]**2, y_t_centered[t]**2)
    pi_hat = np.sum(pi_hat_mat / n_obs - sample_cov**2)

    # Estimate rho-hat (sum of squared errors between sample and target)
    rho_hat = np.sum((sample_cov - F)**2)
    
    # Shrinkage constant
    delta = pi_hat / rho_hat
    delta = np.clip(delta, 0, 1) # Ensure delta is between 0 and 1

    # --- 3. Apply shrinkage ---
    shrunk_cov = (1 - delta) * sample_cov + delta * F
    
    return shrunk_cov, delta


if __name__ == '__main__':
    print("--- Running Shrinkage Module Standalone Test ---")

    # 1. Create a challenging scenario for covariance estimation
    n_samples = 50
    n_features = 80
    np.random.seed(42)
    
    returns_array = np.random.randn(n_samples, n_features) * 0.02
    
    # Calculate the empirical covariance matrix, which will be ill-conditioned
    sample_cov_ill = np.cov(returns_array, rowvar=False)

    print(f"Testing with a challenging data shape: {n_samples} samples, {n_features} assets.")
    
    try:
        cond_sample = np.linalg.cond(sample_cov_ill)
    except np.linalg.LinAlgError:
        cond_sample = np.inf
    print(f"\nCondition number of original matrix: {cond_sample:,.2f}")
    assert np.isinf(cond_sample), "Test setup should result in a singular matrix."

    # 4. Apply the direct Ledoit-Wolf shrinkage
    try:
        shrunk_cov, delta = apply_ledoit_wolf_shrinkage(sample_cov_ill, returns_array)
        
        print(f"\nApplied Ledoit-Wolf shrinkage with optimal delta = {delta:.4f}")
        
        # 5. Calculate the condition number of the shrunken matrix
        cond_shrunk = np.linalg.cond(shrunk_cov)
        print(f"Condition number of shrunken covariance: {cond_shrunk:,.2f}")
        
        assert np.isfinite(cond_shrunk), "Shrunken matrix should be well-conditioned."
        assert cond_shrunk < 1e6, "Condition number should be significantly improved." # Check it's not still huge
        print("\nOK: Shrinkage successfully produced a well-conditioned matrix.")

    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


###############################################################################
### FILE: src/b3alloc/taxes/__init__.py
###############################################################################
# This file marks the 'b3alloc' directory as a Python package.
# It can also be used to define package-level variables or import key functions.

__version__ = "0.1.0"


###############################################################################
### FILE: src/b3alloc/taxes/darf_reporter.py
###############################################################################
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any

def generate_darf_excel_report(
    monthly_tax_summaries: List[Dict[str, Any]],
    output_dir: Path
) -> Path:
    """
    Generates an Excel report summarizing monthly tax liabilities.

    Args:
        monthly_tax_summaries: A list of dictionaries, where each dict is the
                               output from `calculate_monthly_taxes`.
        output_dir: The directory where the report will be saved.

    Returns:
        The path to the generated Excel file.
    """
    report_data = []
    for summary in monthly_tax_summaries:
        if not summary or 'details' not in summary:
            continue
            
        month_str = f"{summary['year']}-{summary['month']:02d}"
        
        for asset_class, details in summary['details'].items():
            if isinstance(details, dict) and details.get('gross_sales', 0) > 0:
                report_data.append({
                    'Month': month_str,
                    'Asset Class': asset_class,
                    'Gross Sales (R$)': details.get('gross_sales', 0),
                    'Net Gain/Loss (R$)': details.get('net_gain', 0),
                    'Tax Due (R$)': details.get('tax_due', 0),
                    'DARF Code': '6015' if details.get('tax_due', 0) > 0 else 'N/A'
                })

    if not report_data:
        print("No taxable events to report. DARF report will not be generated.")
        return None

    report_df = pd.DataFrame(report_data)
    
    # --- Formatting the Excel Output ---
    output_path = output_dir / "darf_tax_report.xlsx"
    writer = pd.ExcelWriter(output_path, engine='openpyxl')
    report_df.to_excel(writer, sheet_name='Monthly Tax Summary', index=False)
    
    # Auto-adjust column widths for readability
    worksheet = writer.sheets['Monthly Tax Summary']
    for column in worksheet.columns:
        max_length = 0
        column_letter = column[0].column_letter
        if report_df[column[0].value].dtype == 'object':
            max_length = report_df[column[0].value].astype(str).map(len).max()
        else:
            max_length = len(str(column[0].value))
        
        adjusted_width = (max_length + 2)
        worksheet.column_dimensions[column_letter].width = adjusted_width

    writer.close()
    
    print(f"Successfully generated DARF tax report at: {output_path}")
    return output_path


if __name__ == '__main__':
    print("--- Running DARF Reporter Module Standalone Test ---")
    
    # --- GIVEN ---
    # Dummy data mirroring the output of the tax_tracker
    dummy_tax_data = [
        {
            'year': 2023, 'month': 3, 'total_tax_due': 855.0,
            'details': {
                'STOCK': {'gross_sales': 43000.0, 'net_gain': 5500.0, 'tax_due': 825.0},
                'BDR': {'gross_sales': 3200.0, 'net_gain': 200.0, 'tax_due': 30.0},
                'ETF': {'gross_sales': 1050.0, 'net_gain': -50.0, 'tax_due': 0.0}
            }
        },
        {'year': 2023, 'month': 4, 'total_tax_due': 0.0, 'details': 'No sales in this month.'}
    ]
    
    temp_dir = Path("./temp_darf_report")
    temp_dir.mkdir(exist_ok=True)
    
    # --- WHEN ---
    try:
        report_path = generate_darf_excel_report(dummy_tax_data, temp_dir)
        
        # --- THEN ---
        assert report_path is not None, "Report path should not be None."
        assert report_path.exists(), "Excel report file was not created."
        
        # Read back the data to verify content
        df_read = pd.read_excel(report_path)
        
        assert df_read.shape == (3, 6), "Report has incorrect shape."
        assert df_read['Tax Due (R$)'].sum() == 855.0, "Tax amounts in report are incorrect."
        assert df_read[df_read['Asset Class'] == 'ETF']['Tax Due (R$)'].iloc[0] == 0.0
        
        print(f"\nSuccessfully created and validated the report: {report_path}")

    except Exception as e:
        import traceback
        print(f"An error occurred: {e}")
        traceback.print_exc()
    finally:
        import shutil
        if temp_dir.exists():
            shutil.rmtree(temp_dir) 


###############################################################################
### FILE: src/b3alloc/taxes/ledger.py
###############################################################################
import pandas as pd
from typing import List, Dict, Tuple
import numpy as np

class TaxLedger:
    """
    Manages a detailed, lot-by-lot inventory of asset positions for tax purposes.

    This ledger tracks individual purchase lots (date, shares, price) to enable
    accurate cost-basis tracking using the FIFO (First-In, First-Out) method.
    """

    def __init__(self):
        # A list of dictionaries, where each dict is a "lot"
        self.lots = []
        # A list to record every sale for tax reporting
        self.sales_log = []

    def record_buy(self, ticker: str, shares: int, price: float, date: pd.Timestamp):
        """Records the purchase of an asset lot."""
        if shares <= 0 or price <= 0:
            return
        
        self.lots.append({
            'ticker': ticker,
            'shares_remaining': int(shares),
            'purchase_price': float(price),
            'purchase_date': date,
            'lot_id': f"{date.strftime('%Y%m%d%H%M%S')}-{ticker}-{shares}" # Unique ID
        })
        # Sort by date to ensure FIFO logic works correctly
        self.lots.sort(key=lambda x: x['purchase_date'])

    def record_sell(
        self, ticker: str, shares_to_sell: int, sale_price: float, sale_date: pd.Timestamp
    ) -> float:
        """
        Records a sale and calculates the capital gain using the FIFO method.

        This method iterates through the oldest lots of the specified ticker,
        "consuming" them to fulfill the sale. It calculates the total cost basis
        and logs the sale's details.

        Args:
            ticker: The ticker being sold.
            shares_to_sell: The number of shares to sell.
            sale_price: The price at which the shares are sold.
            sale_date: The date of the sale.

        Returns:
            The total capital gain (positive) or loss (negative) from the sale.
        """
        if shares_to_sell <= 0:
            return 0.0

        relevant_lots = [lot for lot in self.lots if lot['ticker'] == ticker and lot['shares_remaining'] > 0]
        
        if sum(lot['shares_remaining'] for lot in relevant_lots) < shares_to_sell:
            # This would indicate an attempt to sell more shares than owned.
            # In a real system, this should be a critical error.
            print(f"Warning: Attempted to sell {shares_to_sell} of {ticker}, but only {sum(lot['shares_remaining'] for lot in relevant_lots)} owned.")
            # Sell what's available
            shares_to_sell = sum(lot['shares_remaining'] for lot in relevant_lots)

        shares_sold_so_far = 0
        total_cost_basis = 0.0
        
        for lot in relevant_lots:
            if shares_sold_so_far >= shares_to_sell:
                break
            
            shares_from_this_lot = min(shares_to_sell - shares_sold_so_far, lot['shares_remaining'])
            
            # Update the lot
            lot['shares_remaining'] -= shares_from_this_lot
            
            # Accumulate cost basis
            cost_of_this_portion = shares_from_this_lot * lot['purchase_price']
            total_cost_basis += cost_of_this_portion
            
            shares_sold_so_far += shares_from_this_lot

        # Log the sale
        gross_sale_value = shares_to_sell * sale_price
        capital_gain = gross_sale_value - total_cost_basis
        
        self.sales_log.append({
            'sale_date': sale_date,
            'ticker': ticker,
            'shares_sold': shares_to_sell,
            'sale_price': sale_price,
            'gross_sale_value': gross_sale_value,
            'cost_basis': total_cost_basis,
            'capital_gain': capital_gain
        })
        
        return capital_gain

    def get_current_holdings(self) -> pd.Series:
        """Calculates the total number of shares currently held for each ticker."""
        holdings = {}
        for lot in self.lots:
            if lot['shares_remaining'] > 0:
                holdings[lot['ticker']] = holdings.get(lot['ticker'], 0) + lot['shares_remaining']
        return pd.Series(holdings, dtype=int)

if __name__ == '__main__':
    print("--- Running Tax Ledger Module Standalone Test ---")
    
    # --- GIVEN ---
    ledger = TaxLedger()
    
    # 1. A series of buys for PETR4.SA at different prices
    ledger.record_buy(ticker='PETR4.SA', shares=100, price=25.0, date=pd.to_datetime('2023-01-10'))
    ledger.record_buy(ticker='VALE3.SA', shares=50,  price=70.0, date=pd.to_datetime('2023-01-15'))
    ledger.record_buy(ticker='PETR4.SA', shares=200, price=30.0, date=pd.to_datetime('2023-02-20'))
    
    print("--- Initial Holdings ---")
    print(ledger.get_current_holdings())
    assert ledger.get_current_holdings()['PETR4.SA'] == 300
    
    # --- WHEN ---
    # 2. Sell 150 shares of PETR4.SA
    print("\n--- Selling 150 shares of PETR4.SA ---")
    gain1 = ledger.record_sell(ticker='PETR4.SA', shares_to_sell=150, sale_price=35.0, date=pd.to_datetime('2023-03-01'))
    
    # --- THEN ---
    # FIFO Logic:
    # The first 100 shares sold come from the first lot (bought at R$ 25.0).
    # The next 50 shares sold come from the second lot (bought at R$ 30.0).
    # Cost basis = (100 * 25.0) + (50 * 30.0) = 2500 + 1500 = 4000
    # Gross sale = 150 * 35.0 = 5250
    # Expected gain = 5250 - 4000 = 1250
    
    print(f"Calculated Capital Gain: R$ {gain1:.2f}")
    assert np.isclose(gain1, 1250.0), "FIFO capital gain calculation is incorrect."
    
    # Check remaining holdings
    # First lot of PETR4 should be empty.
    # Second lot should have 200 - 50 = 150 shares remaining.
    remaining_holdings = ledger.get_current_holdings()
    print("\n--- Holdings After Sale ---")
    print(remaining_holdings)
    assert remaining_holdings['PETR4.SA'] == 150
    
    petr4_lot1 = next(lot for lot in ledger.lots if lot['purchase_price'] == 25.0)
    assert petr4_lot1['shares_remaining'] == 0
    
    petr4_lot2 = next(lot for lot in ledger.lots if lot['purchase_price'] == 30.0)
    assert petr4_lot2['shares_remaining'] == 150
    
    print("\nOK: Ledger correctly applied FIFO logic and updated lot shares.")

    # --- Test Edge Case: Selling more shares than available ---
    print("\n--- Testing selling more shares than available ---")
    gain2 = ledger.record_sell(ticker='VALE3.SA', shares_to_sell=100, sale_price=80.0, date=pd.to_datetime('2023-03-05'))
    # Expected: Should sell the 50 available shares and log the gain for that amount.
    expected_gain2 = (50 * 80.0) - (50 * 70.0) # (Sale Value) - (Cost Basis)
    assert np.isclose(gain2, expected_gain2), "Gain calculation for partial sell-off is incorrect."
    assert ledger.get_current_holdings().get('VALE3.SA', 0) == 0, "All shares of VALE3.SA should have been sold."
    print("OK: Ledger correctly handled attempt to sell more shares than owned.")


###############################################################################
### FILE: src/b3alloc/taxes/tax_tracker.py
###############################################################################
import pandas as pd
from typing import Dict, List

# Define constants for Brazilian tax rules as per the amendment
TAX_RATE_STOCKS = 0.15
TAX_RATE_BDRS = 0.15
TAX_RATE_ETFS = 0.15
STOCK_EXEMPTION_THRESHOLD = 20000.0  # R$ 20.000,00

def _classify_asset(ticker: str) -> str:
    """Classifies an asset based on its ticker convention."""
    if ticker.endswith("11.SA"):
        return 'ETF'
    elif ticker.endswith("34.SA") or ticker.endswith("35.SA"): # Add other BDR endings if needed
        return 'BDR'
    else: # Default to common stock
        return 'STOCK'

def calculate_monthly_taxes(
    sales_log: List[Dict],
    year: int,
    month: int
) -> Dict:
    """
    Calculates the total capital gains tax due for a specific month.

    This function processes all sales within a given month, applies the relevant
    tax rules (like the R$ 20k exemption), and computes the final tax liability.

    Args:
        sales_log: A list of sale record dictionaries from a TaxLedger.
        year: The year to calculate taxes for.
        month: The month to calculate taxes for.

    Returns:
        A dictionary summarizing the tax calculation for the month.
    """
    start_date = pd.to_datetime(f"{year}-{month}-01")
    end_date = start_date + pd.offsets.MonthEnd(0)
    
    monthly_sales = [
        s for s in sales_log if start_date <= s['sale_date'] <= end_date
    ]
    
    if not monthly_sales:
        return {'tax_due': 0.0, 'details': 'No sales in this month.'}
        
    # --- Step 1: Aggregate sales and gains by asset class ---
    summary = {
        'STOCK': {'gross_sales': 0.0, 'net_gain': 0.0},
        'BDR': {'gross_sales': 0.0, 'net_gain': 0.0},
        'ETF': {'gross_sales': 0.0, 'net_gain': 0.0}
    }
    
    for sale in monthly_sales:
        asset_class = _classify_asset(sale['ticker'])
        if asset_class in summary:
            summary[asset_class]['gross_sales'] += sale['gross_sale_value']
            summary[asset_class]['net_gain'] += sale['capital_gain']
            
    # --- Step 2: Apply Exemption and Tax Rules ---
    tax_due_stock = 0.0
    tax_due_bdr = 0.0
    tax_due_etf = 0.0
    
    # Stocks (Ações)
    stock_summary = summary['STOCK']
    # The exemption applies to the TOTAL gross sales of common stocks in the month.
    if stock_summary['gross_sales'] > STOCK_EXEMPTION_THRESHOLD:
        if stock_summary['net_gain'] > 0:
            tax_due_stock = stock_summary['net_gain'] * TAX_RATE_STOCKS
    
    # BDRs - No exemption
    bdr_summary = summary['BDR']
    if bdr_summary['net_gain'] > 0:
        tax_due_bdr = bdr_summary['net_gain'] * TAX_RATE_BDRS

    # ETFs - No exemption
    etf_summary = summary['ETF']
    if etf_summary['net_gain'] > 0:
        tax_due_etf = etf_summary['net_gain'] * TAX_RATE_ETFS

    # TODO: Implement loss offsetting logic here in a future iteration.
    # For now, we only tax positive net gains in each category.

    total_tax_due = tax_due_stock + tax_due_bdr + tax_due_etf

    return {
        'year': year,
        'month': month,
        'total_tax_due': total_tax_due,
        'details': {
            'STOCK': {**stock_summary, 'tax_due': tax_due_stock, 'exemption_applied': stock_summary['gross_sales'] <= STOCK_EXEMPTION_THRESHOLD},
            'BDR': {**bdr_summary, 'tax_due': tax_due_bdr},
            'ETF': {**etf_summary, 'tax_due': tax_due_etf}
        }
    }


if __name__ == '__main__':
    from .ledger import TaxLedger
    import numpy as np
    print("--- Running Tax Tracker Module Standalone Test ---")
    
    # --- GIVEN ---
    ledger = TaxLedger()
    # Scenario for March 2023
    
    # Case 1: Stock sale UNDER the exemption limit
    ledger.record_buy('PETR4.SA', 100, 25.0, pd.to_datetime('2023-01-01'))
    ledger.record_sell('PETR4.SA', 100, 30.0, pd.to_datetime('2023-03-05')) # Gross sale = 3000

    # Case 2: Stock sale OVER the exemption limit
    ledger.record_buy('VALE3.SA', 500, 70.0, pd.to_datetime('2023-01-01'))
    ledger.record_sell('VALE3.SA', 500, 80.0, pd.to_datetime('2023-03-10')) # Gross sale = 40000

    # Case 3: BDR sale (no exemption)
    ledger.record_buy('AAPL34.SA', 20, 150.0, pd.to_datetime('2023-01-01'))
    ledger.record_sell('AAPL34.SA', 20, 160.0, pd.to_datetime('2023-03-15')) # Gross sale = 3200

    # Case 4: ETF with a loss
    ledger.record_buy('BOVA11.SA', 10, 110.0, pd.to_datetime('2023-01-01'))
    ledger.record_sell('BOVA11.SA', 10, 105.0, pd.to_datetime('2023-03-20')) # Gross sale = 1050

    # --- WHEN ---
    march_taxes = calculate_monthly_taxes(ledger.sales_log, 2023, 3)

    # --- THEN ---
    print("\n--- Tax Calculation Summary for March 2023 ---")
    import json
    print(json.dumps(march_taxes, indent=2))
    
    # --- Validation ---
    # Total stock gross sales = 3000 (PETR4) + 40000 (VALE3) = 43000.
    # This is > 20k, so the exemption does NOT apply.
    assert not march_taxes['details']['STOCK']['exemption_applied']
    
    # Total stock net gain = (3000-2500) + (40000-35000) = 500 + 5000 = 5500
    expected_stock_tax = 5500 * TAX_RATE_STOCKS
    assert np.isclose(march_taxes['details']['STOCK']['tax_due'], expected_stock_tax)
    
    # BDR gain = (3200 - 3000) = 200
    expected_bdr_tax = 200 * TAX_RATE_BDRS
    assert np.isclose(march_taxes['details']['BDR']['tax_due'], expected_bdr_tax)
    
    # ETF had a loss, so tax should be 0.
    assert march_taxes['details']['ETF']['tax_due'] == 0.0
    
    # Total tax
    expected_total_tax = expected_stock_tax + expected_bdr_tax
    assert np.isclose(march_taxes['total_tax_due'], expected_total_tax)
    
    print(f"\nOK: Total tax due (R$ {march_taxes['total_tax_due']:.2f}) matches expected calculation.")


###############################################################################
### FILE: src/b3alloc/trades/__init__.py
###############################################################################
# This file marks the 'b3alloc' directory as a Python package.
# It can also be used to define package-level variables or import key functions.

__version__ = "0.1.0"


###############################################################################
### FILE: src/b3alloc/trades/trade_calculator.py
###############################################################################
import pandas as pd
import numpy as np
from typing import Literal

def calculate_target_shares(
    target_weights: pd.Series,
    portfolio_value: float,
    last_prices: pd.Series
) -> pd.DataFrame:
    """
    Converts ideal portfolio weights into target integer share counts.

    Args:
        target_weights: A Series of optimal asset weights from the optimizer.
        portfolio_value: The total current market value of the portfolio.
        last_prices: A Series of the most recent closing prices for each asset.

    Returns:
        A DataFrame with columns ['ticker', 'target_value', 'target_shares_frac',
        'last_price'] for each asset in the target portfolio.
    """
    if portfolio_value <= 0:
        return pd.DataFrame()

    # Align inputs to ensure consistent ordering and handling of assets
    weights, prices = target_weights.align(last_prices, join='inner')
    
    # Calculate the ideal monetary value for each asset
    target_value = weights * portfolio_value
    
    # Calculate the ideal fractional number of shares
    target_shares_frac = target_value / prices

    results_df = pd.DataFrame({
        'target_value': target_value,
        'target_shares_frac': target_shares_frac,
        'last_price': prices
    }).reset_index().rename(columns={'index': 'ticker'})
    
    return results_df


def resolve_fractional_shares(
    target_shares_df: pd.DataFrame,
    rounding_mode: Literal['round', 'floor', 'ceil'] = 'round'
) -> pd.DataFrame:
    """
    Rounds fractional target shares to the nearest integer.

    Args:
        target_shares_df: The DataFrame from `calculate_target_shares`.
        rounding_mode: The method for rounding ('round', 'floor', or 'ceil').

    Returns:
        The input DataFrame with an added 'target_shares' integer column.
    """
    if 'target_shares_frac' not in target_shares_df.columns:
        raise ValueError("Input DataFrame must contain 'target_shares_frac' column.")

    if rounding_mode == 'round':
        target_shares_df['target_shares'] = np.round(target_shares_df['target_shares_frac']).astype(int)
    elif rounding_mode == 'floor':
        target_shares_df['target_shares'] = np.floor(target_shares_df['target_shares_frac']).astype(int)
    elif rounding_mode == 'ceil':
        target_shares_df['target_shares'] = np.ceil(target_shares_df['target_shares_frac']).astype(int)
    else:
        raise ValueError(f"Unknown rounding mode: {rounding_mode}")
        
    return target_shares_df


def compute_trade_list(
    current_shares: pd.Series,
    target_shares: pd.Series
) -> pd.DataFrame:
    """
    Generates a list of trades required to move from current to target positions.

    Args:
        current_shares: A Series of current share holdings, indexed by ticker.
        target_shares: A Series of target share holdings, indexed by ticker.

    Returns:
        A DataFrame with columns ['ticker', 'current_shares', 'target_shares',
        'delta_shares', 'action'], detailing the required trades.
    """
    # Combine current and target holdings into a single DataFrame using an outer join
    # to ensure all tickers from both series are included.
    trade_df = pd.DataFrame({'current': current_shares, 'target': target_shares})
    trade_df = trade_df.fillna(0) # Assume 0 shares for any missing tickers
    
    trade_df['delta_shares'] = (trade_df['target'] - trade_df['current']).astype(int)
    
    # Define the action based on the delta
    trade_df['action'] = 'HOLD'
    trade_df.loc[trade_df['delta_shares'] > 0, 'action'] = 'BUY'
    trade_df.loc[trade_df['delta_shares'] < 0, 'action'] = 'SELL'
    
    # Filter out assets with no required action
    final_trades = trade_df[trade_df['action'] != 'HOLD'].copy()
    
    return final_trades.reset_index().rename(columns={
        'index': 'ticker',
        'current': 'current_shares',
        'target': 'target_shares'
    })


if __name__ == '__main__':
    print("--- Running Trade Calculator Module Standalone Test ---")
    
    # --- GIVEN ---
    current_portfolio = pd.Series({'PETR4.SA': 100, 'VALE3.SA': 50, 'BBDC4.SA': 200})
    target_weights = pd.Series({'PETR4.SA': 0.5, 'VALE3.SA': 0.3, 'ITUB4.SA': 0.2})
    latest_prices = pd.Series({'PETR4.SA': 30.0, 'VALE3.SA': 70.0, 'BBDC4.SA': 15.0, 'ITUB4.SA': 28.0})
    
    # Calculate current portfolio value
    pv = (current_portfolio * latest_prices.reindex(current_portfolio.index)).sum()
    print(f"Current Portfolio Value: R$ {pv:,.2f}")
    
    # --- WHEN ---
    # 1. Calculate target shares
    target_df = calculate_target_shares(target_weights, pv, latest_prices)
    
    # 2. Resolve fractional shares
    target_df = resolve_fractional_shares(target_df, rounding_mode='round')
    
    # 3. Compute the final trade list
    target_shares_series = target_df.set_index('ticker')['target_shares']
    trade_list = compute_trade_list(current_portfolio, target_shares_series)
    
    # --- THEN ---
    print("\nTarget Share Allocation:")
    print(target_df)
    
    print("\n--- Final Trade List ---")
    print(trade_list)
    
    # --- Validation ---
    # BBDC4.SA should be a SELL, as it's not in the target weights.
    assert trade_list[trade_list['ticker'] == 'BBDC4.SA']['action'].iloc[0] == 'SELL'
    assert trade_list[trade_list['ticker'] == 'BBDC4.SA']['delta_shares'].iloc[0] == -200
    
    # ITUB4.SA should be a BUY, as it was not in the original portfolio.
    assert trade_list[trade_list['ticker'] == 'ITUB4.SA']['action'].iloc[0] == 'BUY'
    
    # PETR4.SA's target value should be 50% of PV
    expected_petr4_value = 0.5 * pv
    actual_petr4_value = target_df[target_df['ticker'] == 'PETR4.SA']['target_value'].iloc[0]
    assert np.isclose(expected_petr4_value, actual_petr4_value)
    
    print("\nOK: Trade list calculation is correct and intuitive.")

    # --- Test Edge Case: Empty Current Portfolio ---
    print("\n--- Testing with empty current portfolio ---")
    empty_portfolio = pd.Series(dtype=int)
    trade_list_empty = compute_trade_list(empty_portfolio, target_shares_series)
    assert len(trade_list_empty) == len(target_shares_series), "Should generate BUY orders for all target assets."
    assert (trade_list_empty['action'] == 'BUY').all(), "All actions should be BUY."
    print("OK: Correctly handles an empty initial portfolio.")


###############################################################################
### FILE: src/b3alloc/utils_dates.py
###############################################################################
import pandas as pd
import holidays
from functools import lru_cache
import logging

# Using ANBIMA rules from the 'holidays' library as a robust proxy for B3 financial market holidays.
# Caching the holiday generation per year is a significant performance boost.
@lru_cache(maxsize=16)
def _get_b3_holidays_for_year(year: int) -> holidays.HolidayBase:
    """Memoized helper to get all B3 holidays for a single year."""
    return holidays.Brazil(years=year, state="SP")

@lru_cache(maxsize=16)
def get_b3_trading_calendar(
    start_date: str | pd.Timestamp, end_date: str | pd.Timestamp
) -> pd.DatetimeIndex:
    """
    Generates a trading calendar for the B3 Exchange for a given date range.

    Args:
        start_date: The start date of the calendar (inclusive).
        end_date: The end date of the calendar (inclusive).

    Returns:
        A pandas DatetimeIndex containing only the trading days for the B3.
        The dates are timezone-naive, representing the trading day.
    """
    start_date = pd.to_datetime(start_date)
    end_date = pd.to_datetime(end_date)

    # Generate business days (Mon-Fri) and then remove specific holidays.
    business_days = pd.date_range(start=start_date, end=end_date, freq="B")

    # Collect all holidays across the required year range using the cached helper
    all_holidays = set()
    for year in range(start_date.year, end_date.year + 1):
        all_holidays.update(_get_b3_holidays_for_year(year).keys())
    
    holiday_dates = pd.to_datetime(list(all_holidays))

    trading_calendar = business_days[~business_days.isin(holiday_dates)]
    
    # Ensure timezone-naive as per specification for consistent indexing.
    return trading_calendar.tz_localize(None)


def generate_rebalance_dates(
    start_date: str | pd.Timestamp,
    end_date: str | pd.Timestamp,
    frequency: str = "M",
) -> pd.DatetimeIndex:
    """
    Generates a series of rebalancing dates based on a specified frequency.

    This function ensures that rebalance dates are actual B3 trading days.
    For example, for monthly rebalancing, it selects the last trading day of each month.

    Args:
        start_date: The start date for the backtest period.
        end_date: The end date for the backtest period.
        frequency: The rebalancing frequency ('M' for monthly, 'W' for weekly, etc.).
                   Currently, only 'M' is fully supported as per spec.

    Returns:
        A pandas DatetimeIndex of rebalance dates.
    """
    start_date = pd.to_datetime(start_date)
    end_date = pd.to_datetime(end_date)
    full_calendar = get_b3_trading_calendar(start_date, end_date)

    if frequency.lower().startswith("m"): # Accept 'M', 'monthly', etc.
        # Get the last day of all months in the period
        month_ends = pd.date_range(start=start_date, end=end_date, freq="M")
        
        # Find the closest preceding trading day for each month-end.
        # We must apply .asof for each date individually.
        rebalance_dates = month_ends.map(lambda date: full_calendar.asof(date))
        
        # Remove duplicates (if any) and NaTs (if a month had no trading days before it)
        rebalance_dates = rebalance_dates.dropna().unique()
        return pd.DatetimeIndex(rebalance_dates)

    # Future extension: add weekly, quarterly logic.
    elif frequency.lower().startswith("w"): # Accept 'W', 'weekly', etc.
        raise NotImplementedError("Weekly rebalancing is not yet implemented.")
    else:
        raise ValueError(f"Unsupported rebalancing frequency: {frequency}")


def apply_publication_lag(
    event_dates: pd.DatetimeIndex | pd.Series,
    lag_days: int,
    calendar: pd.DatetimeIndex,
) -> pd.DatetimeIndex:
    """
    Shifts a series of event dates forward by a number of days and then snaps
    each date to the next available trading day on the B3 calendar.

    This is crucial for ensuring that fundamental data is only considered
    "known" to the market after its publication lag.

    Args:
        event_dates: A DatetimeIndex or Series of original event dates (e.g., fiscal period ends).
        lag_days: The number of calendar days to shift forward.
        calendar: The canonical B3 trading calendar (a DatetimeIndex).

    Returns:
        A DatetimeIndex of the corresponding "actionable" trading dates.
    """
    shifted_dates = pd.to_datetime(event_dates) + pd.Timedelta(days=lag_days)
    
    # Use the calendar's searchsorted method to find the position of each shifted date.
    # 'left' means that if a date falls on a non-trading day, it will take the *next* available trading day.
    positions = calendar.searchsorted(shifted_dates, side='left')

    # Handle dates that fall beyond the calendar's range
    valid_positions = positions[positions < len(calendar)]
    
    return calendar[valid_positions]


if __name__ == "__main__":
    # --- Example for get_b3_trading_calendar ---
    print("--- Testing get_b3_trading_calendar ---")
    calendar_2024 = get_b3_trading_calendar("2024-01-01", "2024-12-31")
    print(f"Total trading days in 2024: {len(calendar_2024)}")
    carnaval_monday = pd.to_datetime("2024-02-12")
    print(f"Is Feb 12, 2024 (Carnaval) a trading day? {carnaval_monday in calendar_2024}\n")

    # --- Example for generate_rebalance_dates ---
    print("--- Testing generate_rebalance_dates ---")
    rebal_dates = generate_rebalance_dates("2023-01-01", "2023-06-30", frequency="M")
    print("Monthly rebalance dates for H1 2023:")
    print(rebal_dates) # Expect: Jan-31, Feb-28, Mar-31, Apr-28, May-31, Jun-30 (all are trading days)
    
    # Test a month where last day is a weekend
    rebal_dates_sep_2023 = generate_rebalance_dates("2023-09-01", "2023-09-30", "M")
    print("\nRebalance date for Sep 2023 (Sat, Sep 30th is not a trading day):")
    print(rebal_dates_sep_2023) # Expect: 2023-09-29
    
    # --- Example for apply_publication_lag ---
    print("\n--- Testing apply_publication_lag ---")
    full_calendar = get_b3_trading_calendar("2023-03-01", "2023-04-30")
    # Assume fundamentals published for fiscal period ending March 31
    fiscal_period_end = pd.to_datetime(["2023-03-31"])
    print(f"Fiscal period end date: {fiscal_period_end[0].date()}")

    # As per spec, use a 3-day publication lag
    info_becomes_public = apply_publication_lag(fiscal_period_end, lag_days=3, calendar=full_calendar)
    print(f"With a 3-day lag, data is actionable on: {info_becomes_public[0].date()}")
    # March 31, 2023 is a Friday. 3 trading days later should be Wednesday, April 5th.
    # Fri (day 0) -> Mon (day 1) -> Tue (day 2) -> Wed (day 3) -> this is wrong.
    # The lag should be *after* the event.
    # Correct logic: Event on T. Actionable on T+3.
    # March 31 is T. April 3 is T+1, April 4 is T+2, April 5 is T+3. Correct.


###############################################################################
### FILE: src/b3alloc/views/__init__.py
###############################################################################
# This file marks the 'b3alloc' directory as a Python package.
# It can also be used to define package-level variables or import key functions.

__version__ = "0.1.0"


###############################################################################
### FILE: src/b3alloc/views/views_parser.py
###############################################################################
import pandas as pd
import numpy as np
import yaml
from typing import List, Dict, Tuple, Any

def parse_qualitative_views(
    view_definitions: List[Dict[str, Any]],
    universe: List[str]
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Parses a list of qualitative view dictionaries into Black-Litterman P and Q matrices.

    This function supports two types of views:
    1.  'absolute': "Asset X will return Y%".
    2.  'relative': "Asset X will outperform Asset Y by Z%".

    Args:
        view_definitions: A list of view dictionaries, typically loaded from a YAML file.
        universe: A sorted list of all ticker symbols in the investment universe.
                  The order of this list defines the column order of the P matrix.

    Returns:
        A tuple containing:
        - P (np.ndarray): The KxN pick matrix for the qualitative views.
        - Q (np.ndarray): The Kx1 vector of view values.
    """
    # Ensure the universe is sorted for consistent matrix construction
    universe = sorted(universe)

    num_assets = len(universe)
    p_rows = []
    q_rows = []
    
    # Create a mapping from ticker to its index in the universe for quick lookups
    ticker_to_idx = {ticker: i for i, ticker in enumerate(universe)}

    print(f"Parsing {len(view_definitions)} qualitative views...")

    for i, view in enumerate(view_definitions):
        view_type = view.get('type')
        expression = view.get('expr')
        magnitude = view.get('magnitude')

        if not all([view_type, expression, isinstance(magnitude, (int, float))]):
            raise ValueError(f"View #{i+1} is malformed. It must have 'type', 'expr', and 'magnitude'.")
            
        p_row = np.zeros(num_assets)

        if view_type == 'absolute':
            # Absolute view: P has a 1 at the asset's position.
            ticker = expression.strip()
            if ticker not in ticker_to_idx:
                print(f"Warning: Ticker '{ticker}' in absolute view not in universe. Skipping view.")
                continue
            
            p_row[ticker_to_idx[ticker]] = 1.0
        
        elif view_type == 'relative':
            # Relative view: "A - B". P has a 1 at A's position and -1 at B's.
            try:
                ticker_a, ticker_b = [t.strip() for t in expression.split('–')] # Note: using en-dash
                if len(ticker_a) == 0 or len(ticker_b) == 0: # Check for malformed split
                    ticker_a, ticker_b = [t.strip() for t in expression.split('-')]
            except ValueError:
                raise ValueError(f"Relative view '{expression}' is malformed. Expected 'TICKER_A – TICKER_B'.")

            if ticker_a not in ticker_to_idx or ticker_b not in ticker_to_idx:
                print(f"Warning: One or both tickers in relative view '{expression}' not in universe. Skipping view.")
                continue
            
            p_row[ticker_to_idx[ticker_a]] = 1.0
            p_row[ticker_to_idx[ticker_b]] = -1.0
            
        else:
            raise ValueError(f"Unknown view type '{view_type}' in view #{i+1}.")

        p_rows.append(p_row)
        q_rows.append([magnitude])

    if not p_rows:
        # Return empty matrices if no valid views were parsed
        return np.empty((0, num_assets)), np.empty((0, 1))

    return np.array(p_rows), np.array(q_rows)


if __name__ == '__main__':
    print("--- Running Qualitative Views Parser Module Standalone Test ---")

    # --- GIVEN ---
    # 1. A sorted investment universe
    test_universe = sorted(['PETR4.SA', 'VALE3.SA', 'ITUB4.SA', 'MGLU3.SA'])
    
    # 2. A YAML-like structure of qualitative views
    yaml_content = """
    views:
      - type: relative
        expr: "VALE3.SA – PETR4.SA"
        magnitude: 0.03    # Expect VALE3 to outperform PETR4 by 3%
        confidence: 0.70
      - type: absolute
        expr: "MGLU3.SA"
        magnitude: -0.05   # Expect MGLU3 to have a return of -5%
        confidence: 0.50
      - type: relative
        expr: "ITUB4.SA - MGLU3.SA" # Using hyphen instead of en-dash
        magnitude: 0.06
        confidence: 0.80
      - type: absolute
        expr: "NON_EXISTENT.SA" # A ticker not in our universe
        magnitude: 0.10
        confidence: 0.40
    """
    view_defs = yaml.safe_load(yaml_content)['views']
    
    # --- WHEN ---
    P_qual, Q_qual = parse_qualitative_views(view_defs, test_universe)
    
    # --- THEN ---
    print("\nParsed P matrix (Qualitative):\n", P_qual)
    print("\nParsed Q vector (Qualitative):\n", Q_qual)
    
    # --- Validation ---
    # Universe order: ITUB4, MGLU3, PETR4, VALE3
    # Expected P matrix (3 valid views x 4 assets)
    expected_P = np.array([
    # View 1: VALE3 - PETR4
       [0.,  0., -1., 1.],
    # View 2: MGLU3
       [0.,  1.,  0., 0.],
    # View 3: ITUB4 - MGLU3
       [1., -1.,  0., 0.]
    ])
    
    # Expected Q vector
    expected_Q = np.array([[0.03], [-0.05], [0.06]])
    
    assert P_qual.shape == (3, 4), f"P matrix shape is {P_qual.shape}, expected (3, 4)."
    assert Q_qual.shape == (3, 1), "Q vector shape is incorrect."
    
    assert np.allclose(P_qual, expected_P), "P matrix content is incorrect."
    assert np.allclose(Q_qual, expected_Q), "Q vector content is incorrect."
    
    print("\nOK: Parser correctly translates qualitative views into P and Q matrices.")

    # --- Test with unsorted universe ---
    print("\n--- Testing with unsorted universe ---")
    unsorted_universe = ['PETR4.SA', 'VALE3.SA', 'ITUB4.SA', 'MGLU3.SA']
    P_unsorted, Q_unsorted = parse_qualitative_views(view_defs, unsorted_universe)
    assert np.allclose(P_unsorted, expected_P), "P matrix is incorrect when universe is unsorted."
    assert np.allclose(Q_unsorted, expected_Q), "Q vector is incorrect when universe is unsorted."
    print("OK: Parser correctly handles unsorted universe input.")


###############################################################################
### FILE: src/b3alloc/viz/__init__.py
###############################################################################
# This file marks the 'b3alloc' directory as a Python package.
# It can also be used to define package-level variables or import key functions.

__version__ = "0.1.0"


###############################################################################
### FILE: src/b3alloc/viz/plots_portfolio.py
###############################################################################
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.figure import Figure

def plot_equity_curve(
    strategy_returns: pd.Series,
    benchmark_returns: pd.Series
) -> Figure:
    """
    Plots the equity curve (cumulative returns) for the strategy vs. a benchmark.

    Args:
        strategy_returns: A pandas Series of the strategy's periodic returns.
        benchmark_returns: A pandas Series of the benchmark's periodic returns.

    Returns:
        A matplotlib Figure object containing the plot.
    """
    fig, ax = plt.subplots(figsize=(12, 7))
    
    # Calculate cumulative returns
    strategy_cumulative = (1 + strategy_returns).cumprod()
    benchmark_cumulative = (1 + benchmark_returns).cumprod()
    
    ax.plot(strategy_cumulative.index, strategy_cumulative, label="Strategy", color="blue", lw=2)
    ax.plot(benchmark_cumulative.index, benchmark_cumulative, label="Benchmark", color="gray", ls='--')
    
    # Use a log scale for the y-axis to better visualize relative performance
    ax.set_yscale('log')
    
    ax.set_title('Equity Curve (Strategy vs. Benchmark)', fontsize=16)
    ax.set_ylabel('Cumulative Return (Log Scale)', fontsize=12)
    ax.set_xlabel('Date', fontsize=12)
    ax.legend(loc='upper left')
    ax.grid(True, which='both', linestyle='--', linewidth=0.5)
    
    # Format dates on the x-axis
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
    fig.autofmt_xdate()
    
    return fig

def plot_drawdowns(strategy_returns: pd.Series) -> Figure:
    """
    Plots the drawdown periods for the strategy.

    Args:
        strategy_returns: A pandas Series of the strategy's periodic returns.

    Returns:
        A matplotlib Figure object containing the plot.
    """
    fig, ax = plt.subplots(figsize=(12, 5))
    
    cumulative_returns = (1 + strategy_returns).cumprod()
    running_max = cumulative_returns.cummax()
    drawdown = (cumulative_returns - running_max) / running_max
    
    ax.fill_between(drawdown.index, drawdown, 0, color='red', alpha=0.3)
    ax.plot(drawdown.index, drawdown, color='red', lw=1)
    
    ax.set_title('Portfolio Drawdowns', fontsize=16)
    ax.set_ylabel('Drawdown', fontsize=12)
    ax.set_xlabel('Date', fontsize=12)
    ax.grid(True, linestyle='--', linewidth=0.5)
    
    # Format y-axis as percentage
    ax.yaxis.set_major_formatter(plt.FuncFormatter('{:.0%}'.format))
    
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
    fig.autofmt_xdate()
    
    return fig

def plot_weights_evolution(weights_df: pd.DataFrame, top_n: int = 10) -> Figure:
    """
    Plots the evolution of asset weights over time as a stacked area chart.

    To keep the legend readable, only the top N assets by average weight are
    shown explicitly. The rest are aggregated into an 'Others' category.

    Args:
        weights_df: A DataFrame where the index is the date and columns are
                    tickers, containing portfolio weights at each rebalance.
        top_n: The number of top assets to display in the legend.

    Returns:
        A matplotlib Figure object containing the plot.
    """
    fig, ax = plt.subplots(figsize=(12, 7))
    
    # Identify top N assets by average weight
    avg_weights = weights_df.mean().sort_values(ascending=False)
    top_assets = avg_weights.head(top_n).index.tolist()
    
    # Aggregate other assets
    if len(avg_weights) > top_n:
        other_assets = avg_weights.tail(len(avg_weights) - top_n).index.tolist()
        weights_df['Others'] = weights_df[other_assets].sum(axis=1)
        plot_df = weights_df[top_assets + ['Others']]
    else:
        plot_df = weights_df

    # Use pandas' built-in plotting for a stacked area chart
    plot_df.plot.area(ax=ax, stacked=True, lw=0)
    
    ax.set_title('Portfolio Weights Evolution', fontsize=16)
    ax.set_ylabel('Portfolio Weight', fontsize=12)
    ax.set_xlabel('Date', fontsize=12)
    ax.set_ylim(0, 1) # Weights should sum to 1
    ax.grid(True, linestyle='--', linewidth=0.5)
    
    # Improve legend
    ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1), title='Assets')
    
    fig.tight_layout() # Adjust layout to make room for legend
    
    return fig


if __name__ == '__main__':
    print("--- Running Visualization Module Standalone Test ---")
    
    # --- GIVEN ---
    # Create synthetic data for plotting
    periods = 12 * 5 # 5 years of monthly data
    dates = pd.date_range("2018-01-01", periods=periods, freq="MS")
    
    bench_ret = pd.Series(
        np.random.randn(periods) * 0.05 + 0.006, index=dates
    )
    strat_ret = pd.Series(
        0.8 * bench_ret + np.random.randn(periods) * 0.03 + 0.004, index=dates
    )
    
    # Create a dummy weights dataframe with many assets
    tickers = [f'Asset {i}' for i in range(20)]
    weights = np.random.rand(len(dates), len(tickers))
    weights = weights / weights.sum(axis=1, keepdims=True)
    weights_df = pd.DataFrame(weights, index=dates, columns=tickers)

    print("\nGenerating plots from synthetic data...")
    
    # --- WHEN & THEN ---
    try:
        # 1. Test Equity Curve Plot
        fig1 = plot_equity_curve(strat_ret, bench_ret)
        fig1.suptitle("Test: Equity Curve", y=1.02)
        
        # 2. Test Drawdown Plot
        fig2 = plot_drawdowns(strat_ret)
        fig2.suptitle("Test: Drawdowns", y=1.02)

        # 3. Test Weights Evolution Plot
        fig3 = plot_weights_evolution(weights_df, top_n=8)
        fig3.suptitle("Test: Weights Evolution (Top 8 + Others)", y=1.02)
        
        # Validation for weights plot
        assert len(fig3.axes[0].get_legend().get_texts()) == 9 # 8 assets + Others
        
        print("\nOK: All plot functions executed without error.")
        print("Displaying generated plots...")
        plt.show()

    except Exception as e:
        import traceback
        print(f"\nAn error occurred during testing: {e}")
        traceback.print_exc()


